{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.chdir(\"/home/wxdeng/stbr-min/python/ray/dag/tests/experimental\")\n",
    "os.environ[\"RAY_DEDUP_LOGS\"] = \"0\"\n",
    "os.environ[\"RAY_PYTEST_USE_GPU\"] = \"1\"\n",
    "os.environ[\"RAY_CGRAPH_VISUALIZE_SCHEDULE\"] = \"1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "# !python -m coll-sched-0512.ddp --name ddp_bench"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "# !python -m coll-sched-0512.ddp --name ddp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "# !python -m pytest -v -s test_cpu_communicator_dag.py::test_allreduce_wrong_shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "# !python -m pytest -v -s test_execution_schedule.py::TestSelectNextNodes::test_only_one_nccl_write"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m============================= test session starts ==============================\u001b[0m\n",
      "platform linux -- Python 3.13.3, pytest-8.3.5, pluggy-1.5.0 -- /home/wxdeng/miniconda3/envs/stbr-unify/bin/python\n",
      "cachedir: .pytest_cache\n",
      "rootdir: /home/wxdeng/stbr-min\n",
      "configfile: pytest.ini\n",
      "plugins: anyio-4.9.0\n",
      "collected 29 items                                                             \u001b[0m\n",
      "\n",
      "test_execution_schedule.py::TestSelectNextNodes::test_two_candidates_on_same_actor \u001b[32mPASSED\u001b[0m\n",
      "test_execution_schedule.py::TestSelectNextNodes::test_only_one_nccl_write \u001b[32mPASSED\u001b[0m\n",
      "test_execution_schedule.py::TestSelectNextNodes::test_two_nccl_writes \u001b[32mPASSED\u001b[0m\n",
      "test_execution_schedule.py::TestSelectNextNodes::test_only_one_nccl_collective \u001b[32mPASSED\u001b[0m\n",
      "test_execution_schedule.py::TestSelectNextNodes::test_two_nccl_collectives \u001b[32mPASSED\u001b[0m\n",
      "test_execution_schedule.py::TestSelectNextNodes_EXP::test_two_candidates_on_same_actor \u001b[32mPASSED\u001b[0m\n",
      "test_execution_schedule.py::TestSelectNextNodes_EXP::test_only_one_nccl_write \u001b[32mPASSED\u001b[0m\n",
      "test_execution_schedule.py::TestSelectNextNodes_EXP::test_two_nccl_writes \u001b[32mPASSED\u001b[0m\n",
      "test_execution_schedule.py::TestSelectNextNodes_EXP::test_only_one_nccl_collective \u001b[32mPASSED\u001b[0m\n",
      "test_execution_schedule.py::TestSelectNextNodes_EXP::test_two_nccl_collectives \u001b[32mPASSED\u001b[0m\n",
      "test_execution_schedule.py::TestBuildDAGNodeOperationGraph::test_edges_between_read_compute_write \u001b[32mPASSED\u001b[0m\n",
      "test_execution_schedule.py::TestBuildDAGNodeOperationGraph::test_edge_between_writer_and_reader \u001b[32mPASSED\u001b[0m\n",
      "test_execution_schedule.py::TestBuildDAGNodeOperationGraph::test_edge_between_compute_nodes \u001b[32mPASSED\u001b[0m\n",
      "test_execution_schedule.py::TestBuildDAGNodeOperationGraph::test_two_actors \u001b[32mPASSED\u001b[0m\n",
      "test_execution_schedule.py::TestBuildDAGNodeOperationGraph_EXP::test_edge_between_writer_and_reader \u001b[32mPASSED\u001b[0m\n",
      "test_execution_schedule.py::TestBuildDAGNodeOperationGraph_EXP::test_edge_between_compute_nodes \u001b[32mPASSED\u001b[0m\n",
      "test_execution_schedule.py::TestBuildDAGNodeOperationGraph_EXP::test_two_actors \u001b[32mPASSED\u001b[0m\n",
      "test_execution_schedule.py::TestGenerateActorToExecutionSchedule::test_single_actor_1 \u001b[32mPASSED\u001b[0m\n",
      "test_execution_schedule.py::TestGenerateActorToExecutionSchedule::test_single_actor_2 \u001b[32mPASSED\u001b[0m\n",
      "test_execution_schedule.py::TestGenerateActorToExecutionSchedule::test_two_actors_no_nccl \u001b[32mPASSED\u001b[0m\n",
      "test_execution_schedule.py::TestGenerateActorToExecutionSchedule::test_two_actors_with_nccl \u001b[32mPASSED\u001b[0m\n",
      "test_execution_schedule.py::TestGenerateActorToExecutionSchedule::test_simulate_pp_2workers_2batches_1f1b_with_nccl \u001b[32mPASSED\u001b[0m\n",
      "test_execution_schedule.py::TestGenerateActorToExecutionSchedule::test_simulate_pp_2workers_2batches_1f1b_no_nccl \u001b[32mPASSED\u001b[0m\n",
      "test_execution_schedule.py::TestGenerateActorToExecutionSchedule_EXP::test_single_actor_1 \u001b[32mPASSED\u001b[0m\n",
      "test_execution_schedule.py::TestGenerateActorToExecutionSchedule_EXP::test_single_actor_2 \u001b[32mPASSED\u001b[0m\n",
      "test_execution_schedule.py::TestGenerateActorToExecutionSchedule_EXP::test_two_actors_no_nccl \u001b[32mPASSED\u001b[0m\n",
      "test_execution_schedule.py::TestGenerateActorToExecutionSchedule_EXP::test_two_actors_with_nccl \u001b[32mPASSED\u001b[0m\n",
      "test_execution_schedule.py::TestGenerateActorToExecutionSchedule_EXP::test_simulate_pp_2workers_2batches_1f1b_no_nccl \u001b[32mPASSED\u001b[0m\n",
      "test_execution_schedule.py::TestGenerateActorToExecutionSchedule_EXP::test_simulate_pp_2workers_2batches_1f1b_with_nccl \u001b[32mPASSED\u001b[0m\n",
      "\n",
      "\u001b[32m============================== \u001b[32m\u001b[1m29 passed\u001b[0m\u001b[32m in 1.01s\u001b[0m\u001b[32m ==============================\u001b[0m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!python -m pytest -v -s test_execution_schedule.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m============================= test session starts ==============================\u001b[0m\n",
      "platform linux -- Python 3.13.3, pytest-8.3.5, pluggy-1.5.0 -- /home/wxdeng/miniconda3/envs/stbr-unify/bin/python\n",
      "cachedir: .pytest_cache\n",
      "rootdir: /home/wxdeng/stbr-min\n",
      "configfile: pytest.ini\n",
      "plugins: anyio-4.9.0\n",
      "collected 9 items                                                              \u001b[0m\u001b[1m\n",
      "\n",
      "test_collective_dag.py::test_all_reduce_duplicate_actors[ray_start_regular0] 2025-06-01 19:10:36,245\tINFO worker.py:1917 -- Started a local Ray instance.\n",
      "\u001b[32mPASSED\u001b[0m\n",
      "test_collective_dag.py::test_all_reduce_custom_comm_wrong_actors[ray_start_regular0] 2025-06-01 19:10:40,418\tINFO worker.py:1917 -- Started a local Ray instance.\n",
      "\u001b[32mPASSED\u001b[0m\n",
      "test_collective_dag.py::test_comm_all_reduces[ray_start_regular0] 2025-06-01 19:10:44,482\tINFO worker.py:1917 -- Started a local Ray instance.\n",
      "2025-06-01 19:10:47,184\tINFO dag_node_operation.py:1108 -- Writing compiled graph schedule visualization to compiled_graph_schedule.png\n",
      "2025-06-01 19:10:47,302\tINFO compiled_dag_node.py:2774 -- Tearing down compiled DAG\n",
      "2025-06-01 19:10:47,307\tINFO compiled_dag_node.py:2779 -- Cancelling compiled worker on actor: Actor(CPUTorchTensorWorker, 14fa243d12f6615107e62c2d01000000)\n",
      "2025-06-01 19:10:47,307\tINFO compiled_dag_node.py:2779 -- Cancelling compiled worker on actor: Actor(CPUTorchTensorWorker, a37165cd93e87521615a49aa01000000)\n",
      "2025-06-01 19:10:47,629\tINFO compiled_dag_node.py:2801 -- Waiting for worker tasks to exit\n",
      "2025-06-01 19:10:47,659\tINFO compiled_dag_node.py:2804 -- Teardown complete\n",
      "\u001b[32mPASSED\u001b[0m\n",
      "test_collective_dag.py::test_comm_deduplicate_all_reduces[ray_start_regular0] 2025-06-01 19:10:50,467\tINFO worker.py:1917 -- Started a local Ray instance.\n",
      "2025-06-01 19:10:52,771\tINFO dag_node_operation.py:1108 -- Writing compiled graph schedule visualization to compiled_graph_schedule.png\n",
      "2025-06-01 19:10:52,895\tINFO compiled_dag_node.py:2774 -- Tearing down compiled DAG\n",
      "2025-06-01 19:10:52,899\tINFO compiled_dag_node.py:2779 -- Cancelling compiled worker on actor: Actor(CPUTorchTensorWorker, 722f50cc68f6d8cdd352b23001000000)\n",
      "2025-06-01 19:10:52,899\tINFO compiled_dag_node.py:2779 -- Cancelling compiled worker on actor: Actor(CPUTorchTensorWorker, 47c8d9b620b8cc696152efbe01000000)\n",
      "2025-06-01 19:10:53,219\tINFO compiled_dag_node.py:2801 -- Waiting for worker tasks to exit\n",
      "2025-06-01 19:10:53,247\tINFO compiled_dag_node.py:2804 -- Teardown complete\n",
      "\u001b[32mPASSED\u001b[0m\n",
      "test_collective_dag.py::test_comm_deduplicate_p2p_and_collective[ray_start_regular0] 2025-06-01 19:10:56,214\tINFO worker.py:1917 -- Started a local Ray instance.\n",
      "2025-06-01 19:10:58,582\tINFO dag_node_operation.py:1108 -- Writing compiled graph schedule visualization to compiled_graph_schedule.png\n",
      "2025-06-01 19:10:58,768\tINFO compiled_dag_node.py:2774 -- Tearing down compiled DAG\n",
      "2025-06-01 19:10:58,772\tINFO compiled_dag_node.py:2779 -- Cancelling compiled worker on actor: Actor(CPUTorchTensorWorker, 67a0b8c7b85bfa81e6f32e2501000000)\n",
      "2025-06-01 19:10:58,772\tINFO compiled_dag_node.py:2779 -- Cancelling compiled worker on actor: Actor(CPUTorchTensorWorker, 624b2d533496d489c768241401000000)\n",
      "2025-06-01 19:10:59,099\tINFO compiled_dag_node.py:2801 -- Waiting for worker tasks to exit\n",
      "2025-06-01 19:10:59,126\tINFO compiled_dag_node.py:2804 -- Teardown complete\n",
      "2025-06-01 19:10:59,252\tINFO dag_node_operation.py:1108 -- Writing compiled graph schedule visualization to compiled_graph_schedule.png\n",
      "2025-06-01 19:10:59,413\tINFO compiled_dag_node.py:2774 -- Tearing down compiled DAG\n",
      "2025-06-01 19:10:59,416\tINFO compiled_dag_node.py:2779 -- Cancelling compiled worker on actor: Actor(CPUTorchTensorWorker, 67a0b8c7b85bfa81e6f32e2501000000)\n",
      "2025-06-01 19:10:59,417\tINFO compiled_dag_node.py:2779 -- Cancelling compiled worker on actor: Actor(CPUTorchTensorWorker, 624b2d533496d489c768241401000000)\n",
      "2025-06-01 19:10:59,428\tINFO compiled_dag_node.py:2801 -- Waiting for worker tasks to exit\n",
      "2025-06-01 19:10:59,428\tINFO compiled_dag_node.py:2804 -- Teardown complete\n",
      "\u001b[32mPASSED\u001b[0m\n",
      "test_collective_dag.py::test_custom_comm[ray_start_regular0] 2025-06-01 19:11:02,338\tINFO worker.py:1917 -- Started a local Ray instance.\n",
      "2025-06-01 19:11:04,647\tINFO dag_node_operation.py:1108 -- Writing compiled graph schedule visualization to compiled_graph_schedule.png\n",
      "2025-06-01 19:11:04,788\tINFO compiled_dag_node.py:2774 -- Tearing down compiled DAG\n",
      "2025-06-01 19:11:04,791\tINFO compiled_dag_node.py:2779 -- Cancelling compiled worker on actor: Actor(CPUTorchTensorWorker, c88eef5f8a2f970cf44fe16b01000000)\n",
      "2025-06-01 19:11:04,792\tINFO compiled_dag_node.py:2779 -- Cancelling compiled worker on actor: Actor(CPUTorchTensorWorker, 06546b1292e9f917211c2e2301000000)\n",
      "2025-06-01 19:11:05,109\tINFO compiled_dag_node.py:2801 -- Waiting for worker tasks to exit\n",
      "2025-06-01 19:11:05,138\tINFO compiled_dag_node.py:2804 -- Teardown complete\n",
      "2025-06-01 19:11:05,273\tINFO dag_node_operation.py:1108 -- Writing compiled graph schedule visualization to compiled_graph_schedule.png\n",
      "2025-06-01 19:11:05,414\tINFO compiled_dag_node.py:2774 -- Tearing down compiled DAG\n",
      "2025-06-01 19:11:05,418\tINFO compiled_dag_node.py:2779 -- Cancelling compiled worker on actor: Actor(CPUTorchTensorWorker, c88eef5f8a2f970cf44fe16b01000000)\n",
      "2025-06-01 19:11:05,418\tINFO compiled_dag_node.py:2779 -- Cancelling compiled worker on actor: Actor(CPUTorchTensorWorker, 06546b1292e9f917211c2e2301000000)\n",
      "2025-06-01 19:11:05,428\tINFO compiled_dag_node.py:2801 -- Waiting for worker tasks to exit\n",
      "2025-06-01 19:11:05,429\tINFO compiled_dag_node.py:2804 -- Teardown complete\n",
      "\u001b[32mPASSED\u001b[0m\n",
      "test_collective_dag.py::test_custom_comm_init_teardown[ray_start_regular0] 2025-06-01 19:11:09,211\tINFO worker.py:1917 -- Started a local Ray instance.\n",
      "2025-06-01 19:11:11,477\tINFO dag_node_operation.py:1108 -- Writing compiled graph schedule visualization to compiled_graph_schedule.png\n",
      "2025-06-01 19:11:11,640\tINFO compiled_dag_node.py:2774 -- Tearing down compiled DAG\n",
      "2025-06-01 19:11:11,645\tINFO compiled_dag_node.py:2779 -- Cancelling compiled worker on actor: Actor(CPUTorchTensorWorker, b8f1f10324e64b87bf08abfb01000000)\n",
      "2025-06-01 19:11:11,645\tINFO compiled_dag_node.py:2779 -- Cancelling compiled worker on actor: Actor(CPUTorchTensorWorker, 835fe2b26b6df134d41e939701000000)\n",
      "2025-06-01 19:11:11,959\tINFO compiled_dag_node.py:2801 -- Waiting for worker tasks to exit\n",
      "2025-06-01 19:11:11,993\tINFO compiled_dag_node.py:2804 -- Teardown complete\n",
      "2025-06-01 19:11:12,133\tINFO dag_node_operation.py:1108 -- Writing compiled graph schedule visualization to compiled_graph_schedule.png\n",
      "2025-06-01 19:11:12,314\tINFO compiled_dag_node.py:2774 -- Tearing down compiled DAG\n",
      "2025-06-01 19:11:12,317\tINFO compiled_dag_node.py:2779 -- Cancelling compiled worker on actor: Actor(CPUTorchTensorWorker, b8f1f10324e64b87bf08abfb01000000)\n",
      "2025-06-01 19:11:12,317\tINFO compiled_dag_node.py:2779 -- Cancelling compiled worker on actor: Actor(CPUTorchTensorWorker, 835fe2b26b6df134d41e939701000000)\n",
      "2025-06-01 19:11:12,327\tINFO compiled_dag_node.py:2801 -- Waiting for worker tasks to exit\n",
      "2025-06-01 19:11:12,327\tINFO compiled_dag_node.py:2804 -- Teardown complete\n",
      "\u001b[32mPASSED\u001b[0m\n",
      "test_collective_dag.py::test_exec_schedules_ddp[2-ray_start_regular0] 2025-06-01 19:11:15,095\tINFO worker.py:1917 -- Started a local Ray instance.\n",
      "2025-06-01 19:11:15,760\tINFO torch_tensor_nccl_channel.py:770 -- Initializing custom NCCL group 4d0d5d52-c80a-4e09-8c2e-2c1541998f24 on actors: [Actor(DDPWorker, 8e785575252c085cccb3228601000000), Actor(DDPWorker, 333adf0e51eac6f47a2ad2bf01000000)]\n",
      "2025-06-01 19:11:17,410\tINFO torch_tensor_nccl_channel.py:797 -- NCCL group initialized.\n",
      "2025-06-01 19:11:17,540\tINFO dag_node_operation.py:1108 -- Writing compiled graph schedule visualization to compiled_graph_schedule.png\n",
      "\u001b[32mPASSED\u001b[0m2025-06-01 19:11:17,714\tINFO compiled_dag_node.py:2774 -- Tearing down compiled DAG\n",
      "2025-06-01 19:11:17,721\tINFO compiled_dag_node.py:2779 -- Cancelling compiled worker on actor: Actor(DDPWorker, 8e785575252c085cccb3228601000000)\n",
      "2025-06-01 19:11:17,721\tINFO compiled_dag_node.py:2779 -- Cancelling compiled worker on actor: Actor(DDPWorker, 333adf0e51eac6f47a2ad2bf01000000)\n",
      "2025-06-01 19:11:17,730\tINFO compiled_dag_node.py:2801 -- Waiting for worker tasks to exit\n",
      "2025-06-01 19:11:17,730\tINFO compiled_dag_node.py:2762 -- Killing actor: Actor(DDPWorker, 8e785575252c085cccb3228601000000)\n",
      "2025-06-01 19:11:17,731\tINFO compiled_dag_node.py:2762 -- Killing actor: Actor(DDPWorker, 333adf0e51eac6f47a2ad2bf01000000)\n",
      "2025-06-01 19:11:17,731\tINFO compiled_dag_node.py:2804 -- Teardown complete\n",
      "\n",
      "test_collective_dag.py::test_exec_schedules_ddp[4-ray_start_regular0] 2025-06-01 19:11:20,464\tINFO worker.py:1917 -- Started a local Ray instance.\n",
      "2025-06-01 19:11:21,493\tINFO torch_tensor_nccl_channel.py:770 -- Initializing custom NCCL group 868abf7e-b2e5-4de9-95da-66b7b1cb2779 on actors: [Actor(DDPWorker, 8b3d047c5009e530ccd575f701000000), Actor(DDPWorker, 088fb1e2b841fd5c4f7985f601000000), Actor(DDPWorker, aa456a2cfa027b7958bc450001000000), Actor(DDPWorker, 34726f1377f04809f8f9199f01000000)]\n",
      "2025-06-01 19:11:23,177\tINFO torch_tensor_nccl_channel.py:797 -- NCCL group initialized.\n",
      "2025-06-01 19:11:23,498\tINFO dag_node_operation.py:1108 -- Writing compiled graph schedule visualization to compiled_graph_schedule.png\n",
      "\u001b[32mPASSED\u001b[0m2025-06-01 19:11:23,736\tINFO compiled_dag_node.py:2774 -- Tearing down compiled DAG\n",
      "2025-06-01 19:11:23,749\tINFO compiled_dag_node.py:2779 -- Cancelling compiled worker on actor: Actor(DDPWorker, 8b3d047c5009e530ccd575f701000000)\n",
      "2025-06-01 19:11:23,749\tINFO compiled_dag_node.py:2779 -- Cancelling compiled worker on actor: Actor(DDPWorker, 088fb1e2b841fd5c4f7985f601000000)\n",
      "2025-06-01 19:11:23,749\tINFO compiled_dag_node.py:2779 -- Cancelling compiled worker on actor: Actor(DDPWorker, aa456a2cfa027b7958bc450001000000)\n",
      "2025-06-01 19:11:23,749\tINFO compiled_dag_node.py:2779 -- Cancelling compiled worker on actor: Actor(DDPWorker, 34726f1377f04809f8f9199f01000000)\n",
      "2025-06-01 19:11:23,762\tINFO compiled_dag_node.py:2801 -- Waiting for worker tasks to exit\n",
      "2025-06-01 19:11:23,763\tINFO compiled_dag_node.py:2762 -- Killing actor: Actor(DDPWorker, 8b3d047c5009e530ccd575f701000000)\n",
      "2025-06-01 19:11:23,763\tINFO compiled_dag_node.py:2762 -- Killing actor: Actor(DDPWorker, 088fb1e2b841fd5c4f7985f601000000)\n",
      "2025-06-01 19:11:23,763\tINFO compiled_dag_node.py:2762 -- Killing actor: Actor(DDPWorker, aa456a2cfa027b7958bc450001000000)\n",
      "2025-06-01 19:11:23,763\tINFO compiled_dag_node.py:2762 -- Killing actor: Actor(DDPWorker, 34726f1377f04809f8f9199f01000000)\n",
      "2025-06-01 19:11:23,764\tINFO compiled_dag_node.py:2804 -- Teardown complete\n",
      "\n",
      "\n",
      "\u001b[32m============================== \u001b[32m\u001b[1m9 passed\u001b[0m\u001b[32m in 53.21s\u001b[0m\u001b[32m ==============================\u001b[0m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!python -m pytest -v -s test_collective_dag.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m============================= test session starts ==============================\u001b[0m\n",
      "platform linux -- Python 3.13.3, pytest-8.3.5, pluggy-1.5.0 -- /home/wxdeng/miniconda3/envs/stbr-unify/bin/python\n",
      "cachedir: .pytest_cache\n",
      "rootdir: /home/wxdeng/stbr-min\n",
      "configfile: pytest.ini\n",
      "plugins: anyio-4.9.0\n",
      "collected 53 items                                                             \u001b[0m\u001b[1m\n",
      "\n",
      "test_torch_tensor_dag.py::test_torch_tensor_p2p[ray_start_regular0] 2025-06-01 19:11:30,839\tINFO worker.py:1917 -- Started a local Ray instance.\n",
      "2025-06-01 19:11:33,161\tINFO torch_tensor_nccl_channel.py:772 -- Creating NCCL group 39ed74d2-8bd3-46ed-8cfd-222bba1516ed on actors: [Actor(TorchTensorWorker, 98686c96712db28145774acf01000000), Actor(TorchTensorWorker, bdd5e945cb2babe2403a4f5101000000)]\n",
      "2025-06-01 19:11:34,238\tINFO torch_tensor_nccl_channel.py:797 -- NCCL group initialized.\n",
      "2025-06-01 19:11:34,375\tINFO dag_node_operation.py:1108 -- Writing compiled graph schedule visualization to compiled_graph_schedule.png\n",
      "\u001b[32mPASSED\u001b[0m2025-06-01 19:11:34,561\tINFO compiled_dag_node.py:2774 -- Tearing down compiled DAG\n",
      "2025-06-01 19:11:34,561\tINFO compiled_dag_node.py:2779 -- Cancelling compiled worker on actor: Actor(TorchTensorWorker, 98686c96712db28145774acf01000000)\n",
      "2025-06-01 19:11:34,562\tINFO compiled_dag_node.py:2779 -- Cancelling compiled worker on actor: Actor(TorchTensorWorker, bdd5e945cb2babe2403a4f5101000000)\n",
      "\u001b[36m(TorchTensorWorker pid=14527)\u001b[0m Destructing NCCL group on actor: Actor(TorchTensorWorker, 98686c96712db28145774acf01000000)\n",
      "\u001b[36m(TorchTensorWorker pid=14529)\u001b[0m Destructing NCCL group on actor: Actor(TorchTensorWorker, bdd5e945cb2babe2403a4f5101000000)\n",
      "2025-06-01 19:11:35,079\tINFO compiled_dag_node.py:2801 -- Waiting for worker tasks to exit\n",
      "2025-06-01 19:11:35,079\tINFO compiled_dag_node.py:2762 -- Killing actor: Actor(TorchTensorWorker, 98686c96712db28145774acf01000000)\n",
      "2025-06-01 19:11:35,079\tINFO compiled_dag_node.py:2762 -- Killing actor: Actor(TorchTensorWorker, bdd5e945cb2babe2403a4f5101000000)\n",
      "2025-06-01 19:11:35,079\tINFO compiled_dag_node.py:2804 -- Teardown complete\n",
      "\n",
      "test_torch_tensor_dag.py::test_torch_tensor_as_dag_input[ray_start_regular0] 2025-06-01 19:11:39,161\tINFO worker.py:1917 -- Started a local Ray instance.\n",
      "2025-06-01 19:11:41,005\tINFO dag_node_operation.py:1108 -- Writing compiled graph schedule visualization to compiled_graph_schedule.png\n",
      "\u001b[32mPASSED\u001b[0m2025-06-01 19:11:41,293\tINFO compiled_dag_node.py:2774 -- Tearing down compiled DAG\n",
      "2025-06-01 19:11:41,293\tINFO compiled_dag_node.py:2779 -- Cancelling compiled worker on actor: Actor(TorchTensorWorker, 96654a4bb97636ab5caa85f401000000)\n",
      "2025-06-01 19:11:41,297\tINFO compiled_dag_node.py:2801 -- Waiting for worker tasks to exit\n",
      "2025-06-01 19:11:41,297\tINFO compiled_dag_node.py:2762 -- Killing actor: Actor(TorchTensorWorker, 96654a4bb97636ab5caa85f401000000)\n",
      "2025-06-01 19:11:41,297\tINFO compiled_dag_node.py:2804 -- Teardown complete\n",
      "\n",
      "test_torch_tensor_dag.py::test_torch_tensor_nccl[False-False-ray_start_regular0] 2025-06-01 19:11:44,945\tINFO worker.py:1917 -- Started a local Ray instance.\n",
      "2025-06-01 19:11:47,187\tINFO torch_tensor_nccl_channel.py:772 -- Creating NCCL group d938b207-abb6-4325-8041-a42d0966ff98 on actors: [Actor(TorchTensorWorker, 685724546446d8b10f30d56701000000), Actor(TorchTensorWorker, c0e96b98464d189485bfbdd601000000)]\n",
      "2025-06-01 19:11:48,273\tINFO torch_tensor_nccl_channel.py:797 -- NCCL group initialized.\n",
      "2025-06-01 19:11:48,438\tINFO dag_node_operation.py:1108 -- Writing compiled graph schedule visualization to compiled_graph_schedule.png\n",
      "2025-06-01 19:11:48,631\tINFO compiled_dag_node.py:2774 -- Tearing down compiled DAG\n",
      "2025-06-01 19:11:48,631\tINFO compiled_dag_node.py:2779 -- Cancelling compiled worker on actor: Actor(TorchTensorWorker, 685724546446d8b10f30d56701000000)\n",
      "2025-06-01 19:11:48,631\tINFO compiled_dag_node.py:2779 -- Cancelling compiled worker on actor: Actor(TorchTensorWorker, c0e96b98464d189485bfbdd601000000)\n",
      "\u001b[36m(TorchTensorWorker pid=15992)\u001b[0m Destructing NCCL group on actor: Actor(TorchTensorWorker, c0e96b98464d189485bfbdd601000000)\n",
      "\u001b[36m(TorchTensorWorker pid=15991)\u001b[0m Destructing NCCL group on actor: Actor(TorchTensorWorker, 685724546446d8b10f30d56701000000)\n",
      "2025-06-01 19:11:49,148\tINFO compiled_dag_node.py:2801 -- Waiting for worker tasks to exit\n",
      "2025-06-01 19:11:49,148\tINFO compiled_dag_node.py:2804 -- Teardown complete\n",
      "2025-06-01 19:11:49,158\tINFO torch_tensor_nccl_channel.py:772 -- Creating NCCL group abe57098-f8ae-4d4c-8760-6de6ef072559 on actors: [Actor(TorchTensorWorker, 685724546446d8b10f30d56701000000), Actor(TorchTensorWorker, c0e96b98464d189485bfbdd601000000)]\n",
      "2025-06-01 19:11:49,190\tINFO torch_tensor_nccl_channel.py:797 -- NCCL group initialized.\n",
      "2025-06-01 19:11:49,326\tINFO dag_node_operation.py:1108 -- Writing compiled graph schedule visualization to compiled_graph_schedule.png\n",
      "\u001b[32mPASSED\u001b[0m2025-06-01 19:11:49,477\tINFO compiled_dag_node.py:2774 -- Tearing down compiled DAG\n",
      "2025-06-01 19:11:49,478\tINFO compiled_dag_node.py:2779 -- Cancelling compiled worker on actor: Actor(TorchTensorWorker, 685724546446d8b10f30d56701000000)\n",
      "2025-06-01 19:11:49,478\tINFO compiled_dag_node.py:2779 -- Cancelling compiled worker on actor: Actor(TorchTensorWorker, c0e96b98464d189485bfbdd601000000)\n",
      "\u001b[36m(TorchTensorWorker pid=15992)\u001b[0m Destructing NCCL group on actor: Actor(TorchTensorWorker, c0e96b98464d189485bfbdd601000000)\n",
      "\u001b[36m(TorchTensorWorker pid=15991)\u001b[0m Destructing NCCL group on actor: Actor(TorchTensorWorker, 685724546446d8b10f30d56701000000)\n",
      "\u001b[36m(pid=gcs_server)\u001b[0m [2025-06-01 19:11:49,505 E 15687 15687] (gcs_server) gcs_actor_manager.cc:1788: Failed to kill actor d52a7f27e144cffe6f404a4801000000, status: RpcError: RPC Error message: Socket closed; RPC Error details:  rpc_code: 14\n",
      "2025-06-01 19:11:49,995\tINFO compiled_dag_node.py:2801 -- Waiting for worker tasks to exit\n",
      "2025-06-01 19:11:49,995\tINFO compiled_dag_node.py:2762 -- Killing actor: Actor(TorchTensorWorker, 685724546446d8b10f30d56701000000)\n",
      "2025-06-01 19:11:49,996\tINFO compiled_dag_node.py:2762 -- Killing actor: Actor(TorchTensorWorker, c0e96b98464d189485bfbdd601000000)\n",
      "2025-06-01 19:11:49,996\tINFO compiled_dag_node.py:2804 -- Teardown complete\n",
      "\n",
      "test_torch_tensor_dag.py::test_torch_tensor_nccl[False-True-ray_start_regular0] 2025-06-01 19:11:53,151\tINFO worker.py:1917 -- Started a local Ray instance.\n",
      "2025-06-01 19:11:55,451\tINFO torch_tensor_nccl_channel.py:772 -- Creating NCCL group 2d49d94e-8d74-495f-93d8-51d353fd26c2 on actors: [Actor(TorchTensorWorker, 9c4f788683398e77c4b1863401000000), Actor(TorchTensorWorker, 01f9a967bb2fb6fa6e482ffd01000000)]\n",
      "2025-06-01 19:11:56,532\tINFO torch_tensor_nccl_channel.py:797 -- NCCL group initialized.\n",
      "2025-06-01 19:11:56,703\tINFO dag_node_operation.py:1108 -- Writing compiled graph schedule visualization to compiled_graph_schedule.png\n",
      "2025-06-01 19:11:56,899\tINFO compiled_dag_node.py:2774 -- Tearing down compiled DAG\n",
      "2025-06-01 19:11:56,899\tINFO compiled_dag_node.py:2779 -- Cancelling compiled worker on actor: Actor(TorchTensorWorker, 9c4f788683398e77c4b1863401000000)\n",
      "2025-06-01 19:11:56,899\tINFO compiled_dag_node.py:2779 -- Cancelling compiled worker on actor: Actor(TorchTensorWorker, 01f9a967bb2fb6fa6e482ffd01000000)\n",
      "\u001b[36m(TorchTensorWorker pid=16725)\u001b[0m Destructing NCCL group on actor: Actor(TorchTensorWorker, 9c4f788683398e77c4b1863401000000)\n",
      "\u001b[36m(TorchTensorWorker pid=16727)\u001b[0m Destructing NCCL group on actor: Actor(TorchTensorWorker, 01f9a967bb2fb6fa6e482ffd01000000)\n",
      "2025-06-01 19:11:57,416\tINFO compiled_dag_node.py:2801 -- Waiting for worker tasks to exit\n",
      "2025-06-01 19:11:57,416\tINFO compiled_dag_node.py:2804 -- Teardown complete\n",
      "2025-06-01 19:11:57,427\tINFO torch_tensor_nccl_channel.py:772 -- Creating NCCL group dd64ccde-bd2b-46c8-beb7-db33293addfd on actors: [Actor(TorchTensorWorker, 9c4f788683398e77c4b1863401000000), Actor(TorchTensorWorker, 01f9a967bb2fb6fa6e482ffd01000000)]\n",
      "2025-06-01 19:11:57,457\tINFO torch_tensor_nccl_channel.py:797 -- NCCL group initialized.\n",
      "2025-06-01 19:11:57,580\tINFO dag_node_operation.py:1108 -- Writing compiled graph schedule visualization to compiled_graph_schedule.png\n",
      "\u001b[32mPASSED\u001b[0m2025-06-01 19:11:57,737\tINFO compiled_dag_node.py:2774 -- Tearing down compiled DAG\n",
      "2025-06-01 19:11:57,738\tINFO compiled_dag_node.py:2779 -- Cancelling compiled worker on actor: Actor(TorchTensorWorker, 9c4f788683398e77c4b1863401000000)\n",
      "2025-06-01 19:11:57,738\tINFO compiled_dag_node.py:2779 -- Cancelling compiled worker on actor: Actor(TorchTensorWorker, 01f9a967bb2fb6fa6e482ffd01000000)\n",
      "\u001b[36m(TorchTensorWorker pid=16725)\u001b[0m Destructing NCCL group on actor: Actor(TorchTensorWorker, 9c4f788683398e77c4b1863401000000)\n",
      "\u001b[36m(TorchTensorWorker pid=16727)\u001b[0m Destructing NCCL group on actor: Actor(TorchTensorWorker, 01f9a967bb2fb6fa6e482ffd01000000)\n",
      "\u001b[36m(pid=gcs_server)\u001b[0m [2025-06-01 19:11:57,765 E 16449 16449] (gcs_server) gcs_actor_manager.cc:1788: Failed to kill actor 47859baec93e2ab2e86d547801000000, status: RpcError: RPC Error message: Socket closed; RPC Error details:  rpc_code: 14\n",
      "2025-06-01 19:11:58,255\tINFO compiled_dag_node.py:2801 -- Waiting for worker tasks to exit\n",
      "2025-06-01 19:11:58,255\tINFO compiled_dag_node.py:2762 -- Killing actor: Actor(TorchTensorWorker, 9c4f788683398e77c4b1863401000000)\n",
      "2025-06-01 19:11:58,256\tINFO compiled_dag_node.py:2762 -- Killing actor: Actor(TorchTensorWorker, 01f9a967bb2fb6fa6e482ffd01000000)\n",
      "2025-06-01 19:11:58,256\tINFO compiled_dag_node.py:2804 -- Teardown complete\n",
      "\n",
      "test_torch_tensor_dag.py::test_torch_tensor_nccl[True-False-ray_start_regular0] 2025-06-01 19:12:01,263\tINFO worker.py:1917 -- Started a local Ray instance.\n",
      "2025-06-01 19:12:03,519\tINFO torch_tensor_nccl_channel.py:772 -- Creating NCCL group 5067c32a-f3da-4cfc-b940-ca402b01b681 on actors: [Actor(TorchTensorWorker, fa596c6ef1bd016f1c6a382a01000000), Actor(TorchTensorWorker, 318993910d76e5e5f6cad70401000000)]\n",
      "2025-06-01 19:12:04,603\tINFO torch_tensor_nccl_channel.py:797 -- NCCL group initialized.\n",
      "2025-06-01 19:12:04,781\tINFO dag_node_operation.py:1108 -- Writing compiled graph schedule visualization to compiled_graph_schedule.png\n",
      "2025-06-01 19:12:04,971\tINFO compiled_dag_node.py:2774 -- Tearing down compiled DAG\n",
      "2025-06-01 19:12:04,972\tINFO compiled_dag_node.py:2779 -- Cancelling compiled worker on actor: Actor(TorchTensorWorker, 318993910d76e5e5f6cad70401000000)\n",
      "2025-06-01 19:12:04,972\tINFO compiled_dag_node.py:2779 -- Cancelling compiled worker on actor: Actor(TorchTensorWorker, fa596c6ef1bd016f1c6a382a01000000)\n",
      "\u001b[36m(TorchTensorWorker pid=17485)\u001b[0m Destructing NCCL group on actor: Actor(TorchTensorWorker, fa596c6ef1bd016f1c6a382a01000000)\n",
      "\u001b[36m(TorchTensorWorker pid=17484)\u001b[0m Destructing NCCL group on actor: Actor(TorchTensorWorker, 318993910d76e5e5f6cad70401000000)\n",
      "2025-06-01 19:12:05,489\tINFO compiled_dag_node.py:2801 -- Waiting for worker tasks to exit\n",
      "2025-06-01 19:12:05,490\tINFO compiled_dag_node.py:2804 -- Teardown complete\n",
      "2025-06-01 19:12:05,500\tINFO torch_tensor_nccl_channel.py:772 -- Creating NCCL group 45013c34-b3c7-4677-84a1-22cee93cc668 on actors: [Actor(TorchTensorWorker, fa596c6ef1bd016f1c6a382a01000000), Actor(TorchTensorWorker, 318993910d76e5e5f6cad70401000000)]\n",
      "2025-06-01 19:12:05,533\tINFO torch_tensor_nccl_channel.py:797 -- NCCL group initialized.\n",
      "2025-06-01 19:12:05,667\tINFO dag_node_operation.py:1108 -- Writing compiled graph schedule visualization to compiled_graph_schedule.png\n",
      "\u001b[32mPASSED\u001b[0m2025-06-01 19:12:05,821\tINFO compiled_dag_node.py:2774 -- Tearing down compiled DAG\n",
      "2025-06-01 19:12:05,821\tINFO compiled_dag_node.py:2779 -- Cancelling compiled worker on actor: Actor(TorchTensorWorker, 318993910d76e5e5f6cad70401000000)\n",
      "2025-06-01 19:12:05,821\tINFO compiled_dag_node.py:2779 -- Cancelling compiled worker on actor: Actor(TorchTensorWorker, fa596c6ef1bd016f1c6a382a01000000)\n",
      "\u001b[36m(TorchTensorWorker pid=17485)\u001b[0m Destructing NCCL group on actor: Actor(TorchTensorWorker, fa596c6ef1bd016f1c6a382a01000000)\n",
      "\u001b[36m(TorchTensorWorker pid=17484)\u001b[0m Destructing NCCL group on actor: Actor(TorchTensorWorker, 318993910d76e5e5f6cad70401000000)\n",
      "\u001b[36m(pid=gcs_server)\u001b[0m [2025-06-01 19:12:05,846 E 17234 17234] (gcs_server) gcs_actor_manager.cc:1788: Failed to kill actor d9927eff0b4735dd8a40ea5401000000, status: RpcError: RPC Error message: Socket closed; RPC Error details:  rpc_code: 14\n",
      "2025-06-01 19:12:06,324\tINFO compiled_dag_node.py:2801 -- Waiting for worker tasks to exit\n",
      "2025-06-01 19:12:06,324\tINFO compiled_dag_node.py:2762 -- Killing actor: Actor(TorchTensorWorker, 318993910d76e5e5f6cad70401000000)\n",
      "2025-06-01 19:12:06,325\tINFO compiled_dag_node.py:2762 -- Killing actor: Actor(TorchTensorWorker, fa596c6ef1bd016f1c6a382a01000000)\n",
      "2025-06-01 19:12:06,325\tINFO compiled_dag_node.py:2804 -- Teardown complete\n",
      "\n",
      "test_torch_tensor_dag.py::test_torch_tensor_nccl[True-True-ray_start_regular0] 2025-06-01 19:12:09,357\tINFO worker.py:1917 -- Started a local Ray instance.\n",
      "2025-06-01 19:12:11,601\tINFO torch_tensor_nccl_channel.py:772 -- Creating NCCL group eaa0c2e8-3eee-4d7b-8c11-35874f8ab1b7 on actors: [Actor(TorchTensorWorker, 3aea55fcfb0bc3cf75aeca3501000000), Actor(TorchTensorWorker, 1955b14cccdb4529fe1a804801000000)]\n",
      "2025-06-01 19:12:12,688\tINFO torch_tensor_nccl_channel.py:797 -- NCCL group initialized.\n",
      "2025-06-01 19:12:12,864\tINFO dag_node_operation.py:1108 -- Writing compiled graph schedule visualization to compiled_graph_schedule.png\n",
      "2025-06-01 19:12:13,054\tINFO compiled_dag_node.py:2774 -- Tearing down compiled DAG\n",
      "2025-06-01 19:12:13,054\tINFO compiled_dag_node.py:2779 -- Cancelling compiled worker on actor: Actor(TorchTensorWorker, 3aea55fcfb0bc3cf75aeca3501000000)\n",
      "2025-06-01 19:12:13,054\tINFO compiled_dag_node.py:2779 -- Cancelling compiled worker on actor: Actor(TorchTensorWorker, 1955b14cccdb4529fe1a804801000000)\n",
      "\u001b[36m(TorchTensorWorker pid=18268)\u001b[0m Destructing NCCL group on actor: Actor(TorchTensorWorker, 3aea55fcfb0bc3cf75aeca3501000000)\n",
      "\u001b[36m(TorchTensorWorker pid=18269)\u001b[0m Destructing NCCL group on actor: Actor(TorchTensorWorker, 1955b14cccdb4529fe1a804801000000)\n",
      "2025-06-01 19:12:13,550\tINFO compiled_dag_node.py:2801 -- Waiting for worker tasks to exit\n",
      "2025-06-01 19:12:13,550\tINFO compiled_dag_node.py:2804 -- Teardown complete\n",
      "2025-06-01 19:12:13,562\tINFO torch_tensor_nccl_channel.py:772 -- Creating NCCL group c89fb67e-0222-4d5a-b7ba-c5ec9f825b05 on actors: [Actor(TorchTensorWorker, 3aea55fcfb0bc3cf75aeca3501000000), Actor(TorchTensorWorker, 1955b14cccdb4529fe1a804801000000)]\n",
      "2025-06-01 19:12:13,594\tINFO torch_tensor_nccl_channel.py:797 -- NCCL group initialized.\n",
      "2025-06-01 19:12:13,726\tINFO dag_node_operation.py:1108 -- Writing compiled graph schedule visualization to compiled_graph_schedule.png\n",
      "\u001b[32mPASSED\u001b[0m2025-06-01 19:12:13,862\tINFO compiled_dag_node.py:2774 -- Tearing down compiled DAG\n",
      "2025-06-01 19:12:13,863\tINFO compiled_dag_node.py:2779 -- Cancelling compiled worker on actor: Actor(TorchTensorWorker, 3aea55fcfb0bc3cf75aeca3501000000)\n",
      "2025-06-01 19:12:13,863\tINFO compiled_dag_node.py:2779 -- Cancelling compiled worker on actor: Actor(TorchTensorWorker, 1955b14cccdb4529fe1a804801000000)\n",
      "\u001b[36m(TorchTensorWorker pid=18268)\u001b[0m Destructing NCCL group on actor: Actor(TorchTensorWorker, 3aea55fcfb0bc3cf75aeca3501000000)\n",
      "\u001b[36m(TorchTensorWorker pid=18269)\u001b[0m Destructing NCCL group on actor: Actor(TorchTensorWorker, 1955b14cccdb4529fe1a804801000000)\n",
      "\u001b[36m(pid=gcs_server)\u001b[0m [2025-06-01 19:12:13,886 E 17997 17997] (gcs_server) gcs_actor_manager.cc:1788: Failed to kill actor 964f68d11ad1468b7298a65801000000, status: RpcError: RPC Error message: Socket closed; RPC Error details:  rpc_code: 14\n",
      "2025-06-01 19:12:14,364\tINFO compiled_dag_node.py:2801 -- Waiting for worker tasks to exit\n",
      "2025-06-01 19:12:14,365\tINFO compiled_dag_node.py:2762 -- Killing actor: Actor(TorchTensorWorker, 3aea55fcfb0bc3cf75aeca3501000000)\n",
      "2025-06-01 19:12:14,365\tINFO compiled_dag_node.py:2762 -- Killing actor: Actor(TorchTensorWorker, 1955b14cccdb4529fe1a804801000000)\n",
      "2025-06-01 19:12:14,365\tINFO compiled_dag_node.py:2804 -- Teardown complete\n",
      "\n",
      "test_torch_tensor_dag.py::test_torch_tensor_shm[ray_start_regular0] 2025-06-01 19:12:17,470\tINFO worker.py:1917 -- Started a local Ray instance.\n",
      "2025-06-01 19:12:19,548\tINFO dag_node_operation.py:1108 -- Writing compiled graph schedule visualization to compiled_graph_schedule.png\n",
      "2025-06-01 19:12:20,465\tINFO compiled_dag_node.py:2774 -- Tearing down compiled DAG\n",
      "2025-06-01 19:12:20,465\tINFO compiled_dag_node.py:2779 -- Cancelling compiled worker on actor: Actor(TorchTensorWorker, 0c7416c33d99da9b99ea805401000000)\n",
      "2025-06-01 19:12:20,465\tINFO compiled_dag_node.py:2779 -- Cancelling compiled worker on actor: Actor(TorchTensorWorker, 751fb89f71a616eac457dc1501000000)\n",
      "2025-06-01 19:12:20,471\tINFO compiled_dag_node.py:2801 -- Waiting for worker tasks to exit\n",
      "2025-06-01 19:12:20,471\tINFO compiled_dag_node.py:2804 -- Teardown complete\n",
      "2025-06-01 19:12:20,529\tINFO dag_node_operation.py:1108 -- Writing compiled graph schedule visualization to compiled_graph_schedule.png\n",
      "2025-06-01 19:12:20,644\tINFO compiled_dag_node.py:2774 -- Tearing down compiled DAG\n",
      "2025-06-01 19:12:20,644\tINFO compiled_dag_node.py:2779 -- Cancelling compiled worker on actor: Actor(TorchTensorWorker, 0c7416c33d99da9b99ea805401000000)\n",
      "2025-06-01 19:12:20,644\tINFO compiled_dag_node.py:2779 -- Cancelling compiled worker on actor: Actor(TorchTensorWorker, 751fb89f71a616eac457dc1501000000)\n",
      "2025-06-01 19:12:20,649\tINFO compiled_dag_node.py:2801 -- Waiting for worker tasks to exit\n",
      "2025-06-01 19:12:20,650\tINFO compiled_dag_node.py:2804 -- Teardown complete\n",
      "\u001b[32mPASSED\u001b[0m\n",
      "test_torch_tensor_dag.py::test_torch_tensor_auto[num_gpus0-ray_start_regular0] 2025-06-01 19:12:23,547\tINFO worker.py:1917 -- Started a local Ray instance.\n",
      "2025-06-01 19:12:25,629\tINFO dag_node_operation.py:1108 -- Writing compiled graph schedule visualization to compiled_graph_schedule.png\n",
      "2025-06-01 19:12:25,753\tINFO compiled_dag_node.py:2774 -- Tearing down compiled DAG\n",
      "2025-06-01 19:12:25,754\tINFO compiled_dag_node.py:2779 -- Cancelling compiled worker on actor: Actor(TorchTensorWorker, 176d03774a9f552fb2e0870e01000000)\n",
      "2025-06-01 19:12:25,754\tINFO compiled_dag_node.py:2779 -- Cancelling compiled worker on actor: Actor(TorchTensorWorker, 65dcd81682e4cbd6a07e873a01000000)\n",
      "2025-06-01 19:12:25,761\tINFO compiled_dag_node.py:2801 -- Waiting for worker tasks to exit\n",
      "2025-06-01 19:12:25,761\tINFO compiled_dag_node.py:2804 -- Teardown complete\n",
      "2025-06-01 19:12:25,858\tINFO dag_node_operation.py:1108 -- Writing compiled graph schedule visualization to compiled_graph_schedule.png\n",
      "\u001b[32mPASSED\u001b[0m2025-06-01 19:12:25,983\tINFO compiled_dag_node.py:2774 -- Tearing down compiled DAG\n",
      "2025-06-01 19:12:25,984\tINFO compiled_dag_node.py:2779 -- Cancelling compiled worker on actor: Actor(TorchTensorWorker, 176d03774a9f552fb2e0870e01000000)\n",
      "2025-06-01 19:12:25,984\tINFO compiled_dag_node.py:2779 -- Cancelling compiled worker on actor: Actor(TorchTensorWorker, 65dcd81682e4cbd6a07e873a01000000)\n",
      "2025-06-01 19:12:25,991\tINFO compiled_dag_node.py:2801 -- Waiting for worker tasks to exit\n",
      "2025-06-01 19:12:25,992\tINFO compiled_dag_node.py:2762 -- Killing actor: Actor(TorchTensorWorker, 176d03774a9f552fb2e0870e01000000)\n",
      "2025-06-01 19:12:25,992\tINFO compiled_dag_node.py:2762 -- Killing actor: Actor(TorchTensorWorker, 65dcd81682e4cbd6a07e873a01000000)\n",
      "2025-06-01 19:12:25,992\tINFO compiled_dag_node.py:2804 -- Teardown complete\n",
      "\n",
      "test_torch_tensor_dag.py::test_torch_tensor_auto[num_gpus1-ray_start_regular0] 2025-06-01 19:12:29,856\tINFO worker.py:1917 -- Started a local Ray instance.\n",
      "2025-06-01 19:12:31,936\tINFO dag_node_operation.py:1108 -- Writing compiled graph schedule visualization to compiled_graph_schedule.png\n",
      "2025-06-01 19:12:32,456\tINFO compiled_dag_node.py:2774 -- Tearing down compiled DAG\n",
      "2025-06-01 19:12:32,457\tINFO compiled_dag_node.py:2779 -- Cancelling compiled worker on actor: Actor(TorchTensorWorker, 2036003788f3196b15186c3401000000)\n",
      "2025-06-01 19:12:32,457\tINFO compiled_dag_node.py:2779 -- Cancelling compiled worker on actor: Actor(TorchTensorWorker, aa992b17ede11666d3aab17101000000)\n",
      "2025-06-01 19:12:32,463\tINFO compiled_dag_node.py:2801 -- Waiting for worker tasks to exit\n",
      "2025-06-01 19:12:32,463\tINFO compiled_dag_node.py:2804 -- Teardown complete\n",
      "2025-06-01 19:12:32,547\tINFO dag_node_operation.py:1108 -- Writing compiled graph schedule visualization to compiled_graph_schedule.png\n",
      "\u001b[32mPASSED\u001b[0m2025-06-01 19:12:32,666\tINFO compiled_dag_node.py:2774 -- Tearing down compiled DAG\n",
      "2025-06-01 19:12:32,667\tINFO compiled_dag_node.py:2779 -- Cancelling compiled worker on actor: Actor(TorchTensorWorker, 2036003788f3196b15186c3401000000)\n",
      "2025-06-01 19:12:32,667\tINFO compiled_dag_node.py:2779 -- Cancelling compiled worker on actor: Actor(TorchTensorWorker, aa992b17ede11666d3aab17101000000)\n",
      "2025-06-01 19:12:32,673\tINFO compiled_dag_node.py:2801 -- Waiting for worker tasks to exit\n",
      "2025-06-01 19:12:32,674\tINFO compiled_dag_node.py:2762 -- Killing actor: Actor(TorchTensorWorker, 2036003788f3196b15186c3401000000)\n",
      "2025-06-01 19:12:32,674\tINFO compiled_dag_node.py:2762 -- Killing actor: Actor(TorchTensorWorker, aa992b17ede11666d3aab17101000000)\n",
      "2025-06-01 19:12:32,674\tINFO compiled_dag_node.py:2804 -- Teardown complete\n",
      "\n",
      "test_torch_tensor_dag.py::test_torch_tensor_auto[num_gpus2-ray_start_regular0] 2025-06-01 19:12:35,651\tINFO worker.py:1917 -- Started a local Ray instance.\n",
      "2025-06-01 19:12:37,727\tINFO dag_node_operation.py:1108 -- Writing compiled graph schedule visualization to compiled_graph_schedule.png\n",
      "2025-06-01 19:12:38,021\tINFO compiled_dag_node.py:2774 -- Tearing down compiled DAG\n",
      "2025-06-01 19:12:38,021\tINFO compiled_dag_node.py:2779 -- Cancelling compiled worker on actor: Actor(TorchTensorWorker, 7d2da3d23b65e1c2f396f51101000000)\n",
      "2025-06-01 19:12:38,021\tINFO compiled_dag_node.py:2779 -- Cancelling compiled worker on actor: Actor(TorchTensorWorker, c41653bb9ad1a6ac13b887a401000000)\n",
      "2025-06-01 19:12:38,027\tINFO compiled_dag_node.py:2801 -- Waiting for worker tasks to exit\n",
      "2025-06-01 19:12:38,028\tINFO compiled_dag_node.py:2804 -- Teardown complete\n",
      "2025-06-01 19:12:38,124\tINFO dag_node_operation.py:1108 -- Writing compiled graph schedule visualization to compiled_graph_schedule.png\n",
      "\u001b[32mPASSED\u001b[0m2025-06-01 19:12:38,241\tINFO compiled_dag_node.py:2774 -- Tearing down compiled DAG\n",
      "2025-06-01 19:12:38,241\tINFO compiled_dag_node.py:2779 -- Cancelling compiled worker on actor: Actor(TorchTensorWorker, 7d2da3d23b65e1c2f396f51101000000)\n",
      "2025-06-01 19:12:38,241\tINFO compiled_dag_node.py:2779 -- Cancelling compiled worker on actor: Actor(TorchTensorWorker, c41653bb9ad1a6ac13b887a401000000)\n",
      "2025-06-01 19:12:38,247\tINFO compiled_dag_node.py:2801 -- Waiting for worker tasks to exit\n",
      "2025-06-01 19:12:38,248\tINFO compiled_dag_node.py:2762 -- Killing actor: Actor(TorchTensorWorker, 7d2da3d23b65e1c2f396f51101000000)\n",
      "2025-06-01 19:12:38,248\tINFO compiled_dag_node.py:2762 -- Killing actor: Actor(TorchTensorWorker, c41653bb9ad1a6ac13b887a401000000)\n",
      "2025-06-01 19:12:38,248\tINFO compiled_dag_node.py:2804 -- Teardown complete\n",
      "\n",
      "test_torch_tensor_dag.py::test_torch_tensor_auto[num_gpus3-ray_start_regular0] 2025-06-01 19:12:42,039\tINFO worker.py:1917 -- Started a local Ray instance.\n",
      "2025-06-01 19:12:44,399\tINFO torch_tensor_nccl_channel.py:772 -- Creating NCCL group 89bce4ed-5db8-4701-b5f5-8fc3accfd920 on actors: [Actor(TorchTensorWorker, 01803ec42e7bc13372944cdc01000000), Actor(TorchTensorWorker, 91b199382a334a62fa3d7a8001000000)]\n",
      "2025-06-01 19:12:45,487\tINFO torch_tensor_nccl_channel.py:797 -- NCCL group initialized.\n",
      "2025-06-01 19:12:45,654\tINFO dag_node_operation.py:1108 -- Writing compiled graph schedule visualization to compiled_graph_schedule.png\n",
      "2025-06-01 19:12:45,854\tINFO compiled_dag_node.py:2774 -- Tearing down compiled DAG\n",
      "2025-06-01 19:12:45,854\tINFO compiled_dag_node.py:2779 -- Cancelling compiled worker on actor: Actor(TorchTensorWorker, 01803ec42e7bc13372944cdc01000000)\n",
      "2025-06-01 19:12:45,854\tINFO compiled_dag_node.py:2779 -- Cancelling compiled worker on actor: Actor(TorchTensorWorker, 91b199382a334a62fa3d7a8001000000)\n",
      "\u001b[36m(TorchTensorWorker pid=21983)\u001b[0m Destructing NCCL group on actor: Actor(TorchTensorWorker, 91b199382a334a62fa3d7a8001000000)\n",
      "\u001b[36m(TorchTensorWorker pid=21985)\u001b[0m Destructing NCCL group on actor: Actor(TorchTensorWorker, 01803ec42e7bc13372944cdc01000000)\n",
      "2025-06-01 19:12:46,371\tINFO compiled_dag_node.py:2801 -- Waiting for worker tasks to exit\n",
      "2025-06-01 19:12:46,372\tINFO compiled_dag_node.py:2804 -- Teardown complete\n",
      "2025-06-01 19:12:46,386\tINFO torch_tensor_nccl_channel.py:772 -- Creating NCCL group 86232e32-32e1-4a04-ac1d-1af32b2d6861 on actors: [Actor(TorchTensorWorker, 01803ec42e7bc13372944cdc01000000), Actor(TorchTensorWorker, 91b199382a334a62fa3d7a8001000000)]\n",
      "2025-06-01 19:12:46,419\tINFO torch_tensor_nccl_channel.py:797 -- NCCL group initialized.\n",
      "2025-06-01 19:12:46,536\tINFO dag_node_operation.py:1108 -- Writing compiled graph schedule visualization to compiled_graph_schedule.png\n",
      "\u001b[32mPASSED\u001b[0m2025-06-01 19:12:46,701\tINFO compiled_dag_node.py:2774 -- Tearing down compiled DAG\n",
      "2025-06-01 19:12:46,702\tINFO compiled_dag_node.py:2779 -- Cancelling compiled worker on actor: Actor(TorchTensorWorker, 01803ec42e7bc13372944cdc01000000)\n",
      "2025-06-01 19:12:46,702\tINFO compiled_dag_node.py:2779 -- Cancelling compiled worker on actor: Actor(TorchTensorWorker, 91b199382a334a62fa3d7a8001000000)\n",
      "\u001b[36m(TorchTensorWorker pid=21983)\u001b[0m Destructing NCCL group on actor: Actor(TorchTensorWorker, 91b199382a334a62fa3d7a8001000000)\n",
      "\u001b[36m(TorchTensorWorker pid=21985)\u001b[0m Destructing NCCL group on actor: Actor(TorchTensorWorker, 01803ec42e7bc13372944cdc01000000)\n",
      "\u001b[36m(pid=gcs_server)\u001b[0m [2025-06-01 19:12:46,703 E 21697 21697] (gcs_server) gcs_actor_manager.cc:1788: Failed to kill actor 70926c5e45bf2114225858e801000000, status: RpcError: RPC Error message: Socket closed; RPC Error details:  rpc_code: 14\n",
      "2025-06-01 19:12:47,220\tINFO compiled_dag_node.py:2801 -- Waiting for worker tasks to exit\n",
      "2025-06-01 19:12:47,220\tINFO compiled_dag_node.py:2762 -- Killing actor: Actor(TorchTensorWorker, 01803ec42e7bc13372944cdc01000000)\n",
      "2025-06-01 19:12:47,220\tINFO compiled_dag_node.py:2762 -- Killing actor: Actor(TorchTensorWorker, 91b199382a334a62fa3d7a8001000000)\n",
      "2025-06-01 19:12:47,221\tINFO compiled_dag_node.py:2804 -- Teardown complete\n",
      "\n",
      "test_torch_tensor_dag.py::test_torch_tensor_auto[num_gpus4-ray_start_regular0] 2025-06-01 19:12:51,348\tINFO worker.py:1917 -- Started a local Ray instance.\n",
      "2025-06-01 19:12:53,408\tINFO dag_node_operation.py:1108 -- Writing compiled graph schedule visualization to compiled_graph_schedule.png\n",
      "2025-06-01 19:12:54,174\tINFO compiled_dag_node.py:2774 -- Tearing down compiled DAG\n",
      "2025-06-01 19:12:54,174\tINFO compiled_dag_node.py:2779 -- Cancelling compiled worker on actor: Actor(TorchTensorWorker, c683b894f359002c564656c501000000)\n",
      "2025-06-01 19:12:54,174\tINFO compiled_dag_node.py:2779 -- Cancelling compiled worker on actor: Actor(TorchTensorWorker, b3f0a6917db2cac35b334c5201000000)\n",
      "2025-06-01 19:12:54,181\tINFO compiled_dag_node.py:2801 -- Waiting for worker tasks to exit\n",
      "2025-06-01 19:12:54,181\tINFO compiled_dag_node.py:2804 -- Teardown complete\n",
      "2025-06-01 19:12:54,266\tINFO dag_node_operation.py:1108 -- Writing compiled graph schedule visualization to compiled_graph_schedule.png\n",
      "\u001b[32mPASSED\u001b[0m2025-06-01 19:12:54,356\tINFO compiled_dag_node.py:2774 -- Tearing down compiled DAG\n",
      "2025-06-01 19:12:54,356\tINFO compiled_dag_node.py:2779 -- Cancelling compiled worker on actor: Actor(TorchTensorWorker, c683b894f359002c564656c501000000)\n",
      "2025-06-01 19:12:54,356\tINFO compiled_dag_node.py:2779 -- Cancelling compiled worker on actor: Actor(TorchTensorWorker, b3f0a6917db2cac35b334c5201000000)\n",
      "2025-06-01 19:12:54,363\tINFO compiled_dag_node.py:2801 -- Waiting for worker tasks to exit\n",
      "2025-06-01 19:12:54,363\tINFO compiled_dag_node.py:2762 -- Killing actor: Actor(TorchTensorWorker, c683b894f359002c564656c501000000)\n",
      "2025-06-01 19:12:54,363\tINFO compiled_dag_node.py:2762 -- Killing actor: Actor(TorchTensorWorker, b3f0a6917db2cac35b334c5201000000)\n",
      "2025-06-01 19:12:54,363\tINFO compiled_dag_node.py:2804 -- Teardown complete\n",
      "\n",
      "test_torch_tensor_dag.py::test_torch_tensor_nccl_overlap_timed[ray_start_regular0-False] 2025-06-01 19:12:58,261\tINFO worker.py:1917 -- Started a local Ray instance.\n",
      "\u001b[31mFAILED\u001b[0m\n",
      "test_torch_tensor_dag.py::test_torch_tensor_nccl_overlap_timed[ray_start_regular1-True] 2025-06-01 19:13:01,240\tINFO worker.py:1917 -- Started a local Ray instance.\n",
      "\u001b[31mFAILED\u001b[0m\n",
      "test_torch_tensor_dag.py::test_torch_tensor_nccl_disallows_driver 2025-06-01 19:13:04,425\tINFO worker.py:1917 -- Started a local Ray instance.\n",
      "[2025-06-01 19:13:04,831 E 14075 24717] core_worker.cc:2724: Failed to register actor. Error message: SchedulingCancelled: Actor creation cancelled. actor_id=66782d013f75007ebab4825001000000\n",
      "\u001b[32mPASSED\u001b[0m\n",
      "test_torch_tensor_dag.py::test_torch_tensor_custom_comm[ray_start_regular0] 2025-06-01 19:13:08,494\tINFO worker.py:1917 -- Started a local Ray instance.\n",
      "2025-06-01 19:13:10,741\tINFO torch_tensor_nccl_channel.py:770 -- Initializing custom NCCL group f5c795d9-f7b2-43f2-8779-76a37783379f on actors: [Actor(TorchTensorWorker, 947b9bfbd2fcece9e09e73c101000000), Actor(TorchTensorWorker, d5a2427d3f8fa77c3e318f6a01000000)]\n",
      "2025-06-01 19:13:11,631\tINFO torch_tensor_nccl_channel.py:797 -- NCCL group initialized.\n",
      "2025-06-01 19:13:11,824\tINFO dag_node_operation.py:1108 -- Writing compiled graph schedule visualization to compiled_graph_schedule.png\n",
      "\u001b[32mPASSED\u001b[0m2025-06-01 19:13:12,060\tINFO compiled_dag_node.py:2774 -- Tearing down compiled DAG\n",
      "2025-06-01 19:13:12,061\tINFO compiled_dag_node.py:2779 -- Cancelling compiled worker on actor: Actor(TorchTensorWorker, 947b9bfbd2fcece9e09e73c101000000)\n",
      "2025-06-01 19:13:12,061\tINFO compiled_dag_node.py:2779 -- Cancelling compiled worker on actor: Actor(TorchTensorWorker, d5a2427d3f8fa77c3e318f6a01000000)\n",
      "\u001b[36m(TorchTensorWorker pid=25237)\u001b[0m Destructing NCCL group on actor: Actor(TorchTensorWorker, 947b9bfbd2fcece9e09e73c101000000)\n",
      "\u001b[36m(TorchTensorWorker pid=25238)\u001b[0m Destructing NCCL group on actor: Actor(TorchTensorWorker, d5a2427d3f8fa77c3e318f6a01000000)\n",
      "2025-06-01 19:13:12,580\tINFO compiled_dag_node.py:2801 -- Waiting for worker tasks to exit\n",
      "2025-06-01 19:13:12,581\tINFO compiled_dag_node.py:2762 -- Killing actor: Actor(TorchTensorWorker, 947b9bfbd2fcece9e09e73c101000000)\n",
      "2025-06-01 19:13:12,581\tINFO compiled_dag_node.py:2762 -- Killing actor: Actor(TorchTensorWorker, d5a2427d3f8fa77c3e318f6a01000000)\n",
      "2025-06-01 19:13:12,581\tINFO compiled_dag_node.py:2804 -- Teardown complete\n",
      "\n",
      "test_torch_tensor_dag.py::test_torch_tensor_custom_comm_inited[ray_start_regular0] 2025-06-01 19:13:15,517\tINFO worker.py:1917 -- Started a local Ray instance.\n",
      "\u001b[33mSKIPPED\u001b[0m\n",
      "test_torch_tensor_dag.py::test_torch_tensor_default_comm[transports0-ray_start_regular0] 2025-06-01 19:13:18,599\tINFO worker.py:1917 -- Started a local Ray instance.\n",
      "\u001b[31mFAILED\u001b[0m\n",
      "test_torch_tensor_dag.py::test_torch_tensor_default_comm[transports1-ray_start_regular0] 2025-06-01 19:13:21,695\tINFO worker.py:1917 -- Started a local Ray instance.\n",
      "\u001b[31mFAILED\u001b[0m\n",
      "test_torch_tensor_dag.py::test_torch_tensor_default_comm[transports2-ray_start_regular0] 2025-06-01 19:13:25,704\tINFO worker.py:1917 -- Started a local Ray instance.\n",
      "\u001b[31mFAILED\u001b[0m\n",
      "test_torch_tensor_dag.py::test_torch_tensor_default_comm[transports3-ray_start_regular0] 2025-06-01 19:13:28,684\tINFO worker.py:1917 -- Started a local Ray instance.\n",
      "\u001b[31mFAILED\u001b[0m\n",
      "test_torch_tensor_dag.py::test_torch_tensor_invalid_custom_comm[ray_start_regular0] 2025-06-01 19:13:31,756\tINFO worker.py:1917 -- Started a local Ray instance.\n",
      "\u001b[33mSKIPPED\u001b[0m\n",
      "test_torch_tensor_dag.py::test_torch_tensor_nccl_static_shape[ray_start_regular0] 2025-06-01 19:13:35,828\tINFO worker.py:1917 -- Started a local Ray instance.\n",
      "2025-06-01 19:13:38,053\tINFO torch_tensor_nccl_channel.py:772 -- Creating NCCL group 6c7a2137-63b1-4bf6-8d17-a05614c1df37 on actors: [Actor(TorchTensorWorker, 268dc143814b64d9956c3aac01000000), Actor(TorchTensorWorker, f082fd81ce6a3b0b8906d34b01000000)]\n",
      "2025-06-01 19:13:39,135\tINFO torch_tensor_nccl_channel.py:797 -- NCCL group initialized.\n",
      "2025-06-01 19:13:39,238\tINFO dag_node_operation.py:1108 -- Writing compiled graph schedule visualization to compiled_graph_schedule.png\n",
      "2025-06-01 19:13:39,475\tINFO compiled_dag_node.py:2774 -- Tearing down compiled DAG\n",
      "\u001b[32mPASSED\u001b[0m2025-06-01 19:13:39,477\tINFO compiled_dag_node.py:2779 -- Cancelling compiled worker on actor: Actor(TorchTensorWorker, 268dc143814b64d9956c3aac01000000)\n",
      "2025-06-01 19:13:39,477\tINFO compiled_dag_node.py:2779 -- Cancelling compiled worker on actor: Actor(TorchTensorWorker, f082fd81ce6a3b0b8906d34b01000000)\n",
      "\u001b[36m(TorchTensorWorker pid=29684)\u001b[0m ERROR:root:Compiled DAG task exited with exception\n",
      "\u001b[36m(TorchTensorWorker pid=29684)\u001b[0m Traceback (most recent call last):\n",
      "\u001b[36m(TorchTensorWorker pid=29684)\u001b[0m   File \"/home/wxdeng/stbr-min/python/ray/dag/compiled_dag_node.py\", line 330, in do_exec_tasks_EXP\n",
      "\u001b[36m(TorchTensorWorker pid=29684)\u001b[0m     done = tasks[operation.exec_task_idx].exec_operation(\n",
      "\u001b[36m(TorchTensorWorker pid=29684)\u001b[0m         self, overlap_gpu_communication\n",
      "\u001b[36m(TorchTensorWorker pid=29684)\u001b[0m     )\n",
      "\u001b[36m(TorchTensorWorker pid=29684)\u001b[0m   File \"/home/wxdeng/stbr-min/python/ray/dag/compiled_dag_node.py\", line 1069, in exec_operation\n",
      "\u001b[36m(TorchTensorWorker pid=29684)\u001b[0m     return self.exec_operation_inner(\n",
      "\u001b[36m(TorchTensorWorker pid=29684)\u001b[0m            ~~~~~~~~~~~~~~~~~~~~~~~~~^\n",
      "\u001b[36m(TorchTensorWorker pid=29684)\u001b[0m         class_handle, overlap_gpu_communication\n",
      "\u001b[36m(TorchTensorWorker pid=29684)\u001b[0m         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[36m(TorchTensorWorker pid=29684)\u001b[0m     )\n",
      "\u001b[36m(TorchTensorWorker pid=29684)\u001b[0m     ^\n",
      "\u001b[36m(TorchTensorWorker pid=29684)\u001b[0m   File \"/home/wxdeng/stbr-min/python/ray/dag/compiled_dag_node.py\", line 1132, in exec_operation_inner\n",
      "\u001b[36m(TorchTensorWorker pid=29684)\u001b[0m     raise exc\n",
      "\u001b[36m(TorchTensorWorker pid=29684)\u001b[0m   File \"/home/wxdeng/stbr-min/python/ray/dag/compiled_dag_node.py\", line 1127, in exec_operation_inner\n",
      "\u001b[36m(TorchTensorWorker pid=29684)\u001b[0m     output_val = method(*input_values, **self.resolved_kwargs)\n",
      "\u001b[36m(TorchTensorWorker pid=29684)\u001b[0m   File \"/home/wxdeng/stbr-min/python/ray/dag/communication_node.py\", line 72, in execute\n",
      "\u001b[36m(TorchTensorWorker pid=29684)\u001b[0m     self.nccl_ch.write(data)\n",
      "\u001b[36m(TorchTensorWorker pid=29684)\u001b[0m     ~~~~~~~~~~~~~~~~~~^^^^^^\n",
      "\u001b[36m(TorchTensorWorker pid=29684)\u001b[0m   File \"/home/wxdeng/stbr-min/python/ray/experimental/channel/torch_tensor_nccl_channel.py\", line 270, in write\n",
      "\u001b[36m(TorchTensorWorker pid=29684)\u001b[0m     self._send_cpu_and_gpu_data(value, timeout)\n",
      "\u001b[36m(TorchTensorWorker pid=29684)\u001b[0m     ~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^\n",
      "\u001b[36m(TorchTensorWorker pid=29684)\u001b[0m   File \"/home/wxdeng/stbr-min/python/ray/experimental/channel/torch_tensor_nccl_channel.py\", line 203, in _send_cpu_and_gpu_data\n",
      "\u001b[36m(TorchTensorWorker pid=29684)\u001b[0m     self._gpu_data_channel.write(gpu_tensors)\n",
      "\u001b[36m(TorchTensorWorker pid=29684)\u001b[0m     ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^\n",
      "\u001b[36m(TorchTensorWorker pid=29684)\u001b[0m   File \"/home/wxdeng/stbr-min/python/ray/experimental/channel/torch_tensor_nccl_channel.py\", line 559, in write\n",
      "\u001b[36m(TorchTensorWorker pid=29684)\u001b[0m     metadata = self._get_send_tensors_metadata(tensors)\n",
      "\u001b[36m(TorchTensorWorker pid=29684)\u001b[0m   File \"/home/wxdeng/stbr-min/python/ray/experimental/channel/torch_tensor_nccl_channel.py\", line 514, in _get_send_tensors_metadata\n",
      "\u001b[36m(TorchTensorWorker pid=29684)\u001b[0m     raise ValueError(\n",
      "\u001b[36m(TorchTensorWorker pid=29684)\u001b[0m     ...<4 lines>...\n",
      "\u001b[36m(TorchTensorWorker pid=29684)\u001b[0m     )\n",
      "\u001b[36m(TorchTensorWorker pid=29684)\u001b[0m ValueError: Expected torch.Tensors with shapes and dtypes: [(shape=torch.Size([10]), dtype=torch.float16)], found: [(shape=torch.Size([20]), dtype=torch.float16)]. DAG will shut down.\n",
      "\u001b[36m(TorchTensorWorker pid=29684)\u001b[0m Destructing NCCL group on actor: Actor(TorchTensorWorker, 268dc143814b64d9956c3aac01000000)\n",
      "\u001b[36m(TorchTensorWorker pid=29685)\u001b[0m Destructing NCCL group on actor: Actor(TorchTensorWorker, f082fd81ce6a3b0b8906d34b01000000)\n",
      "2025-06-01 19:13:39,955\tINFO compiled_dag_node.py:2801 -- Waiting for worker tasks to exit\n",
      "2025-06-01 19:13:39,956\tINFO compiled_dag_node.py:2804 -- Teardown complete\n",
      "\n",
      "test_torch_tensor_dag.py::test_torch_tensor_nccl_direct_return[ray_start_regular0] 2025-06-01 19:13:44,048\tINFO worker.py:1917 -- Started a local Ray instance.\n",
      "2025-06-01 19:13:46,313\tINFO torch_tensor_nccl_channel.py:772 -- Creating NCCL group 0c9825ae-b576-41e0-9be5-3541e6563ff3 on actors: [Actor(TorchTensorWorker, 48f7876b80de9330794e322b01000000), Actor(TorchTensorWorker, aefdfefcb63670fac51dd43801000000)]\n",
      "2025-06-01 19:13:47,412\tINFO torch_tensor_nccl_channel.py:797 -- NCCL group initialized.\n",
      "2025-06-01 19:13:47,588\tINFO dag_node_operation.py:1108 -- Writing compiled graph schedule visualization to compiled_graph_schedule.png\n",
      "2025-06-01 19:13:47,786\tINFO compiled_dag_node.py:2774 -- Tearing down compiled DAG\n",
      "\u001b[32mPASSED\u001b[0m2025-06-01 19:13:47,788\tINFO compiled_dag_node.py:2779 -- Cancelling compiled worker on actor: Actor(TorchTensorWorker, 48f7876b80de9330794e322b01000000)\n",
      "2025-06-01 19:13:47,788\tINFO compiled_dag_node.py:2779 -- Cancelling compiled worker on actor: Actor(TorchTensorWorker, aefdfefcb63670fac51dd43801000000)\n",
      "\u001b[36m(TorchTensorWorker pid=30388)\u001b[0m Destructing NCCL group on actor: Actor(TorchTensorWorker, aefdfefcb63670fac51dd43801000000)\n",
      "\u001b[36m(TorchTensorWorker pid=30386)\u001b[0m ERROR:root:Compiled DAG task exited with exception\n",
      "\u001b[36m(TorchTensorWorker pid=30386)\u001b[0m Traceback (most recent call last):\n",
      "\u001b[36m(TorchTensorWorker pid=30386)\u001b[0m   File \"/home/wxdeng/stbr-min/python/ray/dag/compiled_dag_node.py\", line 330, in do_exec_tasks_EXP\n",
      "\u001b[36m(TorchTensorWorker pid=30386)\u001b[0m     done = tasks[operation.exec_task_idx].exec_operation(\n",
      "\u001b[36m(TorchTensorWorker pid=30386)\u001b[0m         self, overlap_gpu_communication\n",
      "\u001b[36m(TorchTensorWorker pid=30386)\u001b[0m     )\n",
      "\u001b[36m(TorchTensorWorker pid=30386)\u001b[0m   File \"/home/wxdeng/stbr-min/python/ray/dag/compiled_dag_node.py\", line 1069, in exec_operation\n",
      "\u001b[36m(TorchTensorWorker pid=30386)\u001b[0m     return self.exec_operation_inner(\n",
      "\u001b[36m(TorchTensorWorker pid=30386)\u001b[0m            ~~~~~~~~~~~~~~~~~~~~~~~~~^\n",
      "\u001b[36m(TorchTensorWorker pid=30386)\u001b[0m         class_handle, overlap_gpu_communication\n",
      "\u001b[36m(TorchTensorWorker pid=30386)\u001b[0m         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[36m(TorchTensorWorker pid=30386)\u001b[0m     )\n",
      "\u001b[36m(TorchTensorWorker pid=30386)\u001b[0m     ^\n",
      "\u001b[36m(TorchTensorWorker pid=30386)\u001b[0m   File \"/home/wxdeng/stbr-min/python/ray/dag/compiled_dag_node.py\", line 1132, in exec_operation_inner\n",
      "\u001b[36m(TorchTensorWorker pid=30386)\u001b[0m     raise exc\n",
      "\u001b[36m(TorchTensorWorker pid=30386)\u001b[0m   File \"/home/wxdeng/stbr-min/python/ray/dag/compiled_dag_node.py\", line 1127, in exec_operation_inner\n",
      "\u001b[36m(TorchTensorWorker pid=30386)\u001b[0m     output_val = method(*input_values, **self.resolved_kwargs)\n",
      "\u001b[36m(TorchTensorWorker pid=30386)\u001b[0m   File \"/home/wxdeng/stbr-min/python/ray/dag/communication_node.py\", line 72, in execute\n",
      "\u001b[36m(TorchTensorWorker pid=30386)\u001b[0m     self.nccl_ch.write(data)\n",
      "\u001b[36m(TorchTensorWorker pid=30386)\u001b[0m     ~~~~~~~~~~~~~~~~~~^^^^^^\n",
      "\u001b[36m(TorchTensorWorker pid=30386)\u001b[0m   File \"/home/wxdeng/stbr-min/python/ray/experimental/channel/torch_tensor_nccl_channel.py\", line 257, in write\n",
      "\u001b[36m(TorchTensorWorker pid=30386)\u001b[0m     raise ValueError(\n",
      "\u001b[36m(TorchTensorWorker pid=30386)\u001b[0m     ...<3 lines>...\n",
      "\u001b[36m(TorchTensorWorker pid=30386)\u001b[0m     )\n",
      "\u001b[36m(TorchTensorWorker pid=30386)\u001b[0m ValueError: Task annotated with _direct_return=True must return a CUDA torch.Tensor, instead found value `1`. DAG will shut down.\n",
      "\u001b[36m(TorchTensorWorker pid=30386)\u001b[0m Destructing NCCL group on actor: Actor(TorchTensorWorker, 48f7876b80de9330794e322b01000000)\n",
      "2025-06-01 19:13:48,307\tINFO compiled_dag_node.py:2801 -- Waiting for worker tasks to exit\n",
      "2025-06-01 19:13:48,308\tINFO compiled_dag_node.py:2804 -- Teardown complete\n",
      "\n",
      "test_torch_tensor_dag.py::test_torch_tensor_nccl_nested_dynamic[ray_start_regular0] 2025-06-01 19:13:52,453\tINFO worker.py:1917 -- Started a local Ray instance.\n",
      "2025-06-01 19:13:54,744\tINFO torch_tensor_nccl_channel.py:772 -- Creating NCCL group 31454bad-5ef7-44f6-b591-6749a9303160 on actors: [Actor(TorchTensorWorker, eae25e51426fb27fc091629a01000000), Actor(TorchTensorWorker, 6e28d828a5feda66111af6e001000000)]\n",
      "2025-06-01 19:13:55,824\tINFO torch_tensor_nccl_channel.py:797 -- NCCL group initialized.\n",
      "2025-06-01 19:13:55,923\tINFO dag_node_operation.py:1108 -- Writing compiled graph schedule visualization to compiled_graph_schedule.png\n",
      "\u001b[32mPASSED\u001b[0m2025-06-01 19:13:56,088\tINFO compiled_dag_node.py:2774 -- Tearing down compiled DAG\n",
      "2025-06-01 19:13:56,088\tINFO compiled_dag_node.py:2779 -- Cancelling compiled worker on actor: Actor(TorchTensorWorker, 6e28d828a5feda66111af6e001000000)\n",
      "2025-06-01 19:13:56,088\tINFO compiled_dag_node.py:2779 -- Cancelling compiled worker on actor: Actor(TorchTensorWorker, eae25e51426fb27fc091629a01000000)\n",
      "\u001b[36m(TorchTensorWorker pid=31137)\u001b[0m Destructing NCCL group on actor: Actor(TorchTensorWorker, 6e28d828a5feda66111af6e001000000)\n",
      "\u001b[36m(TorchTensorWorker pid=31138)\u001b[0m Destructing NCCL group on actor: Actor(TorchTensorWorker, eae25e51426fb27fc091629a01000000)\n",
      "2025-06-01 19:13:56,829\tINFO compiled_dag_node.py:2801 -- Waiting for worker tasks to exit\n",
      "2025-06-01 19:13:56,829\tINFO compiled_dag_node.py:2762 -- Killing actor: Actor(TorchTensorWorker, 6e28d828a5feda66111af6e001000000)\n",
      "2025-06-01 19:13:56,830\tINFO compiled_dag_node.py:2762 -- Killing actor: Actor(TorchTensorWorker, eae25e51426fb27fc091629a01000000)\n",
      "2025-06-01 19:13:56,830\tINFO compiled_dag_node.py:2804 -- Teardown complete\n",
      "\n",
      "test_torch_tensor_dag.py::test_torch_tensor_exceptions[False-False-False-ray_start_regular0] 2025-06-01 19:13:59,945\tINFO worker.py:1917 -- Started a local Ray instance.\n",
      "2025-06-01 19:14:02,157\tINFO torch_tensor_nccl_channel.py:772 -- Creating NCCL group 8c37ca98-00f0-476c-b8a0-bb3bc9d95b59 on actors: [Actor(TorchTensorWorker, 1be8390e1895338ca57a459201000000), Actor(TorchTensorWorker, 438fb1ea7ceb829d6abcee2401000000)]\n",
      "2025-06-01 19:14:03,248\tINFO torch_tensor_nccl_channel.py:797 -- NCCL group initialized.\n",
      "2025-06-01 19:14:03,435\tINFO dag_node_operation.py:1108 -- Writing compiled graph schedule visualization to compiled_graph_schedule.png\n",
      "\u001b[32mPASSED\u001b[0m2025-06-01 19:14:03,633\tINFO compiled_dag_node.py:2774 -- Tearing down compiled DAG\n",
      "2025-06-01 19:14:03,634\tINFO compiled_dag_node.py:2779 -- Cancelling compiled worker on actor: Actor(TorchTensorWorker, 438fb1ea7ceb829d6abcee2401000000)\n",
      "2025-06-01 19:14:03,634\tINFO compiled_dag_node.py:2779 -- Cancelling compiled worker on actor: Actor(TorchTensorWorker, 1be8390e1895338ca57a459201000000)\n",
      "\u001b[36m(TorchTensorWorker pid=31841)\u001b[0m Destructing NCCL group on actor: Actor(TorchTensorWorker, 438fb1ea7ceb829d6abcee2401000000)\n",
      "\u001b[36m(TorchTensorWorker pid=31840)\u001b[0m Destructing NCCL group on actor: Actor(TorchTensorWorker, 1be8390e1895338ca57a459201000000)\n",
      "2025-06-01 19:14:04,150\tINFO compiled_dag_node.py:2801 -- Waiting for worker tasks to exit\n",
      "2025-06-01 19:14:04,151\tINFO compiled_dag_node.py:2762 -- Killing actor: Actor(TorchTensorWorker, 438fb1ea7ceb829d6abcee2401000000)\n",
      "2025-06-01 19:14:04,151\tINFO compiled_dag_node.py:2762 -- Killing actor: Actor(TorchTensorWorker, 1be8390e1895338ca57a459201000000)\n",
      "2025-06-01 19:14:04,151\tINFO compiled_dag_node.py:2804 -- Teardown complete\n",
      "\n",
      "test_torch_tensor_dag.py::test_torch_tensor_exceptions[False-False-True-ray_start_regular0] 2025-06-01 19:14:06,921\tINFO worker.py:1917 -- Started a local Ray instance.\n",
      "2025-06-01 19:14:09,182\tINFO torch_tensor_nccl_channel.py:772 -- Creating NCCL group 69e261ed-1454-48ae-9446-9c4c1c9af868 on actors: [Actor(TorchTensorWorker, 98c360626232e9288d03c5f501000000), Actor(TorchTensorWorker, 7cb4ce8a28f8432f41ed784001000000)]\n",
      "2025-06-01 19:14:10,263\tINFO torch_tensor_nccl_channel.py:797 -- NCCL group initialized.\n",
      "2025-06-01 19:14:10,457\tINFO dag_node_operation.py:1108 -- Writing compiled graph schedule visualization to compiled_graph_schedule.png\n",
      "2025-06-01 19:14:10,660\tINFO compiled_dag_node.py:2774 -- Tearing down compiled DAG\n",
      "\u001b[32mPASSED\u001b[0m2025-06-01 19:14:10,663\tINFO compiled_dag_node.py:2779 -- Cancelling compiled worker on actor: Actor(TorchTensorWorker, 98c360626232e9288d03c5f501000000)\n",
      "2025-06-01 19:14:10,663\tINFO compiled_dag_node.py:2779 -- Cancelling compiled worker on actor: Actor(TorchTensorWorker, 7cb4ce8a28f8432f41ed784001000000)\n",
      "\u001b[36m(TorchTensorWorker pid=32593)\u001b[0m Destructing NCCL group on actor: Actor(TorchTensorWorker, 7cb4ce8a28f8432f41ed784001000000)\n",
      "\u001b[36m(TorchTensorWorker pid=32594)\u001b[0m ERROR:root:Compiled DAG task exited with exception\n",
      "\u001b[36m(TorchTensorWorker pid=32594)\u001b[0m Traceback (most recent call last):\n",
      "\u001b[36m(TorchTensorWorker pid=32594)\u001b[0m   File \"/home/wxdeng/stbr-min/python/ray/dag/compiled_dag_node.py\", line 330, in do_exec_tasks_EXP\n",
      "\u001b[36m(TorchTensorWorker pid=32594)\u001b[0m     done = tasks[operation.exec_task_idx].exec_operation(\n",
      "\u001b[36m(TorchTensorWorker pid=32594)\u001b[0m         self, overlap_gpu_communication\n",
      "\u001b[36m(TorchTensorWorker pid=32594)\u001b[0m     )\n",
      "\u001b[36m(TorchTensorWorker pid=32594)\u001b[0m   File \"/home/wxdeng/stbr-min/python/ray/dag/compiled_dag_node.py\", line 1069, in exec_operation\n",
      "\u001b[36m(TorchTensorWorker pid=32594)\u001b[0m     return self.exec_operation_inner(\n",
      "\u001b[36m(TorchTensorWorker pid=32594)\u001b[0m            ~~~~~~~~~~~~~~~~~~~~~~~~~^\n",
      "\u001b[36m(TorchTensorWorker pid=32594)\u001b[0m         class_handle, overlap_gpu_communication\n",
      "\u001b[36m(TorchTensorWorker pid=32594)\u001b[0m         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[36m(TorchTensorWorker pid=32594)\u001b[0m     )\n",
      "\u001b[36m(TorchTensorWorker pid=32594)\u001b[0m     ^\n",
      "\u001b[36m(TorchTensorWorker pid=32594)\u001b[0m   File \"/home/wxdeng/stbr-min/python/ray/dag/compiled_dag_node.py\", line 1132, in exec_operation_inner\n",
      "\u001b[36m(TorchTensorWorker pid=32594)\u001b[0m     raise exc\n",
      "\u001b[36m(TorchTensorWorker pid=32594)\u001b[0m   File \"/home/wxdeng/stbr-min/python/ray/dag/compiled_dag_node.py\", line 1127, in exec_operation_inner\n",
      "\u001b[36m(TorchTensorWorker pid=32594)\u001b[0m     output_val = method(*input_values, **self.resolved_kwargs)\n",
      "\u001b[36m(TorchTensorWorker pid=32594)\u001b[0m   File \"/home/wxdeng/stbr-min/python/ray/dag/communication_node.py\", line 72, in execute\n",
      "\u001b[36m(TorchTensorWorker pid=32594)\u001b[0m     self.nccl_ch.write(data)\n",
      "\u001b[36m(TorchTensorWorker pid=32594)\u001b[0m     ~~~~~~~~~~~~~~~~~~^^^^^^\n",
      "\u001b[36m(TorchTensorWorker pid=32594)\u001b[0m   File \"/home/wxdeng/stbr-min/python/ray/experimental/channel/torch_tensor_nccl_channel.py\", line 243, in write\n",
      "\u001b[36m(TorchTensorWorker pid=32594)\u001b[0m     raise value\n",
      "\u001b[36m(TorchTensorWorker pid=32594)\u001b[0m   File \"/home/wxdeng/stbr-min/python/ray/dag/compiled_dag_node.py\", line 1091, in exec_operation_inner\n",
      "\u001b[36m(TorchTensorWorker pid=32594)\u001b[0m     _process_return_vals(input_data, return_single_output=False)\n",
      "\u001b[36m(TorchTensorWorker pid=32594)\u001b[0m     ~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[36m(TorchTensorWorker pid=32594)\u001b[0m   File \"/home/wxdeng/stbr-min/python/ray/experimental/compiled_dag_ref.py\", line 27, in _process_return_vals\n",
      "\u001b[36m(TorchTensorWorker pid=32594)\u001b[0m     raise val.as_instanceof_cause()\n",
      "\u001b[36m(TorchTensorWorker pid=32594)\u001b[0m ray.exceptions.RayTaskError(RuntimeError): \u001b[36mray::TorchTensorWorker.__ray_call__()\u001b[39m (pid=32594, ip=128.208.3.124)\n",
      "\u001b[36m(TorchTensorWorker pid=32594)\u001b[0m   File \"/home/wxdeng/stbr-min/python/ray/dag/tests/experimental/test_torch_tensor_dag.py\", line 67, in send_or_raise\n",
      "\u001b[36m(TorchTensorWorker pid=32594)\u001b[0m     raise RuntimeError()\n",
      "\u001b[36m(TorchTensorWorker pid=32594)\u001b[0m RuntimeError\n",
      "\u001b[36m(TorchTensorWorker pid=32594)\u001b[0m Destructing NCCL group on actor: Actor(TorchTensorWorker, 98c360626232e9288d03c5f501000000)\n",
      "2025-06-01 19:14:11,148\tINFO compiled_dag_node.py:2801 -- Waiting for worker tasks to exit\n",
      "2025-06-01 19:14:11,149\tINFO compiled_dag_node.py:2804 -- Teardown complete\n",
      "\n",
      "test_torch_tensor_dag.py::test_torch_tensor_exceptions[False-True-False-ray_start_regular0] 2025-06-01 19:14:15,149\tINFO worker.py:1917 -- Started a local Ray instance.\n",
      "2025-06-01 19:14:17,315\tINFO torch_tensor_nccl_channel.py:772 -- Creating NCCL group 1a087ea2-c6d4-4e18-9a40-11034ed0b100 on actors: [Actor(TorchTensorWorker, 35f3dc20a1a3c2fe75cf97f801000000), Actor(TorchTensorWorker, 1c6ac9bc81de5618b4c94c1a01000000)]\n",
      "2025-06-01 19:14:18,392\tINFO torch_tensor_nccl_channel.py:797 -- NCCL group initialized.\n",
      "2025-06-01 19:14:18,505\tINFO dag_node_operation.py:1108 -- Writing compiled graph schedule visualization to compiled_graph_schedule.png\n",
      "2025-06-01 19:14:18,706\tINFO compiled_dag_node.py:2774 -- Tearing down compiled DAG\n",
      "\u001b[32mPASSED\u001b[0m2025-06-01 19:14:18,708\tINFO compiled_dag_node.py:2779 -- Cancelling compiled worker on actor: Actor(TorchTensorWorker, 1c6ac9bc81de5618b4c94c1a01000000)\n",
      "2025-06-01 19:14:18,708\tINFO compiled_dag_node.py:2779 -- Cancelling compiled worker on actor: Actor(TorchTensorWorker, 35f3dc20a1a3c2fe75cf97f801000000)\n",
      "\u001b[36m(TorchTensorWorker pid=33339)\u001b[0m ERROR:root:Compiled DAG task exited with exception\n",
      "\u001b[36m(TorchTensorWorker pid=33339)\u001b[0m Traceback (most recent call last):\n",
      "\u001b[36m(TorchTensorWorker pid=33339)\u001b[0m   File \"/home/wxdeng/stbr-min/python/ray/dag/compiled_dag_node.py\", line 330, in do_exec_tasks_EXP\n",
      "\u001b[36m(TorchTensorWorker pid=33339)\u001b[0m     done = tasks[operation.exec_task_idx].exec_operation(\n",
      "\u001b[36m(TorchTensorWorker pid=33339)\u001b[0m         self, overlap_gpu_communication\n",
      "\u001b[36m(TorchTensorWorker pid=33339)\u001b[0m     )\n",
      "\u001b[36m(TorchTensorWorker pid=33339)\u001b[0m   File \"/home/wxdeng/stbr-min/python/ray/dag/compiled_dag_node.py\", line 1069, in exec_operation\n",
      "\u001b[36m(TorchTensorWorker pid=33339)\u001b[0m     return self.exec_operation_inner(\n",
      "\u001b[36m(TorchTensorWorker pid=33339)\u001b[0m            ~~~~~~~~~~~~~~~~~~~~~~~~~^\n",
      "\u001b[36m(TorchTensorWorker pid=33339)\u001b[0m         class_handle, overlap_gpu_communication\n",
      "\u001b[36m(TorchTensorWorker pid=33339)\u001b[0m         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[36m(TorchTensorWorker pid=33339)\u001b[0m     )\n",
      "\u001b[36m(TorchTensorWorker pid=33339)\u001b[0m     ^\n",
      "\u001b[36m(TorchTensorWorker pid=33339)\u001b[0m   File \"/home/wxdeng/stbr-min/python/ray/dag/compiled_dag_node.py\", line 1132, in exec_operation_inner\n",
      "\u001b[36m(TorchTensorWorker pid=33339)\u001b[0m     raise exc\n",
      "\u001b[36m(TorchTensorWorker pid=33339)\u001b[0m   File \"/home/wxdeng/stbr-min/python/ray/dag/compiled_dag_node.py\", line 1127, in exec_operation_inner\n",
      "\u001b[36m(TorchTensorWorker pid=33339)\u001b[0m     output_val = method(*input_values, **self.resolved_kwargs)\n",
      "\u001b[36m(TorchTensorWorker pid=33339)\u001b[0m   File \"/home/wxdeng/stbr-min/python/ray/dag/communication_node.py\", line 72, in execute\n",
      "\u001b[36m(TorchTensorWorker pid=33339)\u001b[0m     self.nccl_ch.write(data)\n",
      "\u001b[36m(TorchTensorWorker pid=33339)\u001b[0m     ~~~~~~~~~~~~~~~~~~^^^^^^\n",
      "\u001b[36m(TorchTensorWorker pid=33339)\u001b[0m   File \"/home/wxdeng/stbr-min/python/ray/experimental/channel/torch_tensor_nccl_channel.py\", line 243, in write\n",
      "\u001b[36m(TorchTensorWorker pid=33339)\u001b[0m     raise value\n",
      "\u001b[36m(TorchTensorWorker pid=33339)\u001b[0m   File \"/home/wxdeng/stbr-min/python/ray/dag/compiled_dag_node.py\", line 1091, in exec_operation_inner\n",
      "\u001b[36m(TorchTensorWorker pid=33339)\u001b[0m     _process_return_vals(input_data, return_single_output=False)\n",
      "\u001b[36m(TorchTensorWorker pid=33339)\u001b[0m     ~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[36m(TorchTensorWorker pid=33339)\u001b[0m   File \"/home/wxdeng/stbr-min/python/ray/experimental/compiled_dag_ref.py\", line 27, in _process_return_vals\n",
      "\u001b[36m(TorchTensorWorker pid=33339)\u001b[0m     raise val.as_instanceof_cause()\n",
      "\u001b[36m(TorchTensorWorker pid=33339)\u001b[0m ray.exceptions.RayTaskError(RuntimeError): \u001b[36mray::TorchTensorWorker.__ray_call__()\u001b[39m (pid=33339, ip=128.208.3.124)\n",
      "\u001b[36m(TorchTensorWorker pid=33339)\u001b[0m   File \"/home/wxdeng/stbr-min/python/ray/dag/tests/experimental/test_torch_tensor_dag.py\", line 67, in send_or_raise\n",
      "\u001b[36m(TorchTensorWorker pid=33339)\u001b[0m     raise RuntimeError()\n",
      "\u001b[36m(TorchTensorWorker pid=33339)\u001b[0m RuntimeError\n",
      "\u001b[36m(TorchTensorWorker pid=33339)\u001b[0m Destructing NCCL group on actor: Actor(TorchTensorWorker, 1c6ac9bc81de5618b4c94c1a01000000)\n",
      "\u001b[36m(TorchTensorWorker pid=33340)\u001b[0m Destructing NCCL group on actor: Actor(TorchTensorWorker, 35f3dc20a1a3c2fe75cf97f801000000)\n",
      "2025-06-01 19:14:19,225\tINFO compiled_dag_node.py:2801 -- Waiting for worker tasks to exit\n",
      "2025-06-01 19:14:19,225\tINFO compiled_dag_node.py:2804 -- Teardown complete\n",
      "\n",
      "test_torch_tensor_dag.py::test_torch_tensor_exceptions[False-True-True-ray_start_regular0] 2025-06-01 19:14:23,180\tINFO worker.py:1917 -- Started a local Ray instance.\n",
      "2025-06-01 19:14:25,380\tINFO torch_tensor_nccl_channel.py:772 -- Creating NCCL group b6f14a0b-ba20-46c2-9010-8ce3f9fff730 on actors: [Actor(TorchTensorWorker, ad3565bb44b98be5e0103c3701000000), Actor(TorchTensorWorker, cb7bcc310f3ce9d926eb299601000000)]\n",
      "2025-06-01 19:14:26,471\tINFO torch_tensor_nccl_channel.py:797 -- NCCL group initialized.\n",
      "2025-06-01 19:14:26,655\tINFO dag_node_operation.py:1108 -- Writing compiled graph schedule visualization to compiled_graph_schedule.png\n",
      "2025-06-01 19:14:26,855\tINFO compiled_dag_node.py:2774 -- Tearing down compiled DAG\n",
      "\u001b[32mPASSED\u001b[0m2025-06-01 19:14:26,857\tINFO compiled_dag_node.py:2779 -- Cancelling compiled worker on actor: Actor(TorchTensorWorker, cb7bcc310f3ce9d926eb299601000000)\n",
      "2025-06-01 19:14:26,857\tINFO compiled_dag_node.py:2779 -- Cancelling compiled worker on actor: Actor(TorchTensorWorker, ad3565bb44b98be5e0103c3701000000)\n",
      "\u001b[36m(TorchTensorWorker pid=34124)\u001b[0m ERROR:root:Compiled DAG task exited with exception\n",
      "\u001b[36m(TorchTensorWorker pid=34124)\u001b[0m Traceback (most recent call last):\n",
      "\u001b[36m(TorchTensorWorker pid=34124)\u001b[0m   File \"/home/wxdeng/stbr-min/python/ray/dag/compiled_dag_node.py\", line 330, in do_exec_tasks_EXP\n",
      "\u001b[36m(TorchTensorWorker pid=34124)\u001b[0m     done = tasks[operation.exec_task_idx].exec_operation(\n",
      "\u001b[36m(TorchTensorWorker pid=34124)\u001b[0m         self, overlap_gpu_communication\n",
      "\u001b[36m(TorchTensorWorker pid=34124)\u001b[0m     )\n",
      "\u001b[36m(TorchTensorWorker pid=34124)\u001b[0m   File \"/home/wxdeng/stbr-min/python/ray/dag/compiled_dag_node.py\", line 1069, in exec_operation\n",
      "\u001b[36m(TorchTensorWorker pid=34124)\u001b[0m     return self.exec_operation_inner(\n",
      "\u001b[36m(TorchTensorWorker pid=34124)\u001b[0m            ~~~~~~~~~~~~~~~~~~~~~~~~~^\n",
      "\u001b[36m(TorchTensorWorker pid=34124)\u001b[0m         class_handle, overlap_gpu_communication\n",
      "\u001b[36m(TorchTensorWorker pid=34124)\u001b[0m         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[36m(TorchTensorWorker pid=34124)\u001b[0m     )\n",
      "\u001b[36m(TorchTensorWorker pid=34124)\u001b[0m     ^\n",
      "\u001b[36m(TorchTensorWorker pid=34124)\u001b[0m   File \"/home/wxdeng/stbr-min/python/ray/dag/compiled_dag_node.py\", line 1132, in exec_operation_inner\n",
      "\u001b[36m(TorchTensorWorker pid=34124)\u001b[0m     raise exc\n",
      "\u001b[36m(TorchTensorWorker pid=34124)\u001b[0m   File \"/home/wxdeng/stbr-min/python/ray/dag/compiled_dag_node.py\", line 1127, in exec_operation_inner\n",
      "\u001b[36m(TorchTensorWorker pid=34124)\u001b[0m     output_val = method(*input_values, **self.resolved_kwargs)\n",
      "\u001b[36m(TorchTensorWorker pid=34124)\u001b[0m   File \"/home/wxdeng/stbr-min/python/ray/dag/communication_node.py\", line 72, in execute\n",
      "\u001b[36m(TorchTensorWorker pid=34124)\u001b[0m     self.nccl_ch.write(data)\n",
      "\u001b[36m(TorchTensorWorker pid=34124)\u001b[0m     ~~~~~~~~~~~~~~~~~~^^^^^^\n",
      "\u001b[36m(TorchTensorWorker pid=34124)\u001b[0m   File \"/home/wxdeng/stbr-min/python/ray/experimental/channel/torch_tensor_nccl_channel.py\", line 243, in write\n",
      "\u001b[36m(TorchTensorWorker pid=34124)\u001b[0m     raise value\n",
      "\u001b[36m(TorchTensorWorker pid=34124)\u001b[0m   File \"/home/wxdeng/stbr-min/python/ray/dag/compiled_dag_node.py\", line 1091, in exec_operation_inner\n",
      "\u001b[36m(TorchTensorWorker pid=34124)\u001b[0m     _process_return_vals(input_data, return_single_output=False)\n",
      "\u001b[36m(TorchTensorWorker pid=34124)\u001b[0m     ~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[36m(TorchTensorWorker pid=34124)\u001b[0m   File \"/home/wxdeng/stbr-min/python/ray/experimental/compiled_dag_ref.py\", line 27, in _process_return_vals\n",
      "\u001b[36m(TorchTensorWorker pid=34124)\u001b[0m     raise val.as_instanceof_cause()\n",
      "\u001b[36m(TorchTensorWorker pid=34124)\u001b[0m ray.exceptions.RayTaskError(RuntimeError): \u001b[36mray::TorchTensorWorker.__ray_call__()\u001b[39m (pid=34124, ip=128.208.3.124)\n",
      "\u001b[36m(TorchTensorWorker pid=34124)\u001b[0m   File \"/home/wxdeng/stbr-min/python/ray/dag/tests/experimental/test_torch_tensor_dag.py\", line 67, in send_or_raise\n",
      "\u001b[36m(TorchTensorWorker pid=34124)\u001b[0m     raise RuntimeError()\n",
      "\u001b[36m(TorchTensorWorker pid=34124)\u001b[0m RuntimeError\n",
      "\u001b[36m(TorchTensorWorker pid=34125)\u001b[0m Destructing NCCL group on actor: Actor(TorchTensorWorker, ad3565bb44b98be5e0103c3701000000)\n",
      "\u001b[36m(TorchTensorWorker pid=34124)\u001b[0m Destructing NCCL group on actor: Actor(TorchTensorWorker, cb7bcc310f3ce9d926eb299601000000)\n",
      "2025-06-01 19:14:27,344\tINFO compiled_dag_node.py:2801 -- Waiting for worker tasks to exit\n",
      "2025-06-01 19:14:27,344\tINFO compiled_dag_node.py:2804 -- Teardown complete\n",
      "\n",
      "test_torch_tensor_dag.py::test_torch_tensor_exceptions[True-False-False-ray_start_regular0] 2025-06-01 19:14:30,405\tINFO worker.py:1917 -- Started a local Ray instance.\n",
      "2025-06-01 19:14:32,627\tINFO torch_tensor_nccl_channel.py:772 -- Creating NCCL group ac821120-9a84-41ef-bc08-469106377f31 on actors: [Actor(TorchTensorWorker, 161b1e4d2c4800402c45ed2e01000000), Actor(TorchTensorWorker, 6821e6ad86d358f0f7631c0101000000)]\n",
      "2025-06-01 19:14:33,710\tINFO torch_tensor_nccl_channel.py:797 -- NCCL group initialized.\n",
      "2025-06-01 19:14:33,904\tINFO dag_node_operation.py:1108 -- Writing compiled graph schedule visualization to compiled_graph_schedule.png\n",
      "\u001b[32mPASSED\u001b[0m2025-06-01 19:14:34,099\tINFO compiled_dag_node.py:2774 -- Tearing down compiled DAG\n",
      "2025-06-01 19:14:34,100\tINFO compiled_dag_node.py:2779 -- Cancelling compiled worker on actor: Actor(TorchTensorWorker, 161b1e4d2c4800402c45ed2e01000000)\n",
      "2025-06-01 19:14:34,100\tINFO compiled_dag_node.py:2779 -- Cancelling compiled worker on actor: Actor(TorchTensorWorker, 6821e6ad86d358f0f7631c0101000000)\n",
      "\u001b[36m(TorchTensorWorker pid=34825)\u001b[0m Destructing NCCL group on actor: Actor(TorchTensorWorker, 161b1e4d2c4800402c45ed2e01000000)\n",
      "\u001b[36m(TorchTensorWorker pid=34824)\u001b[0m Destructing NCCL group on actor: Actor(TorchTensorWorker, 6821e6ad86d358f0f7631c0101000000)\n",
      "2025-06-01 19:14:34,620\tINFO compiled_dag_node.py:2801 -- Waiting for worker tasks to exit\n",
      "2025-06-01 19:14:34,620\tINFO compiled_dag_node.py:2762 -- Killing actor: Actor(TorchTensorWorker, 161b1e4d2c4800402c45ed2e01000000)\n",
      "2025-06-01 19:14:34,621\tINFO compiled_dag_node.py:2762 -- Killing actor: Actor(TorchTensorWorker, 6821e6ad86d358f0f7631c0101000000)\n",
      "2025-06-01 19:14:34,621\tINFO compiled_dag_node.py:2804 -- Teardown complete\n",
      "\n",
      "test_torch_tensor_dag.py::test_torch_tensor_exceptions[True-False-True-ray_start_regular0] 2025-06-01 19:14:38,507\tINFO worker.py:1917 -- Started a local Ray instance.\n",
      "2025-06-01 19:14:40,759\tINFO torch_tensor_nccl_channel.py:772 -- Creating NCCL group 5e51d77f-4921-44d9-a65e-2d92ce95a79f on actors: [Actor(TorchTensorWorker, bfbff2c8fdbcbb7b4c95d2df01000000), Actor(TorchTensorWorker, 9f77b23fda0fef97aa1bcfa101000000)]\n",
      "2025-06-01 19:14:41,849\tINFO torch_tensor_nccl_channel.py:797 -- NCCL group initialized.\n",
      "2025-06-01 19:14:42,042\tINFO dag_node_operation.py:1108 -- Writing compiled graph schedule visualization to compiled_graph_schedule.png\n",
      "2025-06-01 19:14:42,266\tINFO compiled_dag_node.py:2774 -- Tearing down compiled DAG\n",
      "\u001b[32mPASSED\u001b[0m2025-06-01 19:14:42,269\tINFO compiled_dag_node.py:2779 -- Cancelling compiled worker on actor: Actor(TorchTensorWorker, bfbff2c8fdbcbb7b4c95d2df01000000)\n",
      "2025-06-01 19:14:42,269\tINFO compiled_dag_node.py:2779 -- Cancelling compiled worker on actor: Actor(TorchTensorWorker, 9f77b23fda0fef97aa1bcfa101000000)\n",
      "\u001b[36m(TorchTensorWorker pid=35581)\u001b[0m Destructing NCCL group on actor: Actor(TorchTensorWorker, 9f77b23fda0fef97aa1bcfa101000000)\n",
      "\u001b[36m(TorchTensorWorker pid=35580)\u001b[0m ERROR:root:Compiled DAG task exited with exception\n",
      "\u001b[36m(TorchTensorWorker pid=35580)\u001b[0m Traceback (most recent call last):\n",
      "\u001b[36m(TorchTensorWorker pid=35580)\u001b[0m   File \"/home/wxdeng/stbr-min/python/ray/dag/compiled_dag_node.py\", line 330, in do_exec_tasks_EXP\n",
      "\u001b[36m(TorchTensorWorker pid=35580)\u001b[0m     done = tasks[operation.exec_task_idx].exec_operation(\n",
      "\u001b[36m(TorchTensorWorker pid=35580)\u001b[0m         self, overlap_gpu_communication\n",
      "\u001b[36m(TorchTensorWorker pid=35580)\u001b[0m     )\n",
      "\u001b[36m(TorchTensorWorker pid=35580)\u001b[0m   File \"/home/wxdeng/stbr-min/python/ray/dag/compiled_dag_node.py\", line 1069, in exec_operation\n",
      "\u001b[36m(TorchTensorWorker pid=35580)\u001b[0m     return self.exec_operation_inner(\n",
      "\u001b[36m(TorchTensorWorker pid=35580)\u001b[0m            ~~~~~~~~~~~~~~~~~~~~~~~~~^\n",
      "\u001b[36m(TorchTensorWorker pid=35580)\u001b[0m         class_handle, overlap_gpu_communication\n",
      "\u001b[36m(TorchTensorWorker pid=35580)\u001b[0m         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[36m(TorchTensorWorker pid=35580)\u001b[0m     )\n",
      "\u001b[36m(TorchTensorWorker pid=35580)\u001b[0m     ^\n",
      "\u001b[36m(TorchTensorWorker pid=35580)\u001b[0m   File \"/home/wxdeng/stbr-min/python/ray/dag/compiled_dag_node.py\", line 1132, in exec_operation_inner\n",
      "\u001b[36m(TorchTensorWorker pid=35580)\u001b[0m     raise exc\n",
      "\u001b[36m(TorchTensorWorker pid=35580)\u001b[0m   File \"/home/wxdeng/stbr-min/python/ray/dag/compiled_dag_node.py\", line 1127, in exec_operation_inner\n",
      "\u001b[36m(TorchTensorWorker pid=35580)\u001b[0m     output_val = method(*input_values, **self.resolved_kwargs)\n",
      "\u001b[36m(TorchTensorWorker pid=35580)\u001b[0m   File \"/home/wxdeng/stbr-min/python/ray/dag/communication_node.py\", line 72, in execute\n",
      "\u001b[36m(TorchTensorWorker pid=35580)\u001b[0m     self.nccl_ch.write(data)\n",
      "\u001b[36m(TorchTensorWorker pid=35580)\u001b[0m     ~~~~~~~~~~~~~~~~~~^^^^^^\n",
      "\u001b[36m(TorchTensorWorker pid=35580)\u001b[0m   File \"/home/wxdeng/stbr-min/python/ray/experimental/channel/torch_tensor_nccl_channel.py\", line 243, in write\n",
      "\u001b[36m(TorchTensorWorker pid=35580)\u001b[0m     raise value\n",
      "\u001b[36m(TorchTensorWorker pid=35580)\u001b[0m   File \"/home/wxdeng/stbr-min/python/ray/dag/compiled_dag_node.py\", line 1091, in exec_operation_inner\n",
      "\u001b[36m(TorchTensorWorker pid=35580)\u001b[0m     _process_return_vals(input_data, return_single_output=False)\n",
      "\u001b[36m(TorchTensorWorker pid=35580)\u001b[0m     ~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[36m(TorchTensorWorker pid=35580)\u001b[0m   File \"/home/wxdeng/stbr-min/python/ray/experimental/compiled_dag_ref.py\", line 27, in _process_return_vals\n",
      "\u001b[36m(TorchTensorWorker pid=35580)\u001b[0m     raise val.as_instanceof_cause()\n",
      "\u001b[36m(TorchTensorWorker pid=35580)\u001b[0m ray.exceptions.RayTaskError(RuntimeError): \u001b[36mray::TorchTensorWorker.__ray_call__()\u001b[39m (pid=35580, ip=128.208.3.124)\n",
      "\u001b[36m(TorchTensorWorker pid=35580)\u001b[0m   File \"/home/wxdeng/stbr-min/python/ray/dag/tests/experimental/test_torch_tensor_dag.py\", line 67, in send_or_raise\n",
      "\u001b[36m(TorchTensorWorker pid=35580)\u001b[0m     raise RuntimeError()\n",
      "\u001b[36m(TorchTensorWorker pid=35580)\u001b[0m RuntimeError\n",
      "\u001b[36m(TorchTensorWorker pid=35580)\u001b[0m Destructing NCCL group on actor: Actor(TorchTensorWorker, bfbff2c8fdbcbb7b4c95d2df01000000)\n",
      "2025-06-01 19:14:42,760\tINFO compiled_dag_node.py:2801 -- Waiting for worker tasks to exit\n",
      "2025-06-01 19:14:42,761\tINFO compiled_dag_node.py:2804 -- Teardown complete\n",
      "\n",
      "test_torch_tensor_dag.py::test_torch_tensor_exceptions[True-True-False-ray_start_regular0] 2025-06-01 19:14:45,908\tINFO worker.py:1917 -- Started a local Ray instance.\n",
      "2025-06-01 19:14:48,159\tINFO torch_tensor_nccl_channel.py:772 -- Creating NCCL group b9b827c0-e99c-4d4f-8e02-f35560278d40 on actors: [Actor(TorchTensorWorker, 5781dac1eac53366f663f7aa01000000), Actor(TorchTensorWorker, 66a66aff5096a7d40c67624e01000000)]\n",
      "2025-06-01 19:14:49,241\tINFO torch_tensor_nccl_channel.py:797 -- NCCL group initialized.\n",
      "2025-06-01 19:14:49,434\tINFO dag_node_operation.py:1108 -- Writing compiled graph schedule visualization to compiled_graph_schedule.png\n",
      "2025-06-01 19:14:49,634\tINFO compiled_dag_node.py:2774 -- Tearing down compiled DAG\n",
      "\u001b[32mPASSED\u001b[0m2025-06-01 19:14:49,636\tINFO compiled_dag_node.py:2779 -- Cancelling compiled worker on actor: Actor(TorchTensorWorker, 5781dac1eac53366f663f7aa01000000)\n",
      "2025-06-01 19:14:49,636\tINFO compiled_dag_node.py:2779 -- Cancelling compiled worker on actor: Actor(TorchTensorWorker, 66a66aff5096a7d40c67624e01000000)\n",
      "\u001b[36m(TorchTensorWorker pid=36230)\u001b[0m ERROR:root:Compiled DAG task exited with exception\n",
      "\u001b[36m(TorchTensorWorker pid=36230)\u001b[0m Traceback (most recent call last):\n",
      "\u001b[36m(TorchTensorWorker pid=36230)\u001b[0m   File \"/home/wxdeng/stbr-min/python/ray/dag/compiled_dag_node.py\", line 330, in do_exec_tasks_EXP\n",
      "\u001b[36m(TorchTensorWorker pid=36230)\u001b[0m     done = tasks[operation.exec_task_idx].exec_operation(\n",
      "\u001b[36m(TorchTensorWorker pid=36230)\u001b[0m         self, overlap_gpu_communication\n",
      "\u001b[36m(TorchTensorWorker pid=36230)\u001b[0m     )\n",
      "\u001b[36m(TorchTensorWorker pid=36230)\u001b[0m   File \"/home/wxdeng/stbr-min/python/ray/dag/compiled_dag_node.py\", line 1069, in exec_operation\n",
      "\u001b[36m(TorchTensorWorker pid=36230)\u001b[0m     return self.exec_operation_inner(\n",
      "\u001b[36m(TorchTensorWorker pid=36230)\u001b[0m            ~~~~~~~~~~~~~~~~~~~~~~~~~^\n",
      "\u001b[36m(TorchTensorWorker pid=36230)\u001b[0m         class_handle, overlap_gpu_communication\n",
      "\u001b[36m(TorchTensorWorker pid=36230)\u001b[0m         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[36m(TorchTensorWorker pid=36230)\u001b[0m     )\n",
      "\u001b[36m(TorchTensorWorker pid=36230)\u001b[0m     ^\n",
      "\u001b[36m(TorchTensorWorker pid=36230)\u001b[0m   File \"/home/wxdeng/stbr-min/python/ray/dag/compiled_dag_node.py\", line 1132, in exec_operation_inner\n",
      "\u001b[36m(TorchTensorWorker pid=36230)\u001b[0m     raise exc\n",
      "\u001b[36m(TorchTensorWorker pid=36230)\u001b[0m   File \"/home/wxdeng/stbr-min/python/ray/dag/compiled_dag_node.py\", line 1127, in exec_operation_inner\n",
      "\u001b[36m(TorchTensorWorker pid=36230)\u001b[0m     output_val = method(*input_values, **self.resolved_kwargs)\n",
      "\u001b[36m(TorchTensorWorker pid=36230)\u001b[0m   File \"/home/wxdeng/stbr-min/python/ray/dag/communication_node.py\", line 72, in execute\n",
      "\u001b[36m(TorchTensorWorker pid=36230)\u001b[0m     self.nccl_ch.write(data)\n",
      "\u001b[36m(TorchTensorWorker pid=36230)\u001b[0m     ~~~~~~~~~~~~~~~~~~^^^^^^\n",
      "\u001b[36m(TorchTensorWorker pid=36230)\u001b[0m   File \"/home/wxdeng/stbr-min/python/ray/experimental/channel/torch_tensor_nccl_channel.py\", line 243, in write\n",
      "\u001b[36m(TorchTensorWorker pid=36230)\u001b[0m     raise value\n",
      "\u001b[36m(TorchTensorWorker pid=36230)\u001b[0m   File \"/home/wxdeng/stbr-min/python/ray/dag/compiled_dag_node.py\", line 1091, in exec_operation_inner\n",
      "\u001b[36m(TorchTensorWorker pid=36230)\u001b[0m     _process_return_vals(input_data, return_single_output=False)\n",
      "\u001b[36m(TorchTensorWorker pid=36230)\u001b[0m     ~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[36m(TorchTensorWorker pid=36230)\u001b[0m   File \"/home/wxdeng/stbr-min/python/ray/experimental/compiled_dag_ref.py\", line 27, in _process_return_vals\n",
      "\u001b[36m(TorchTensorWorker pid=36230)\u001b[0m     raise val.as_instanceof_cause()\n",
      "\u001b[36m(TorchTensorWorker pid=36230)\u001b[0m ray.exceptions.RayTaskError(RuntimeError): \u001b[36mray::TorchTensorWorker.__ray_call__()\u001b[39m (pid=36230, ip=128.208.3.124)\n",
      "\u001b[36m(TorchTensorWorker pid=36230)\u001b[0m   File \"/home/wxdeng/stbr-min/python/ray/dag/tests/experimental/test_torch_tensor_dag.py\", line 67, in send_or_raise\n",
      "\u001b[36m(TorchTensorWorker pid=36230)\u001b[0m     raise RuntimeError()\n",
      "\u001b[36m(TorchTensorWorker pid=36230)\u001b[0m RuntimeError\n",
      "\u001b[36m(TorchTensorWorker pid=36230)\u001b[0m Destructing NCCL group on actor: Actor(TorchTensorWorker, 5781dac1eac53366f663f7aa01000000)\n",
      "\u001b[36m(TorchTensorWorker pid=36231)\u001b[0m Destructing NCCL group on actor: Actor(TorchTensorWorker, 66a66aff5096a7d40c67624e01000000)\n",
      "2025-06-01 19:14:50,155\tINFO compiled_dag_node.py:2801 -- Waiting for worker tasks to exit\n",
      "2025-06-01 19:14:50,156\tINFO compiled_dag_node.py:2804 -- Teardown complete\n",
      "\n",
      "test_torch_tensor_dag.py::test_torch_tensor_exceptions[True-True-True-ray_start_regular0] 2025-06-01 19:14:53,214\tINFO worker.py:1917 -- Started a local Ray instance.\n",
      "2025-06-01 19:14:55,421\tINFO torch_tensor_nccl_channel.py:772 -- Creating NCCL group b29a44d2-687f-4717-a674-2395164b00d9 on actors: [Actor(TorchTensorWorker, c3fd9424732846f454fe02b001000000), Actor(TorchTensorWorker, 5cbc93121715021436cd8c6201000000)]\n",
      "2025-06-01 19:14:56,506\tINFO torch_tensor_nccl_channel.py:797 -- NCCL group initialized.\n",
      "2025-06-01 19:14:56,702\tINFO dag_node_operation.py:1108 -- Writing compiled graph schedule visualization to compiled_graph_schedule.png\n",
      "\u001b[36m(TorchTensorWorker pid=36986)\u001b[0m ERROR:root:Compiled DAG task exited with exception\n",
      "\u001b[36m(TorchTensorWorker pid=36986)\u001b[0m Traceback (most recent call last):\n",
      "\u001b[36m(TorchTensorWorker pid=36986)\u001b[0m   File \"/home/wxdeng/stbr-min/python/ray/dag/compiled_dag_node.py\", line 330, in do_exec_tasks_EXP\n",
      "\u001b[36m(TorchTensorWorker pid=36986)\u001b[0m     done = tasks[operation.exec_task_idx].exec_operation(\n",
      "\u001b[36m(TorchTensorWorker pid=36986)\u001b[0m         self, overlap_gpu_communication\n",
      "\u001b[36m(TorchTensorWorker pid=36986)\u001b[0m     )\n",
      "\u001b[36m(TorchTensorWorker pid=36986)\u001b[0m   File \"/home/wxdeng/stbr-min/python/ray/dag/compiled_dag_node.py\", line 1069, in exec_operation\n",
      "\u001b[36m(TorchTensorWorker pid=36986)\u001b[0m     return self.exec_operation_inner(\n",
      "\u001b[36m(TorchTensorWorker pid=36986)\u001b[0m            ~~~~~~~~~~~~~~~~~~~~~~~~~^\n",
      "\u001b[36m(TorchTensorWorker pid=36986)\u001b[0m         class_handle, overlap_gpu_communication\n",
      "\u001b[36m(TorchTensorWorker pid=36986)\u001b[0m         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[36m(TorchTensorWorker pid=36986)\u001b[0m     )\n",
      "\u001b[36m(TorchTensorWorker pid=36986)\u001b[0m     ^\n",
      "\u001b[36m(TorchTensorWorker pid=36986)\u001b[0m   File \"/home/wxdeng/stbr-min/python/ray/dag/compiled_dag_node.py\", line 1132, in exec_operation_inner\n",
      "\u001b[36m(TorchTensorWorker pid=36986)\u001b[0m     raise exc\n",
      "\u001b[36m(TorchTensorWorker pid=36986)\u001b[0m   File \"/home/wxdeng/stbr-min/python/ray/dag/compiled_dag_node.py\", line 1127, in exec_operation_inner\n",
      "\u001b[36m(TorchTensorWorker pid=36986)\u001b[0m     output_val = method(*input_values, **self.resolved_kwargs)\n",
      "\u001b[36m(TorchTensorWorker pid=36986)\u001b[0m   File \"/home/wxdeng/stbr-min/python/ray/dag/communication_node.py\", line 72, in execute\n",
      "\u001b[36m(TorchTensorWorker pid=36986)\u001b[0m     self.nccl_ch.write(data)\n",
      "\u001b[36m(TorchTensorWorker pid=36986)\u001b[0m     ~~~~~~~~~~~~~~~~~~^^^^^^\n",
      "\u001b[36m(TorchTensorWorker pid=36986)\u001b[0m   File \"/home/wxdeng/stbr-min/python/ray/experimental/channel/torch_tensor_nccl_channel.py\", line 243, in write\n",
      "\u001b[36m(TorchTensorWorker pid=36986)\u001b[0m     raise value\n",
      "\u001b[36m(TorchTensorWorker pid=36986)\u001b[0m   File \"/home/wxdeng/stbr-min/python/ray/dag/compiled_dag_node.py\", line 1091, in exec_operation_inner\n",
      "\u001b[36m(TorchTensorWorker pid=36986)\u001b[0m     _process_return_vals(input_data, return_single_output=False)\n",
      "\u001b[36m(TorchTensorWorker pid=36986)\u001b[0m     ~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[36m(TorchTensorWorker pid=36986)\u001b[0m   File \"/home/wxdeng/stbr-min/python/ray/experimental/compiled_dag_ref.py\", line 27, in _process_return_vals\n",
      "\u001b[36m(TorchTensorWorker pid=36986)\u001b[0m     raise val.as_instanceof_cause()\n",
      "\u001b[36m(TorchTensorWorker pid=36986)\u001b[0m ray.exceptions.RayTaskError(RuntimeError): \u001b[36mray::TorchTensorWorker.__ray_call__()\u001b[39m (pid=36986, ip=128.208.3.124)\n",
      "\u001b[36m(TorchTensorWorker pid=36986)\u001b[0m   File \"/home/wxdeng/stbr-min/python/ray/dag/tests/experimental/test_torch_tensor_dag.py\", line 67, in send_or_raise\n",
      "\u001b[36m(TorchTensorWorker pid=36986)\u001b[0m     raise RuntimeError()\n",
      "\u001b[36m(TorchTensorWorker pid=36986)\u001b[0m RuntimeError\n",
      "2025-06-01 19:14:56,903\tINFO compiled_dag_node.py:2774 -- Tearing down compiled DAG\n",
      "\u001b[32mPASSED\u001b[0m2025-06-01 19:14:56,905\tINFO compiled_dag_node.py:2779 -- Cancelling compiled worker on actor: Actor(TorchTensorWorker, c3fd9424732846f454fe02b001000000)\n",
      "2025-06-01 19:14:56,905\tINFO compiled_dag_node.py:2779 -- Cancelling compiled worker on actor: Actor(TorchTensorWorker, 5cbc93121715021436cd8c6201000000)\n",
      "\u001b[36m(TorchTensorWorker pid=36985)\u001b[0m Destructing NCCL group on actor: Actor(TorchTensorWorker, 5cbc93121715021436cd8c6201000000)\n",
      "\u001b[36m(TorchTensorWorker pid=36986)\u001b[0m Destructing NCCL group on actor: Actor(TorchTensorWorker, c3fd9424732846f454fe02b001000000)\n",
      "2025-06-01 19:14:57,395\tINFO compiled_dag_node.py:2801 -- Waiting for worker tasks to exit\n",
      "2025-06-01 19:14:57,395\tINFO compiled_dag_node.py:2804 -- Teardown complete\n",
      "\n",
      "test_torch_tensor_dag.py::test_torch_tensor_exceptions2[ray_start_regular0] 2025-06-01 19:15:01,284\tINFO worker.py:1917 -- Started a local Ray instance.\n",
      "2025-06-01 19:15:03,517\tINFO torch_tensor_nccl_channel.py:772 -- Creating NCCL group 1b8a1367-c3e2-4690-891e-21a5f2e88056 on actors: [Actor(TorchTensorWorker, 37dfd56726e6932ae3ecefa601000000), Actor(TorchTensorWorker, ded1b1721eb678595a5db5fe01000000)]\n",
      "2025-06-01 19:15:04,609\tINFO torch_tensor_nccl_channel.py:797 -- NCCL group initialized.\n",
      "2025-06-01 19:15:04,693\tINFO dag_node_operation.py:1108 -- Writing compiled graph schedule visualization to compiled_graph_schedule.png\n",
      "2025-06-01 19:15:04,823\tINFO compiled_dag_node.py:2774 -- Tearing down compiled DAG\n",
      "\u001b[32mPASSED\u001b[0m2025-06-01 19:15:04,825\tINFO compiled_dag_node.py:2779 -- Cancelling compiled worker on actor: Actor(TorchTensorWorker, ded1b1721eb678595a5db5fe01000000)\n",
      "2025-06-01 19:15:04,825\tINFO compiled_dag_node.py:2779 -- Cancelling compiled worker on actor: Actor(TorchTensorWorker, 37dfd56726e6932ae3ecefa601000000)\n",
      "\u001b[36m(TorchTensorWorker pid=37746)\u001b[0m ERROR:root:Compiled DAG task exited with exception\n",
      "\u001b[36m(TorchTensorWorker pid=37746)\u001b[0m Traceback (most recent call last):\n",
      "\u001b[36m(TorchTensorWorker pid=37746)\u001b[0m   File \"/home/wxdeng/stbr-min/python/ray/dag/compiled_dag_node.py\", line 330, in do_exec_tasks_EXP\n",
      "\u001b[36m(TorchTensorWorker pid=37746)\u001b[0m     done = tasks[operation.exec_task_idx].exec_operation(\n",
      "\u001b[36m(TorchTensorWorker pid=37746)\u001b[0m         self, overlap_gpu_communication\n",
      "\u001b[36m(TorchTensorWorker pid=37746)\u001b[0m     )\n",
      "\u001b[36m(TorchTensorWorker pid=37746)\u001b[0m   File \"/home/wxdeng/stbr-min/python/ray/dag/compiled_dag_node.py\", line 1069, in exec_operation\n",
      "\u001b[36m(TorchTensorWorker pid=37746)\u001b[0m     return self.exec_operation_inner(\n",
      "\u001b[36m(TorchTensorWorker pid=37746)\u001b[0m            ~~~~~~~~~~~~~~~~~~~~~~~~~^\n",
      "\u001b[36m(TorchTensorWorker pid=37746)\u001b[0m         class_handle, overlap_gpu_communication\n",
      "\u001b[36m(TorchTensorWorker pid=37746)\u001b[0m         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[36m(TorchTensorWorker pid=37746)\u001b[0m     )\n",
      "\u001b[36m(TorchTensorWorker pid=37746)\u001b[0m     ^\n",
      "\u001b[36m(TorchTensorWorker pid=37746)\u001b[0m   File \"/home/wxdeng/stbr-min/python/ray/dag/compiled_dag_node.py\", line 1132, in exec_operation_inner\n",
      "\u001b[36m(TorchTensorWorker pid=37746)\u001b[0m     raise exc\n",
      "\u001b[36m(TorchTensorWorker pid=37746)\u001b[0m   File \"/home/wxdeng/stbr-min/python/ray/dag/compiled_dag_node.py\", line 1127, in exec_operation_inner\n",
      "\u001b[36m(TorchTensorWorker pid=37746)\u001b[0m     output_val = method(*input_values, **self.resolved_kwargs)\n",
      "\u001b[36m(TorchTensorWorker pid=37746)\u001b[0m   File \"/home/wxdeng/stbr-min/python/ray/dag/communication_node.py\", line 72, in execute\n",
      "\u001b[36m(TorchTensorWorker pid=37746)\u001b[0m     self.nccl_ch.write(data)\n",
      "\u001b[36m(TorchTensorWorker pid=37746)\u001b[0m     ~~~~~~~~~~~~~~~~~~^^^^^^\n",
      "\u001b[36m(TorchTensorWorker pid=37746)\u001b[0m   File \"/home/wxdeng/stbr-min/python/ray/experimental/channel/torch_tensor_nccl_channel.py\", line 257, in write\n",
      "\u001b[36m(TorchTensorWorker pid=37746)\u001b[0m     raise ValueError(\n",
      "\u001b[36m(TorchTensorWorker pid=37746)\u001b[0m     ...<3 lines>...\n",
      "\u001b[36m(TorchTensorWorker pid=37746)\u001b[0m     )\n",
      "\u001b[36m(TorchTensorWorker pid=37746)\u001b[0m ValueError: Task annotated with _direct_return=True must return a CUDA torch.Tensor, instead found value `1`. DAG will shut down.\n",
      "\u001b[36m(TorchTensorWorker pid=37746)\u001b[0m Destructing NCCL group on actor: Actor(TorchTensorWorker, ded1b1721eb678595a5db5fe01000000)\n",
      "\u001b[36m(TorchTensorWorker pid=37747)\u001b[0m Destructing NCCL group on actor: Actor(TorchTensorWorker, 37dfd56726e6932ae3ecefa601000000)\n",
      "2025-06-01 19:15:05,615\tINFO compiled_dag_node.py:2801 -- Waiting for worker tasks to exit\n",
      "2025-06-01 19:15:05,616\tINFO compiled_dag_node.py:2804 -- Teardown complete\n",
      "\n",
      "test_torch_tensor_dag.py::test_torch_tensor_explicit_communicator[ray_start_regular0] 2025-06-01 19:15:08,702\tINFO worker.py:1917 -- Started a local Ray instance.\n",
      "\u001b[36m(pid=gcs_server)\u001b[0m [2025-06-01 19:15:10,610 E 38202 38202] (gcs_server) gcs_actor_manager.cc:1788: Failed to kill actor d475b0ae22c449e009351f7101000000, status: RpcError: RPC Error message: Socket closed; RPC Error details:  rpc_code: 14\n",
      "\u001b[32mPASSED\u001b[0m\n",
      "test_torch_tensor_dag.py::test_torch_tensor_nccl_collective_ops[operation0-None-ray_start_regular0] 2025-06-01 19:15:14,620\tINFO worker.py:1917 -- Started a local Ray instance.\n",
      "2025-06-01 19:15:16,882\tINFO torch_tensor_nccl_channel.py:772 -- Creating NCCL group b9fe49a7-a3d5-4614-bbe9-a6675b0b15f0 on actors: [Actor(TorchTensorWorker, 323747685516e3b19b9607a101000000), Actor(TorchTensorWorker, 5e06daeb2b6c299059f4ffb501000000)]\n",
      "2025-06-01 19:15:17,986\tINFO torch_tensor_nccl_channel.py:797 -- NCCL group initialized.\n",
      "2025-06-01 19:15:18,166\tINFO dag_node_operation.py:1108 -- Writing compiled graph schedule visualization to compiled_graph_schedule.png\n",
      "\u001b[32mPASSED\u001b[0m2025-06-01 19:15:18,662\tINFO compiled_dag_node.py:2774 -- Tearing down compiled DAG\n",
      "2025-06-01 19:15:18,662\tINFO compiled_dag_node.py:2779 -- Cancelling compiled worker on actor: Actor(TorchTensorWorker, 323747685516e3b19b9607a101000000)\n",
      "2025-06-01 19:15:18,662\tINFO compiled_dag_node.py:2779 -- Cancelling compiled worker on actor: Actor(TorchTensorWorker, 5e06daeb2b6c299059f4ffb501000000)\n",
      "\u001b[36m(TorchTensorWorker pid=39180)\u001b[0m Destructing NCCL group on actor: Actor(TorchTensorWorker, 323747685516e3b19b9607a101000000)\n",
      "\u001b[36m(TorchTensorWorker pid=39181)\u001b[0m Destructing NCCL group on actor: Actor(TorchTensorWorker, 5e06daeb2b6c299059f4ffb501000000)\n",
      "2025-06-01 19:15:18,877\tINFO compiled_dag_node.py:2801 -- Waiting for worker tasks to exit\n",
      "2025-06-01 19:15:18,878\tINFO compiled_dag_node.py:2762 -- Killing actor: Actor(TorchTensorWorker, 323747685516e3b19b9607a101000000)\n",
      "2025-06-01 19:15:18,878\tINFO compiled_dag_node.py:2762 -- Killing actor: Actor(TorchTensorWorker, 5e06daeb2b6c299059f4ffb501000000)\n",
      "2025-06-01 19:15:18,878\tINFO compiled_dag_node.py:2804 -- Teardown complete\n",
      "\n",
      "test_torch_tensor_dag.py::test_torch_tensor_nccl_collective_ops[operation1-ReduceOp.SUM-ray_start_regular0] 2025-06-01 19:15:21,528\tINFO worker.py:1917 -- Started a local Ray instance.\n",
      "2025-06-01 19:15:23,803\tINFO torch_tensor_nccl_channel.py:772 -- Creating NCCL group d11a2ee8-72dd-4312-8a01-2952d072e0f2 on actors: [Actor(TorchTensorWorker, 8fdf4a6f922fbbdcd30dcfd601000000), Actor(TorchTensorWorker, 9865cb7cd448de00aab7dc2801000000)]\n",
      "2025-06-01 19:15:24,662\tINFO torch_tensor_nccl_channel.py:797 -- NCCL group initialized.\n",
      "2025-06-01 19:15:24,841\tINFO dag_node_operation.py:1108 -- Writing compiled graph schedule visualization to compiled_graph_schedule.png\n",
      "\u001b[32mPASSED\u001b[0m2025-06-01 19:15:25,059\tINFO compiled_dag_node.py:2774 -- Tearing down compiled DAG\n",
      "2025-06-01 19:15:25,060\tINFO compiled_dag_node.py:2779 -- Cancelling compiled worker on actor: Actor(TorchTensorWorker, 8fdf4a6f922fbbdcd30dcfd601000000)\n",
      "2025-06-01 19:15:25,060\tINFO compiled_dag_node.py:2779 -- Cancelling compiled worker on actor: Actor(TorchTensorWorker, 9865cb7cd448de00aab7dc2801000000)\n",
      "\u001b[36m(TorchTensorWorker pid=39883)\u001b[0m Destructing NCCL group on actor: Actor(TorchTensorWorker, 8fdf4a6f922fbbdcd30dcfd601000000)\n",
      "\u001b[36m(TorchTensorWorker pid=39884)\u001b[0m Destructing NCCL group on actor: Actor(TorchTensorWorker, 9865cb7cd448de00aab7dc2801000000)\n",
      "2025-06-01 19:15:25,583\tINFO compiled_dag_node.py:2801 -- Waiting for worker tasks to exit\n",
      "2025-06-01 19:15:25,584\tINFO compiled_dag_node.py:2762 -- Killing actor: Actor(TorchTensorWorker, 8fdf4a6f922fbbdcd30dcfd601000000)\n",
      "2025-06-01 19:15:25,584\tINFO compiled_dag_node.py:2762 -- Killing actor: Actor(TorchTensorWorker, 9865cb7cd448de00aab7dc2801000000)\n",
      "2025-06-01 19:15:25,585\tINFO compiled_dag_node.py:2804 -- Teardown complete\n",
      "\n",
      "test_torch_tensor_dag.py::test_torch_tensor_nccl_collective_ops[operation2-ReduceOp.PRODUCT-ray_start_regular0] 2025-06-01 19:15:28,208\tINFO worker.py:1917 -- Started a local Ray instance.\n",
      "2025-06-01 19:15:30,471\tINFO torch_tensor_nccl_channel.py:772 -- Creating NCCL group 6756d1ad-4b2b-4153-9fe0-485f5675db78 on actors: [Actor(TorchTensorWorker, 68d9598c1c0e7bfca39d1b9301000000), Actor(TorchTensorWorker, 0470f19956c52669d36eb4d401000000)]\n",
      "2025-06-01 19:15:31,320\tINFO torch_tensor_nccl_channel.py:797 -- NCCL group initialized.\n",
      "2025-06-01 19:15:31,501\tINFO dag_node_operation.py:1108 -- Writing compiled graph schedule visualization to compiled_graph_schedule.png\n",
      "\u001b[32mPASSED\u001b[0m2025-06-01 19:15:31,689\tINFO compiled_dag_node.py:2774 -- Tearing down compiled DAG\n",
      "2025-06-01 19:15:31,690\tINFO compiled_dag_node.py:2779 -- Cancelling compiled worker on actor: Actor(TorchTensorWorker, 68d9598c1c0e7bfca39d1b9301000000)\n",
      "2025-06-01 19:15:31,690\tINFO compiled_dag_node.py:2779 -- Cancelling compiled worker on actor: Actor(TorchTensorWorker, 0470f19956c52669d36eb4d401000000)\n",
      "\u001b[36m(TorchTensorWorker pid=40610)\u001b[0m Destructing NCCL group on actor: Actor(TorchTensorWorker, 0470f19956c52669d36eb4d401000000)\n",
      "\u001b[36m(TorchTensorWorker pid=40611)\u001b[0m Destructing NCCL group on actor: Actor(TorchTensorWorker, 68d9598c1c0e7bfca39d1b9301000000)\n",
      "2025-06-01 19:15:32,180\tINFO compiled_dag_node.py:2801 -- Waiting for worker tasks to exit\n",
      "2025-06-01 19:15:32,181\tINFO compiled_dag_node.py:2762 -- Killing actor: Actor(TorchTensorWorker, 68d9598c1c0e7bfca39d1b9301000000)\n",
      "2025-06-01 19:15:32,181\tINFO compiled_dag_node.py:2762 -- Killing actor: Actor(TorchTensorWorker, 0470f19956c52669d36eb4d401000000)\n",
      "2025-06-01 19:15:32,181\tINFO compiled_dag_node.py:2804 -- Teardown complete\n",
      "\n",
      "test_torch_tensor_dag.py::test_torch_tensor_nccl_collective_ops[operation3-ReduceOp.MIN-ray_start_regular0] 2025-06-01 19:15:34,994\tINFO worker.py:1917 -- Started a local Ray instance.\n",
      "2025-06-01 19:15:37,226\tINFO torch_tensor_nccl_channel.py:772 -- Creating NCCL group 2a741e35-ee34-44e6-bf0b-b39a663f2a86 on actors: [Actor(TorchTensorWorker, 84a0c3fde9d6eea6e486456001000000), Actor(TorchTensorWorker, 840123caea3d9a3244966d6e01000000)]\n",
      "2025-06-01 19:15:38,078\tINFO torch_tensor_nccl_channel.py:797 -- NCCL group initialized.\n",
      "2025-06-01 19:15:38,249\tINFO dag_node_operation.py:1108 -- Writing compiled graph schedule visualization to compiled_graph_schedule.png\n",
      "\u001b[32mPASSED\u001b[0m2025-06-01 19:15:38,472\tINFO compiled_dag_node.py:2774 -- Tearing down compiled DAG\n",
      "2025-06-01 19:15:38,472\tINFO compiled_dag_node.py:2779 -- Cancelling compiled worker on actor: Actor(TorchTensorWorker, 84a0c3fde9d6eea6e486456001000000)\n",
      "2025-06-01 19:15:38,472\tINFO compiled_dag_node.py:2779 -- Cancelling compiled worker on actor: Actor(TorchTensorWorker, 840123caea3d9a3244966d6e01000000)\n",
      "\u001b[36m(TorchTensorWorker pid=41329)\u001b[0m Destructing NCCL group on actor: Actor(TorchTensorWorker, 84a0c3fde9d6eea6e486456001000000)\n",
      "\u001b[36m(TorchTensorWorker pid=41327)\u001b[0m Destructing NCCL group on actor: Actor(TorchTensorWorker, 840123caea3d9a3244966d6e01000000)\n",
      "2025-06-01 19:15:38,960\tINFO compiled_dag_node.py:2801 -- Waiting for worker tasks to exit\n",
      "2025-06-01 19:15:38,961\tINFO compiled_dag_node.py:2762 -- Killing actor: Actor(TorchTensorWorker, 84a0c3fde9d6eea6e486456001000000)\n",
      "2025-06-01 19:15:38,961\tINFO compiled_dag_node.py:2762 -- Killing actor: Actor(TorchTensorWorker, 840123caea3d9a3244966d6e01000000)\n",
      "2025-06-01 19:15:38,961\tINFO compiled_dag_node.py:2804 -- Teardown complete\n",
      "\n",
      "test_torch_tensor_dag.py::test_torch_tensor_nccl_collective_ops[operation4-ReduceOp.MAX-ray_start_regular0] 2025-06-01 19:15:42,891\tINFO worker.py:1917 -- Started a local Ray instance.\n",
      "2025-06-01 19:15:45,194\tINFO torch_tensor_nccl_channel.py:772 -- Creating NCCL group 2b9b1d89-a431-461b-a0e8-5199957f0ba1 on actors: [Actor(TorchTensorWorker, 7c3a78d98a096fc3a42f7cdc01000000), Actor(TorchTensorWorker, 859a64a397d1949db3f6822a01000000)]\n",
      "2025-06-01 19:15:46,053\tINFO torch_tensor_nccl_channel.py:797 -- NCCL group initialized.\n",
      "2025-06-01 19:15:46,221\tINFO dag_node_operation.py:1108 -- Writing compiled graph schedule visualization to compiled_graph_schedule.png\n",
      "\u001b[32mPASSED\u001b[0m2025-06-01 19:15:46,414\tINFO compiled_dag_node.py:2774 -- Tearing down compiled DAG\n",
      "2025-06-01 19:15:46,415\tINFO compiled_dag_node.py:2779 -- Cancelling compiled worker on actor: Actor(TorchTensorWorker, 7c3a78d98a096fc3a42f7cdc01000000)\n",
      "2025-06-01 19:15:46,415\tINFO compiled_dag_node.py:2779 -- Cancelling compiled worker on actor: Actor(TorchTensorWorker, 859a64a397d1949db3f6822a01000000)\n",
      "\u001b[36m(TorchTensorWorker pid=42116)\u001b[0m Destructing NCCL group on actor: Actor(TorchTensorWorker, 7c3a78d98a096fc3a42f7cdc01000000)\n",
      "\u001b[36m(TorchTensorWorker pid=42115)\u001b[0m Destructing NCCL group on actor: Actor(TorchTensorWorker, 859a64a397d1949db3f6822a01000000)\n",
      "2025-06-01 19:15:46,902\tINFO compiled_dag_node.py:2801 -- Waiting for worker tasks to exit\n",
      "2025-06-01 19:15:46,902\tINFO compiled_dag_node.py:2762 -- Killing actor: Actor(TorchTensorWorker, 7c3a78d98a096fc3a42f7cdc01000000)\n",
      "2025-06-01 19:15:46,903\tINFO compiled_dag_node.py:2762 -- Killing actor: Actor(TorchTensorWorker, 859a64a397d1949db3f6822a01000000)\n",
      "2025-06-01 19:15:46,903\tINFO compiled_dag_node.py:2804 -- Teardown complete\n",
      "\n",
      "test_torch_tensor_dag.py::test_torch_tensor_nccl_collective_ops[operation5-ReduceOp.SUM-ray_start_regular0] 2025-06-01 19:15:49,891\tINFO worker.py:1917 -- Started a local Ray instance.\n",
      "2025-06-01 19:15:52,127\tINFO torch_tensor_nccl_channel.py:772 -- Creating NCCL group c8200e05-bd9c-4fb6-a9c4-6813651a88bf on actors: [Actor(TorchTensorWorker, 828dcd79d58bb6f285b37cb101000000), Actor(TorchTensorWorker, b2fb6951294310f0622f081d01000000)]\n",
      "2025-06-01 19:15:52,987\tINFO torch_tensor_nccl_channel.py:797 -- NCCL group initialized.\n",
      "2025-06-01 19:15:53,149\tINFO dag_node_operation.py:1108 -- Writing compiled graph schedule visualization to compiled_graph_schedule.png\n",
      "\u001b[32mPASSED\u001b[0m2025-06-01 19:15:53,346\tINFO compiled_dag_node.py:2774 -- Tearing down compiled DAG\n",
      "2025-06-01 19:15:53,346\tINFO compiled_dag_node.py:2779 -- Cancelling compiled worker on actor: Actor(TorchTensorWorker, 828dcd79d58bb6f285b37cb101000000)\n",
      "2025-06-01 19:15:53,346\tINFO compiled_dag_node.py:2779 -- Cancelling compiled worker on actor: Actor(TorchTensorWorker, b2fb6951294310f0622f081d01000000)\n",
      "\u001b[36m(TorchTensorWorker pid=42831)\u001b[0m Destructing NCCL group on actor: Actor(TorchTensorWorker, b2fb6951294310f0622f081d01000000)\n",
      "\u001b[36m(TorchTensorWorker pid=42830)\u001b[0m Destructing NCCL group on actor: Actor(TorchTensorWorker, 828dcd79d58bb6f285b37cb101000000)\n",
      "2025-06-01 19:15:53,837\tINFO compiled_dag_node.py:2801 -- Waiting for worker tasks to exit\n",
      "2025-06-01 19:15:53,837\tINFO compiled_dag_node.py:2762 -- Killing actor: Actor(TorchTensorWorker, 828dcd79d58bb6f285b37cb101000000)\n",
      "2025-06-01 19:15:53,837\tINFO compiled_dag_node.py:2762 -- Killing actor: Actor(TorchTensorWorker, b2fb6951294310f0622f081d01000000)\n",
      "2025-06-01 19:15:53,838\tINFO compiled_dag_node.py:2804 -- Teardown complete\n",
      "\n",
      "test_torch_tensor_dag.py::test_torch_tensor_nccl_collective_ops[operation6-ReduceOp.PRODUCT-ray_start_regular0] 2025-06-01 19:15:56,536\tINFO worker.py:1917 -- Started a local Ray instance.\n",
      "2025-06-01 19:15:58,837\tINFO torch_tensor_nccl_channel.py:772 -- Creating NCCL group 829494f8-7cca-4350-9025-183c1eceda09 on actors: [Actor(TorchTensorWorker, 4fae3592bbedfff68853b9b801000000), Actor(TorchTensorWorker, 3abc71e2fb91738c448d975301000000)]\n",
      "2025-06-01 19:15:59,690\tINFO torch_tensor_nccl_channel.py:797 -- NCCL group initialized.\n",
      "2025-06-01 19:15:59,826\tINFO dag_node_operation.py:1108 -- Writing compiled graph schedule visualization to compiled_graph_schedule.png\n",
      "\u001b[32mPASSED\u001b[0m2025-06-01 19:16:00,021\tINFO compiled_dag_node.py:2774 -- Tearing down compiled DAG\n",
      "2025-06-01 19:16:00,022\tINFO compiled_dag_node.py:2779 -- Cancelling compiled worker on actor: Actor(TorchTensorWorker, 4fae3592bbedfff68853b9b801000000)\n",
      "2025-06-01 19:16:00,022\tINFO compiled_dag_node.py:2779 -- Cancelling compiled worker on actor: Actor(TorchTensorWorker, 3abc71e2fb91738c448d975301000000)\n",
      "\u001b[36m(TorchTensorWorker pid=43525)\u001b[0m Destructing NCCL group on actor: Actor(TorchTensorWorker, 3abc71e2fb91738c448d975301000000)\n",
      "\u001b[36m(TorchTensorWorker pid=43526)\u001b[0m Destructing NCCL group on actor: Actor(TorchTensorWorker, 4fae3592bbedfff68853b9b801000000)\n",
      "2025-06-01 19:16:00,511\tINFO compiled_dag_node.py:2801 -- Waiting for worker tasks to exit\n",
      "2025-06-01 19:16:00,512\tINFO compiled_dag_node.py:2762 -- Killing actor: Actor(TorchTensorWorker, 4fae3592bbedfff68853b9b801000000)\n",
      "2025-06-01 19:16:00,512\tINFO compiled_dag_node.py:2762 -- Killing actor: Actor(TorchTensorWorker, 3abc71e2fb91738c448d975301000000)\n",
      "2025-06-01 19:16:00,512\tINFO compiled_dag_node.py:2804 -- Teardown complete\n",
      "\n",
      "test_torch_tensor_dag.py::test_torch_tensor_nccl_collective_ops[operation7-ReduceOp.MIN-ray_start_regular0] 2025-06-01 19:16:04,385\tINFO worker.py:1917 -- Started a local Ray instance.\n",
      "2025-06-01 19:16:06,596\tINFO torch_tensor_nccl_channel.py:772 -- Creating NCCL group eb49088c-a470-40a5-980b-fbeeaf6e1c70 on actors: [Actor(TorchTensorWorker, a840121cfd668acc53ab7dec01000000), Actor(TorchTensorWorker, ab05e3d87319e8b35b8f018c01000000)]\n",
      "2025-06-01 19:16:07,460\tINFO torch_tensor_nccl_channel.py:797 -- NCCL group initialized.\n",
      "2025-06-01 19:16:07,629\tINFO dag_node_operation.py:1108 -- Writing compiled graph schedule visualization to compiled_graph_schedule.png\n",
      "\u001b[32mPASSED\u001b[0m2025-06-01 19:16:07,821\tINFO compiled_dag_node.py:2774 -- Tearing down compiled DAG\n",
      "2025-06-01 19:16:07,821\tINFO compiled_dag_node.py:2779 -- Cancelling compiled worker on actor: Actor(TorchTensorWorker, a840121cfd668acc53ab7dec01000000)\n",
      "2025-06-01 19:16:07,821\tINFO compiled_dag_node.py:2779 -- Cancelling compiled worker on actor: Actor(TorchTensorWorker, ab05e3d87319e8b35b8f018c01000000)\n",
      "\u001b[36m(TorchTensorWorker pid=44273)\u001b[0m Destructing NCCL group on actor: Actor(TorchTensorWorker, a840121cfd668acc53ab7dec01000000)\n",
      "\u001b[36m(TorchTensorWorker pid=44274)\u001b[0m Destructing NCCL group on actor: Actor(TorchTensorWorker, ab05e3d87319e8b35b8f018c01000000)\n",
      "2025-06-01 19:16:08,311\tINFO compiled_dag_node.py:2801 -- Waiting for worker tasks to exit\n",
      "2025-06-01 19:16:08,311\tINFO compiled_dag_node.py:2762 -- Killing actor: Actor(TorchTensorWorker, a840121cfd668acc53ab7dec01000000)\n",
      "2025-06-01 19:16:08,312\tINFO compiled_dag_node.py:2762 -- Killing actor: Actor(TorchTensorWorker, ab05e3d87319e8b35b8f018c01000000)\n",
      "2025-06-01 19:16:08,312\tINFO compiled_dag_node.py:2804 -- Teardown complete\n",
      "\n",
      "test_torch_tensor_dag.py::test_torch_tensor_nccl_collective_ops[operation8-ReduceOp.MAX-ray_start_regular0] 2025-06-01 19:16:12,220\tINFO worker.py:1917 -- Started a local Ray instance.\n",
      "2025-06-01 19:16:14,475\tINFO torch_tensor_nccl_channel.py:772 -- Creating NCCL group 24d2a5fe-0683-4715-ab34-b08faec6687a on actors: [Actor(TorchTensorWorker, 669c1d9d37065f9f8205349201000000), Actor(TorchTensorWorker, 8d995702732bee9cb9712f6a01000000)]\n",
      "2025-06-01 19:16:15,332\tINFO torch_tensor_nccl_channel.py:797 -- NCCL group initialized.\n",
      "2025-06-01 19:16:15,510\tINFO dag_node_operation.py:1108 -- Writing compiled graph schedule visualization to compiled_graph_schedule.png\n",
      "\u001b[32mPASSED\u001b[0m2025-06-01 19:16:15,730\tINFO compiled_dag_node.py:2774 -- Tearing down compiled DAG\n",
      "2025-06-01 19:16:15,730\tINFO compiled_dag_node.py:2779 -- Cancelling compiled worker on actor: Actor(TorchTensorWorker, 669c1d9d37065f9f8205349201000000)\n",
      "2025-06-01 19:16:15,730\tINFO compiled_dag_node.py:2779 -- Cancelling compiled worker on actor: Actor(TorchTensorWorker, 8d995702732bee9cb9712f6a01000000)\n",
      "\u001b[36m(TorchTensorWorker pid=45020)\u001b[0m Destructing NCCL group on actor: Actor(TorchTensorWorker, 8d995702732bee9cb9712f6a01000000)\n",
      "\u001b[36m(TorchTensorWorker pid=45021)\u001b[0m Destructing NCCL group on actor: Actor(TorchTensorWorker, 669c1d9d37065f9f8205349201000000)\n",
      "2025-06-01 19:16:16,222\tINFO compiled_dag_node.py:2801 -- Waiting for worker tasks to exit\n",
      "2025-06-01 19:16:16,222\tINFO compiled_dag_node.py:2762 -- Killing actor: Actor(TorchTensorWorker, 669c1d9d37065f9f8205349201000000)\n",
      "2025-06-01 19:16:16,223\tINFO compiled_dag_node.py:2762 -- Killing actor: Actor(TorchTensorWorker, 8d995702732bee9cb9712f6a01000000)\n",
      "2025-06-01 19:16:16,223\tINFO compiled_dag_node.py:2804 -- Teardown complete\n",
      "\n",
      "test_torch_tensor_dag.py::test_torch_tensor_nccl_all_reduce_get_partial[ray_start_regular0] 2025-06-01 19:16:19,056\tINFO worker.py:1917 -- Started a local Ray instance.\n",
      "2025-06-01 19:16:21,269\tINFO torch_tensor_nccl_channel.py:772 -- Creating NCCL group 5d3a03ed-27f7-4c4f-a3c4-883e625c02b0 on actors: [Actor(TorchTensorWorker, 51dde4257b5d049c1d1a121e01000000), Actor(TorchTensorWorker, 67e3143114be347a514c672501000000)]\n",
      "2025-06-01 19:16:22,133\tINFO torch_tensor_nccl_channel.py:797 -- NCCL group initialized.\n",
      "2025-06-01 19:16:22,291\tINFO dag_node_operation.py:1108 -- Writing compiled graph schedule visualization to compiled_graph_schedule.png\n",
      "\u001b[32mPASSED\u001b[0m2025-06-01 19:16:22,506\tINFO compiled_dag_node.py:2774 -- Tearing down compiled DAG\n",
      "2025-06-01 19:16:22,507\tINFO compiled_dag_node.py:2779 -- Cancelling compiled worker on actor: Actor(TorchTensorWorker, 51dde4257b5d049c1d1a121e01000000)\n",
      "2025-06-01 19:16:22,507\tINFO compiled_dag_node.py:2779 -- Cancelling compiled worker on actor: Actor(TorchTensorWorker, 67e3143114be347a514c672501000000)\n",
      "\u001b[36m(TorchTensorWorker pid=45686)\u001b[0m Destructing NCCL group on actor: Actor(TorchTensorWorker, 67e3143114be347a514c672501000000)\n",
      "\u001b[36m(TorchTensorWorker pid=45685)\u001b[0m Destructing NCCL group on actor: Actor(TorchTensorWorker, 51dde4257b5d049c1d1a121e01000000)\n",
      "2025-06-01 19:16:22,991\tINFO compiled_dag_node.py:2801 -- Waiting for worker tasks to exit\n",
      "2025-06-01 19:16:22,992\tINFO compiled_dag_node.py:2762 -- Killing actor: Actor(TorchTensorWorker, 51dde4257b5d049c1d1a121e01000000)\n",
      "2025-06-01 19:16:22,992\tINFO compiled_dag_node.py:2762 -- Killing actor: Actor(TorchTensorWorker, 67e3143114be347a514c672501000000)\n",
      "2025-06-01 19:16:22,992\tINFO compiled_dag_node.py:2804 -- Teardown complete\n",
      "\n",
      "test_torch_tensor_dag.py::test_torch_tensor_nccl_all_reduce_wrong_shape[ray_start_regular0] 2025-06-01 19:16:26,149\tINFO worker.py:1917 -- Started a local Ray instance.\n",
      "2025-06-01 19:16:28,373\tINFO torch_tensor_nccl_channel.py:772 -- Creating NCCL group 22a64352-1b31-4b7e-be14-d997bb98a303 on actors: [Actor(TorchTensorWorker, c2dd6c415b97d0dc52c4a9ba01000000), Actor(TorchTensorWorker, 1ab80c8016467d9aa42fb68101000000)]\n",
      "2025-06-01 19:16:29,238\tINFO torch_tensor_nccl_channel.py:797 -- NCCL group initialized.\n",
      "2025-06-01 19:16:29,358\tINFO dag_node_operation.py:1108 -- Writing compiled graph schedule visualization to compiled_graph_schedule.png\n",
      "\u001b[32mPASSED\u001b[0m2025-06-01 19:16:49,558\tINFO compiled_dag_node.py:2774 -- Tearing down compiled DAG\n",
      "2025-06-01 19:16:49,558\tINFO compiled_dag_node.py:2779 -- Cancelling compiled worker on actor: Actor(TorchTensorWorker, c2dd6c415b97d0dc52c4a9ba01000000)\n",
      "2025-06-01 19:16:49,558\tINFO compiled_dag_node.py:2779 -- Cancelling compiled worker on actor: Actor(TorchTensorWorker, 1ab80c8016467d9aa42fb68101000000)\n",
      "\u001b[36m(TorchTensorWorker pid=46430)\u001b[0m Destructing NCCL group on actor: Actor(TorchTensorWorker, c2dd6c415b97d0dc52c4a9ba01000000)\n",
      "\u001b[36m(TorchTensorWorker pid=46431)\u001b[0m Destructing NCCL group on actor: Actor(TorchTensorWorker, 1ab80c8016467d9aa42fb68101000000)\n",
      "2025-06-01 19:16:49,655\tINFO compiled_dag_node.py:2801 -- Waiting for worker tasks to exit\n",
      "2025-06-01 19:16:49,655\tINFO compiled_dag_node.py:2762 -- Killing actor: Actor(TorchTensorWorker, c2dd6c415b97d0dc52c4a9ba01000000)\n",
      "2025-06-01 19:16:49,655\tINFO compiled_dag_node.py:2762 -- Killing actor: Actor(TorchTensorWorker, 1ab80c8016467d9aa42fb68101000000)\n",
      "2025-06-01 19:16:49,656\tINFO compiled_dag_node.py:2804 -- Teardown complete\n",
      "\n",
      "test_torch_tensor_dag.py::test_torch_tensor_nccl_all_reduce_custom_comm[ray_start_regular0] 2025-06-01 19:16:52,497\tINFO worker.py:1917 -- Started a local Ray instance.\n",
      "2025-06-01 19:16:54,781\tINFO torch_tensor_nccl_channel.py:770 -- Initializing custom NCCL group 6b24f6fe-fd59-4669-ab37-7f8458df7b13 on actors: [Actor(TorchTensorWorker, 420de0b5878ce9589b2f042b01000000), Actor(TorchTensorWorker, b4fcdaa167cee888d0d0e2fb01000000)]\n",
      "2025-06-01 19:16:55,535\tINFO torch_tensor_nccl_channel.py:797 -- NCCL group initialized.\n",
      "2025-06-01 19:16:55,667\tINFO dag_node_operation.py:1108 -- Writing compiled graph schedule visualization to compiled_graph_schedule.png\n",
      "\u001b[32mPASSED\u001b[0m2025-06-01 19:16:55,932\tINFO compiled_dag_node.py:2774 -- Tearing down compiled DAG\n",
      "2025-06-01 19:16:55,932\tINFO compiled_dag_node.py:2779 -- Cancelling compiled worker on actor: Actor(TorchTensorWorker, 420de0b5878ce9589b2f042b01000000)\n",
      "2025-06-01 19:16:55,932\tINFO compiled_dag_node.py:2779 -- Cancelling compiled worker on actor: Actor(TorchTensorWorker, b4fcdaa167cee888d0d0e2fb01000000)\n",
      "2025-06-01 19:16:55,943\tINFO compiled_dag_node.py:2801 -- Waiting for worker tasks to exit\n",
      "2025-06-01 19:16:55,944\tINFO compiled_dag_node.py:2762 -- Killing actor: Actor(TorchTensorWorker, 420de0b5878ce9589b2f042b01000000)\n",
      "2025-06-01 19:16:55,944\tINFO compiled_dag_node.py:2762 -- Killing actor: Actor(TorchTensorWorker, b4fcdaa167cee888d0d0e2fb01000000)\n",
      "2025-06-01 19:16:55,944\tINFO compiled_dag_node.py:2804 -- Teardown complete\n",
      "\n",
      "test_torch_tensor_dag.py::test_torch_tensor_nccl_all_reduce_scheduling[ray_start_regular0] 2025-06-01 19:16:58,604\tINFO worker.py:1917 -- Started a local Ray instance.\n",
      "2025-06-01 19:17:00,829\tINFO torch_tensor_nccl_channel.py:772 -- Creating NCCL group a767ee9c-e287-4302-988d-2c24c914aa92 on actors: [Actor(TorchTensorWorker, 2bab0ed96dbe3c1abadde12201000000), Actor(TorchTensorWorker, 52a459748365726a3d30558301000000)]\n",
      "2025-06-01 19:17:01,728\tINFO torch_tensor_nccl_channel.py:797 -- NCCL group initialized.\n",
      "2025-06-01 19:17:01,905\tINFO dag_node_operation.py:1108 -- Writing compiled graph schedule visualization to compiled_graph_schedule.png\n",
      "\u001b[32mPASSED\u001b[0m2025-06-01 19:17:02,143\tINFO compiled_dag_node.py:2774 -- Tearing down compiled DAG\n",
      "2025-06-01 19:17:02,144\tINFO compiled_dag_node.py:2779 -- Cancelling compiled worker on actor: Actor(TorchTensorWorker, 2bab0ed96dbe3c1abadde12201000000)\n",
      "2025-06-01 19:17:02,144\tINFO compiled_dag_node.py:2779 -- Cancelling compiled worker on actor: Actor(TorchTensorWorker, 52a459748365726a3d30558301000000)\n",
      "\u001b[36m(TorchTensorWorker pid=47963)\u001b[0m Destructing NCCL group on actor: Actor(TorchTensorWorker, 52a459748365726a3d30558301000000)\n",
      "\u001b[36m(TorchTensorWorker pid=47962)\u001b[0m Destructing NCCL group on actor: Actor(TorchTensorWorker, 2bab0ed96dbe3c1abadde12201000000)\n",
      "2025-06-01 19:17:02,667\tINFO compiled_dag_node.py:2801 -- Waiting for worker tasks to exit\n",
      "2025-06-01 19:17:02,667\tINFO compiled_dag_node.py:2762 -- Killing actor: Actor(TorchTensorWorker, 2bab0ed96dbe3c1abadde12201000000)\n",
      "2025-06-01 19:17:02,667\tINFO compiled_dag_node.py:2762 -- Killing actor: Actor(TorchTensorWorker, 52a459748365726a3d30558301000000)\n",
      "2025-06-01 19:17:02,668\tINFO compiled_dag_node.py:2804 -- Teardown complete\n",
      "\n",
      "test_torch_tensor_dag.py::test_nccl_all_reduce_with_class_method_output_node[ray_start_regular0] 2025-06-01 19:17:05,931\tINFO worker.py:1917 -- Started a local Ray instance.\n",
      "2025-06-01 19:17:08,246\tINFO torch_tensor_nccl_channel.py:772 -- Creating NCCL group 7febac5f-5d5b-43c2-81d5-181d0b9fb421 on actors: [Actor(TorchTensorWorker, 9f3cf118ffaffc5e9bda1dc301000000), Actor(TorchTensorWorker, 822b341ee1574dea0265efa201000000)]\n",
      "2025-06-01 19:17:09,100\tINFO torch_tensor_nccl_channel.py:797 -- NCCL group initialized.\n",
      "2025-06-01 19:17:09,374\tINFO dag_node_operation.py:1108 -- Writing compiled graph schedule visualization to compiled_graph_schedule.png\n",
      "\u001b[32mPASSED\u001b[0m2025-06-01 19:17:09,639\tINFO compiled_dag_node.py:2774 -- Tearing down compiled DAG\n",
      "2025-06-01 19:17:09,640\tINFO compiled_dag_node.py:2779 -- Cancelling compiled worker on actor: Actor(TorchTensorWorker, 9f3cf118ffaffc5e9bda1dc301000000)\n",
      "2025-06-01 19:17:09,640\tINFO compiled_dag_node.py:2779 -- Cancelling compiled worker on actor: Actor(TorchTensorWorker, 822b341ee1574dea0265efa201000000)\n",
      "\u001b[36m(TorchTensorWorker pid=48686)\u001b[0m Destructing NCCL group on actor: Actor(TorchTensorWorker, 822b341ee1574dea0265efa201000000)\n",
      "\u001b[36m(TorchTensorWorker pid=48688)\u001b[0m Destructing NCCL group on actor: Actor(TorchTensorWorker, 9f3cf118ffaffc5e9bda1dc301000000)\n",
      "2025-06-01 19:17:10,075\tINFO compiled_dag_node.py:2801 -- Waiting for worker tasks to exit\n",
      "2025-06-01 19:17:10,075\tINFO compiled_dag_node.py:2762 -- Killing actor: Actor(TorchTensorWorker, 9f3cf118ffaffc5e9bda1dc301000000)\n",
      "2025-06-01 19:17:10,076\tINFO compiled_dag_node.py:2762 -- Killing actor: Actor(TorchTensorWorker, 822b341ee1574dea0265efa201000000)\n",
      "2025-06-01 19:17:10,076\tINFO compiled_dag_node.py:2804 -- Teardown complete\n",
      "\n",
      "test_torch_tensor_dag.py::test_tensor_writable_warning_suppressed[ray_start_regular0] 2025-06-01 19:17:12,966\tINFO worker.py:1917 -- Started a local Ray instance.\n",
      "2025-06-01 19:17:13,512\tINFO dag_node_operation.py:1108 -- Writing compiled graph schedule visualization to compiled_graph_schedule.png\n",
      "2025-06-01 19:17:17,456\tINFO compiled_dag_node.py:2774 -- Tearing down compiled DAG\n",
      "2025-06-01 19:17:17,456\tINFO compiled_dag_node.py:2779 -- Cancelling compiled worker on actor: Actor(test_tensor_writable_warning_suppressed.<locals>.A, d978087f0b1da1394e138d2f01000000)\n",
      "2025-06-01 19:17:17,461\tINFO compiled_dag_node.py:2801 -- Waiting for worker tasks to exit\n",
      "2025-06-01 19:17:17,461\tINFO compiled_dag_node.py:2804 -- Teardown complete\n",
      "\u001b[32mPASSED\u001b[0m\n",
      "test_torch_tensor_dag.py::test_torch_nccl_channel_with_local_reader[ray_start_regular0] 2025-06-01 19:17:20,212\tINFO worker.py:1917 -- Started a local Ray instance.\n",
      "2025-06-01 19:17:22,425\tINFO torch_tensor_nccl_channel.py:772 -- Creating NCCL group 33205801-2d29-462b-b699-15e08bdf4e7e on actors: [Actor(TorchTensorWorker, cabaca6e989564e36592748501000000), Actor(TorchTensorWorker, 067ec50415301acf9fa995cf01000000)]\n",
      "2025-06-01 19:17:23,282\tINFO torch_tensor_nccl_channel.py:797 -- NCCL group initialized.\n",
      "2025-06-01 19:17:23,431\tINFO dag_node_operation.py:1108 -- Writing compiled graph schedule visualization to compiled_graph_schedule.png\n",
      "\u001b[32mPASSED\u001b[0m2025-06-01 19:17:23,656\tINFO compiled_dag_node.py:2774 -- Tearing down compiled DAG\n",
      "2025-06-01 19:17:23,657\tINFO compiled_dag_node.py:2779 -- Cancelling compiled worker on actor: Actor(TorchTensorWorker, 067ec50415301acf9fa995cf01000000)\n",
      "2025-06-01 19:17:23,657\tINFO compiled_dag_node.py:2779 -- Cancelling compiled worker on actor: Actor(TorchTensorWorker, cabaca6e989564e36592748501000000)\n",
      "\u001b[36m(TorchTensorWorker pid=49982)\u001b[0m Destructing NCCL group on actor: Actor(TorchTensorWorker, cabaca6e989564e36592748501000000)\n",
      "\u001b[36m(TorchTensorWorker pid=49981)\u001b[0m Destructing NCCL group on actor: Actor(TorchTensorWorker, 067ec50415301acf9fa995cf01000000)\n",
      "2025-06-01 19:17:24,150\tINFO compiled_dag_node.py:2801 -- Waiting for worker tasks to exit\n",
      "2025-06-01 19:17:24,150\tINFO compiled_dag_node.py:2762 -- Killing actor: Actor(TorchTensorWorker, 067ec50415301acf9fa995cf01000000)\n",
      "2025-06-01 19:17:24,150\tINFO compiled_dag_node.py:2762 -- Killing actor: Actor(TorchTensorWorker, cabaca6e989564e36592748501000000)\n",
      "2025-06-01 19:17:24,151\tINFO compiled_dag_node.py:2804 -- Teardown complete\n",
      "\n",
      "test_torch_tensor_dag.py::test_torch_nccl_channel_with_two_local_readers[ray_start_regular0] 2025-06-01 19:17:28,272\tINFO worker.py:1917 -- Started a local Ray instance.\n",
      "2025-06-01 19:17:30,458\tINFO torch_tensor_nccl_channel.py:772 -- Creating NCCL group 42369f72-155e-4b8f-89a1-417991e20b77 on actors: [Actor(TorchTensorWorker, 38209c44e65024891c54047801000000), Actor(TorchTensorWorker, f1143fb5ea475f2ba194fa2f01000000)]\n",
      "2025-06-01 19:17:31,369\tINFO torch_tensor_nccl_channel.py:797 -- NCCL group initialized.\n",
      "2025-06-01 19:17:31,602\tINFO dag_node_operation.py:1108 -- Writing compiled graph schedule visualization to compiled_graph_schedule.png\n",
      "\u001b[32mPASSED\u001b[0m2025-06-01 19:17:31,805\tINFO compiled_dag_node.py:2774 -- Tearing down compiled DAG\n",
      "2025-06-01 19:17:31,805\tINFO compiled_dag_node.py:2779 -- Cancelling compiled worker on actor: Actor(TorchTensorWorker, f1143fb5ea475f2ba194fa2f01000000)\n",
      "2025-06-01 19:17:31,805\tINFO compiled_dag_node.py:2779 -- Cancelling compiled worker on actor: Actor(TorchTensorWorker, 38209c44e65024891c54047801000000)\n",
      "\u001b[36m(TorchTensorWorker pid=50759)\u001b[0m Destructing NCCL group on actor: Actor(TorchTensorWorker, f1143fb5ea475f2ba194fa2f01000000)\n",
      "\u001b[36m(TorchTensorWorker pid=50761)\u001b[0m Destructing NCCL group on actor: Actor(TorchTensorWorker, 38209c44e65024891c54047801000000)\n",
      "2025-06-01 19:17:32,296\tINFO compiled_dag_node.py:2801 -- Waiting for worker tasks to exit\n",
      "2025-06-01 19:17:32,296\tINFO compiled_dag_node.py:2762 -- Killing actor: Actor(TorchTensorWorker, f1143fb5ea475f2ba194fa2f01000000)\n",
      "2025-06-01 19:17:32,296\tINFO compiled_dag_node.py:2762 -- Killing actor: Actor(TorchTensorWorker, 38209c44e65024891c54047801000000)\n",
      "2025-06-01 19:17:32,296\tINFO compiled_dag_node.py:2804 -- Teardown complete\n",
      "\n",
      "test_torch_tensor_dag.py::test_torch_nccl_channel_with_all_local_readers[ray_start_regular0] 2025-06-01 19:17:35,336\tINFO worker.py:1917 -- Started a local Ray instance.\n",
      "2025-06-01 19:17:37,438\tINFO torch_tensor_nccl_channel.py:772 -- Creating NCCL group 25b40084-e07f-4d60-b523-a435c0c81fde on actors: [Actor(TorchTensorWorker, 1e4f8c9702bee76c165ec93801000000)]\n",
      "2025-06-01 19:17:37,926\tINFO torch_tensor_nccl_channel.py:797 -- NCCL group initialized.\n",
      "\u001b[32mPASSED\u001b[0m\n",
      "\n",
      "=================================== FAILURES ===================================\n",
      "\u001b[31m\u001b[1m________ test_torch_tensor_nccl_overlap_timed[ray_start_regular0-False] ________\u001b[0m\n",
      "\n",
      "ray_start_regular = RayContext(dashboard_url='', python_version='3.13.3', ray_version='3.0.0.dev0', ray_commit='{{RAY_COMMIT_SHA}}')\n",
      "overlap_gpu_communication = False\n",
      "\n",
      "    \u001b[0m\u001b[37m@pytest\u001b[39;49;00m.mark.skipif(\u001b[95mnot\u001b[39;49;00m USE_GPU, reason=\u001b[33m\"\u001b[39;49;00m\u001b[33mSkipping GPU Test\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[33m\"\u001b[39;49;00m\u001b[33mray_start_regular, overlap_gpu_communication\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m,\u001b[90m\u001b[39;49;00m\n",
      "        [({\u001b[33m\"\u001b[39;49;00m\u001b[33mnum_cpus\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m: \u001b[94m4\u001b[39;49;00m}, \u001b[94mFalse\u001b[39;49;00m), ({\u001b[33m\"\u001b[39;49;00m\u001b[33mnum_cpus\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m: \u001b[94m4\u001b[39;49;00m}, \u001b[94mTrue\u001b[39;49;00m)],\u001b[90m\u001b[39;49;00m\n",
      "        indirect=[\u001b[33m\"\u001b[39;49;00m\u001b[33mray_start_regular\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m],\u001b[90m\u001b[39;49;00m\n",
      "    )\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mtest_torch_tensor_nccl_overlap_timed\u001b[39;49;00m(ray_start_regular, overlap_gpu_communication):\u001b[90m\u001b[39;49;00m\n",
      ">       \u001b[94massert\u001b[39;49;00m (\u001b[90m\u001b[39;49;00m\n",
      "            \u001b[96msum\u001b[39;49;00m(node[\u001b[33m\"\u001b[39;49;00m\u001b[33mResources\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m].get(\u001b[33m\"\u001b[39;49;00m\u001b[33mGPU\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[94m0\u001b[39;49;00m) \u001b[94mfor\u001b[39;49;00m node \u001b[95min\u001b[39;49;00m ray.nodes()) >= \u001b[94m4\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      "        ), \u001b[33m\"\u001b[39;49;00m\u001b[33mThis test requires at least 4 GPUs\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      "\u001b[1m\u001b[31mE       AssertionError: This test requires at least 4 GPUs\u001b[0m\n",
      "\u001b[1m\u001b[31mE       assert 2.0 >= 4\u001b[0m\n",
      "\u001b[1m\u001b[31mE        +  where 2.0 = sum(<generator object test_torch_tensor_nccl_overlap_timed.<locals>.<genexpr> at 0x7cfaf0f8bbc0>)\u001b[0m\n",
      "\n",
      "\u001b[1m\u001b[31mtest_torch_tensor_dag.py\u001b[0m:379: AssertionError\n",
      "\u001b[31m\u001b[1m________ test_torch_tensor_nccl_overlap_timed[ray_start_regular1-True] _________\u001b[0m\n",
      "\n",
      "ray_start_regular = RayContext(dashboard_url='', python_version='3.13.3', ray_version='3.0.0.dev0', ray_commit='{{RAY_COMMIT_SHA}}')\n",
      "overlap_gpu_communication = True\n",
      "\n",
      "    \u001b[0m\u001b[37m@pytest\u001b[39;49;00m.mark.skipif(\u001b[95mnot\u001b[39;49;00m USE_GPU, reason=\u001b[33m\"\u001b[39;49;00m\u001b[33mSkipping GPU Test\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[33m\"\u001b[39;49;00m\u001b[33mray_start_regular, overlap_gpu_communication\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m,\u001b[90m\u001b[39;49;00m\n",
      "        [({\u001b[33m\"\u001b[39;49;00m\u001b[33mnum_cpus\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m: \u001b[94m4\u001b[39;49;00m}, \u001b[94mFalse\u001b[39;49;00m), ({\u001b[33m\"\u001b[39;49;00m\u001b[33mnum_cpus\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m: \u001b[94m4\u001b[39;49;00m}, \u001b[94mTrue\u001b[39;49;00m)],\u001b[90m\u001b[39;49;00m\n",
      "        indirect=[\u001b[33m\"\u001b[39;49;00m\u001b[33mray_start_regular\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m],\u001b[90m\u001b[39;49;00m\n",
      "    )\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mtest_torch_tensor_nccl_overlap_timed\u001b[39;49;00m(ray_start_regular, overlap_gpu_communication):\u001b[90m\u001b[39;49;00m\n",
      ">       \u001b[94massert\u001b[39;49;00m (\u001b[90m\u001b[39;49;00m\n",
      "            \u001b[96msum\u001b[39;49;00m(node[\u001b[33m\"\u001b[39;49;00m\u001b[33mResources\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m].get(\u001b[33m\"\u001b[39;49;00m\u001b[33mGPU\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[94m0\u001b[39;49;00m) \u001b[94mfor\u001b[39;49;00m node \u001b[95min\u001b[39;49;00m ray.nodes()) >= \u001b[94m4\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      "        ), \u001b[33m\"\u001b[39;49;00m\u001b[33mThis test requires at least 4 GPUs\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      "\u001b[1m\u001b[31mE       AssertionError: This test requires at least 4 GPUs\u001b[0m\n",
      "\u001b[1m\u001b[31mE       assert 2.0 >= 4\u001b[0m\n",
      "\u001b[1m\u001b[31mE        +  where 2.0 = sum(<generator object test_torch_tensor_nccl_overlap_timed.<locals>.<genexpr> at 0x7cfaf0f8adc0>)\u001b[0m\n",
      "\n",
      "\u001b[1m\u001b[31mtest_torch_tensor_dag.py\u001b[0m:379: AssertionError\n",
      "\u001b[31m\u001b[1m________ test_torch_tensor_default_comm[transports0-ray_start_regular0] ________\u001b[0m\n",
      "\n",
      "ray_start_regular = RayContext(dashboard_url='', python_version='3.13.3', ray_version='3.0.0.dev0', ray_commit='{{RAY_COMMIT_SHA}}')\n",
      "transports = ['auto', 'nccl']\n",
      "\n",
      "    \u001b[0m\u001b[37m@pytest\u001b[39;49;00m.mark.skipif(\u001b[95mnot\u001b[39;49;00m USE_GPU, reason=\u001b[33m\"\u001b[39;49;00m\u001b[33mSkipping GPU Test\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mray_start_regular\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [{\u001b[33m\"\u001b[39;49;00m\u001b[33mnum_cpus\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m: \u001b[94m4\u001b[39;49;00m}], indirect=\u001b[94mTrue\u001b[39;49;00m)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[33m\"\u001b[39;49;00m\u001b[33mtransports\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m,\u001b[90m\u001b[39;49;00m\n",
      "        [[\u001b[33m\"\u001b[39;49;00m\u001b[33mauto\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33mnccl\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m], [\u001b[33m\"\u001b[39;49;00m\u001b[33mcustom\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33mnccl\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m], [\u001b[33m\"\u001b[39;49;00m\u001b[33mauto\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33mnccl\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m], [\u001b[33m\"\u001b[39;49;00m\u001b[33mcustom\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33mcustom\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m]],\u001b[90m\u001b[39;49;00m\n",
      "    )\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mtest_torch_tensor_default_comm\u001b[39;49;00m(ray_start_regular, transports):\u001b[90m\u001b[39;49;00m\n",
      ">       \u001b[94massert\u001b[39;49;00m (\u001b[90m\u001b[39;49;00m\n",
      "            \u001b[96msum\u001b[39;49;00m(node[\u001b[33m\"\u001b[39;49;00m\u001b[33mResources\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m].get(\u001b[33m\"\u001b[39;49;00m\u001b[33mGPU\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[94m0\u001b[39;49;00m) \u001b[94mfor\u001b[39;49;00m node \u001b[95min\u001b[39;49;00m ray.nodes()) > \u001b[94m2\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      "        ), \u001b[33m\"\u001b[39;49;00m\u001b[33mThis test requires at least 3 GPUs\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      "\u001b[1m\u001b[31mE       AssertionError: This test requires at least 3 GPUs\u001b[0m\n",
      "\u001b[1m\u001b[31mE       assert 2.0 > 2\u001b[0m\n",
      "\u001b[1m\u001b[31mE        +  where 2.0 = sum(<generator object test_torch_tensor_default_comm.<locals>.<genexpr> at 0x7cfaef553ae0>)\u001b[0m\n",
      "\n",
      "\u001b[1m\u001b[31mtest_torch_tensor_dag.py\u001b[0m:751: AssertionError\n",
      "\u001b[31m\u001b[1m________ test_torch_tensor_default_comm[transports1-ray_start_regular0] ________\u001b[0m\n",
      "\n",
      "ray_start_regular = RayContext(dashboard_url='', python_version='3.13.3', ray_version='3.0.0.dev0', ray_commit='{{RAY_COMMIT_SHA}}')\n",
      "transports = ['custom', 'nccl']\n",
      "\n",
      "    \u001b[0m\u001b[37m@pytest\u001b[39;49;00m.mark.skipif(\u001b[95mnot\u001b[39;49;00m USE_GPU, reason=\u001b[33m\"\u001b[39;49;00m\u001b[33mSkipping GPU Test\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mray_start_regular\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [{\u001b[33m\"\u001b[39;49;00m\u001b[33mnum_cpus\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m: \u001b[94m4\u001b[39;49;00m}], indirect=\u001b[94mTrue\u001b[39;49;00m)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[33m\"\u001b[39;49;00m\u001b[33mtransports\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m,\u001b[90m\u001b[39;49;00m\n",
      "        [[\u001b[33m\"\u001b[39;49;00m\u001b[33mauto\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33mnccl\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m], [\u001b[33m\"\u001b[39;49;00m\u001b[33mcustom\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33mnccl\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m], [\u001b[33m\"\u001b[39;49;00m\u001b[33mauto\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33mnccl\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m], [\u001b[33m\"\u001b[39;49;00m\u001b[33mcustom\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33mcustom\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m]],\u001b[90m\u001b[39;49;00m\n",
      "    )\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mtest_torch_tensor_default_comm\u001b[39;49;00m(ray_start_regular, transports):\u001b[90m\u001b[39;49;00m\n",
      ">       \u001b[94massert\u001b[39;49;00m (\u001b[90m\u001b[39;49;00m\n",
      "            \u001b[96msum\u001b[39;49;00m(node[\u001b[33m\"\u001b[39;49;00m\u001b[33mResources\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m].get(\u001b[33m\"\u001b[39;49;00m\u001b[33mGPU\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[94m0\u001b[39;49;00m) \u001b[94mfor\u001b[39;49;00m node \u001b[95min\u001b[39;49;00m ray.nodes()) > \u001b[94m2\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      "        ), \u001b[33m\"\u001b[39;49;00m\u001b[33mThis test requires at least 3 GPUs\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      "\u001b[1m\u001b[31mE       AssertionError: This test requires at least 3 GPUs\u001b[0m\n",
      "\u001b[1m\u001b[31mE       assert 2.0 > 2\u001b[0m\n",
      "\u001b[1m\u001b[31mE        +  where 2.0 = sum(<generator object test_torch_tensor_default_comm.<locals>.<genexpr> at 0x7cfaef553920>)\u001b[0m\n",
      "\n",
      "\u001b[1m\u001b[31mtest_torch_tensor_dag.py\u001b[0m:751: AssertionError\n",
      "\u001b[31m\u001b[1m________ test_torch_tensor_default_comm[transports2-ray_start_regular0] ________\u001b[0m\n",
      "\n",
      "ray_start_regular = RayContext(dashboard_url='', python_version='3.13.3', ray_version='3.0.0.dev0', ray_commit='{{RAY_COMMIT_SHA}}')\n",
      "transports = ['auto', 'nccl']\n",
      "\n",
      "    \u001b[0m\u001b[37m@pytest\u001b[39;49;00m.mark.skipif(\u001b[95mnot\u001b[39;49;00m USE_GPU, reason=\u001b[33m\"\u001b[39;49;00m\u001b[33mSkipping GPU Test\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mray_start_regular\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [{\u001b[33m\"\u001b[39;49;00m\u001b[33mnum_cpus\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m: \u001b[94m4\u001b[39;49;00m}], indirect=\u001b[94mTrue\u001b[39;49;00m)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[33m\"\u001b[39;49;00m\u001b[33mtransports\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m,\u001b[90m\u001b[39;49;00m\n",
      "        [[\u001b[33m\"\u001b[39;49;00m\u001b[33mauto\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33mnccl\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m], [\u001b[33m\"\u001b[39;49;00m\u001b[33mcustom\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33mnccl\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m], [\u001b[33m\"\u001b[39;49;00m\u001b[33mauto\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33mnccl\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m], [\u001b[33m\"\u001b[39;49;00m\u001b[33mcustom\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33mcustom\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m]],\u001b[90m\u001b[39;49;00m\n",
      "    )\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mtest_torch_tensor_default_comm\u001b[39;49;00m(ray_start_regular, transports):\u001b[90m\u001b[39;49;00m\n",
      ">       \u001b[94massert\u001b[39;49;00m (\u001b[90m\u001b[39;49;00m\n",
      "            \u001b[96msum\u001b[39;49;00m(node[\u001b[33m\"\u001b[39;49;00m\u001b[33mResources\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m].get(\u001b[33m\"\u001b[39;49;00m\u001b[33mGPU\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[94m0\u001b[39;49;00m) \u001b[94mfor\u001b[39;49;00m node \u001b[95min\u001b[39;49;00m ray.nodes()) > \u001b[94m2\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      "        ), \u001b[33m\"\u001b[39;49;00m\u001b[33mThis test requires at least 3 GPUs\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      "\u001b[1m\u001b[31mE       AssertionError: This test requires at least 3 GPUs\u001b[0m\n",
      "\u001b[1m\u001b[31mE       assert 2.0 > 2\u001b[0m\n",
      "\u001b[1m\u001b[31mE        +  where 2.0 = sum(<generator object test_torch_tensor_default_comm.<locals>.<genexpr> at 0x7cfaef553220>)\u001b[0m\n",
      "\n",
      "\u001b[1m\u001b[31mtest_torch_tensor_dag.py\u001b[0m:751: AssertionError\n",
      "\u001b[31m\u001b[1m________ test_torch_tensor_default_comm[transports3-ray_start_regular0] ________\u001b[0m\n",
      "\n",
      "ray_start_regular = RayContext(dashboard_url='', python_version='3.13.3', ray_version='3.0.0.dev0', ray_commit='{{RAY_COMMIT_SHA}}')\n",
      "transports = ['custom', 'custom']\n",
      "\n",
      "    \u001b[0m\u001b[37m@pytest\u001b[39;49;00m.mark.skipif(\u001b[95mnot\u001b[39;49;00m USE_GPU, reason=\u001b[33m\"\u001b[39;49;00m\u001b[33mSkipping GPU Test\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mray_start_regular\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [{\u001b[33m\"\u001b[39;49;00m\u001b[33mnum_cpus\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m: \u001b[94m4\u001b[39;49;00m}], indirect=\u001b[94mTrue\u001b[39;49;00m)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[33m\"\u001b[39;49;00m\u001b[33mtransports\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m,\u001b[90m\u001b[39;49;00m\n",
      "        [[\u001b[33m\"\u001b[39;49;00m\u001b[33mauto\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33mnccl\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m], [\u001b[33m\"\u001b[39;49;00m\u001b[33mcustom\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33mnccl\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m], [\u001b[33m\"\u001b[39;49;00m\u001b[33mauto\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33mnccl\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m], [\u001b[33m\"\u001b[39;49;00m\u001b[33mcustom\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33mcustom\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m]],\u001b[90m\u001b[39;49;00m\n",
      "    )\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mtest_torch_tensor_default_comm\u001b[39;49;00m(ray_start_regular, transports):\u001b[90m\u001b[39;49;00m\n",
      ">       \u001b[94massert\u001b[39;49;00m (\u001b[90m\u001b[39;49;00m\n",
      "            \u001b[96msum\u001b[39;49;00m(node[\u001b[33m\"\u001b[39;49;00m\u001b[33mResources\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m].get(\u001b[33m\"\u001b[39;49;00m\u001b[33mGPU\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[94m0\u001b[39;49;00m) \u001b[94mfor\u001b[39;49;00m node \u001b[95min\u001b[39;49;00m ray.nodes()) > \u001b[94m2\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      "        ), \u001b[33m\"\u001b[39;49;00m\u001b[33mThis test requires at least 3 GPUs\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      "\u001b[1m\u001b[31mE       AssertionError: This test requires at least 3 GPUs\u001b[0m\n",
      "\u001b[1m\u001b[31mE       assert 2.0 > 2\u001b[0m\n",
      "\u001b[1m\u001b[31mE        +  where 2.0 = sum(<generator object test_torch_tensor_default_comm.<locals>.<genexpr> at 0x7cfaf0f8ab20>)\u001b[0m\n",
      "\n",
      "\u001b[1m\u001b[31mtest_torch_tensor_dag.py\u001b[0m:751: AssertionError\n",
      "\u001b[36m\u001b[1m=========================== short test summary info ============================\u001b[0m\n",
      "\u001b[31mFAILED\u001b[0m test_torch_tensor_dag.py::\u001b[1mtest_torch_tensor_nccl_overlap_timed[ray_start_regular0-False]\u001b[0m - AssertionError: This test requires at least 4 GPUs\n",
      "\u001b[31mFAILED\u001b[0m test_torch_tensor_dag.py::\u001b[1mtest_torch_tensor_nccl_overlap_timed[ray_start_regular1-True]\u001b[0m - AssertionError: This test requires at least 4 GPUs\n",
      "\u001b[31mFAILED\u001b[0m test_torch_tensor_dag.py::\u001b[1mtest_torch_tensor_default_comm[transports0-ray_start_regular0]\u001b[0m - AssertionError: This test requires at least 3 GPUs\n",
      "\u001b[31mFAILED\u001b[0m test_torch_tensor_dag.py::\u001b[1mtest_torch_tensor_default_comm[transports1-ray_start_regular0]\u001b[0m - AssertionError: This test requires at least 3 GPUs\n",
      "\u001b[31mFAILED\u001b[0m test_torch_tensor_dag.py::\u001b[1mtest_torch_tensor_default_comm[transports2-ray_start_regular0]\u001b[0m - AssertionError: This test requires at least 3 GPUs\n",
      "\u001b[31mFAILED\u001b[0m test_torch_tensor_dag.py::\u001b[1mtest_torch_tensor_default_comm[transports3-ray_start_regular0]\u001b[0m - AssertionError: This test requires at least 3 GPUs\n",
      "\u001b[31m============= \u001b[31m\u001b[1m6 failed\u001b[0m, \u001b[32m45 passed\u001b[0m, \u001b[33m2 skipped\u001b[0m\u001b[31m in 372.94s (0:06:12)\u001b[0m\u001b[31m ==============\u001b[0m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!python -m pytest -v -s test_torch_tensor_dag.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m============================= test session starts ==============================\u001b[0m\n",
      "platform linux -- Python 3.13.3, pytest-8.3.5, pluggy-1.5.0 -- /home/wxdeng/miniconda3/envs/stbr-unify/bin/python\n",
      "cachedir: .pytest_cache\n",
      "rootdir: /home/wxdeng/stbr-min\n",
      "configfile: pytest.ini\n",
      "plugins: anyio-4.9.0\n",
      "collected 1 item                                                               \u001b[0m\u001b[1m\n",
      "\n",
      "test_compiled_graphs.py::test_event_profiling 2025-06-01 19:17:44,057\tINFO worker.py:1917 -- Started a local Ray instance.\n",
      "2025-06-01 19:17:45,451\tINFO dag_node_operation.py:1108 -- Writing compiled graph schedule visualization to compiled_graph_schedule.png\n",
      "\u001b[32mPASSED\u001b[0m2025-06-01 19:17:46,895\tINFO compiled_dag_node.py:2774 -- Tearing down compiled DAG\n",
      "2025-06-01 19:17:46,896\tINFO compiled_dag_node.py:2779 -- Cancelling compiled worker on actor: Actor(Actor, 46cd761a177dda008a4abb6e01000000)\n",
      "2025-06-01 19:17:46,896\tINFO compiled_dag_node.py:2779 -- Cancelling compiled worker on actor: Actor(Actor, 3faf4fd3590a49a3d3217af201000000)\n",
      "2025-06-01 19:17:46,901\tINFO compiled_dag_node.py:2801 -- Waiting for worker tasks to exit\n",
      "2025-06-01 19:17:46,901\tINFO compiled_dag_node.py:2762 -- Killing actor: Actor(Actor, 46cd761a177dda008a4abb6e01000000)\n",
      "2025-06-01 19:17:46,901\tINFO compiled_dag_node.py:2762 -- Killing actor: Actor(Actor, 3faf4fd3590a49a3d3217af201000000)\n",
      "2025-06-01 19:17:46,901\tINFO compiled_dag_node.py:2804 -- Teardown complete\n",
      "\n",
      "\n",
      "\u001b[32m============================== \u001b[32m\u001b[1m1 passed\u001b[0m\u001b[32m in 7.01s\u001b[0m\u001b[32m ===============================\u001b[0m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!python -m pytest -v -s test_compiled_graphs.py::test_event_profiling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m============================= test session starts ==============================\u001b[0m\n",
      "platform linux -- Python 3.13.3, pytest-8.3.5, pluggy-1.5.0 -- /home/wxdeng/miniconda3/envs/stbr-unify/bin/python\n",
      "cachedir: .pytest_cache\n",
      "rootdir: /home/wxdeng/stbr-min\n",
      "configfile: pytest.ini\n",
      "plugins: anyio-4.9.0\n",
      "collected 7 items                                                              \u001b[0m\u001b[1m\n",
      "\n",
      "test_cpu_communicator_dag.py::test_p2p_basic[ray_start_cluster0] 2025-06-01 19:17:53,626\tINFO worker.py:1723 -- Connecting to existing Ray cluster at address: 128.208.3.124:53916...\n",
      "2025-06-01 19:17:53,641\tINFO worker.py:1917 -- Connected to Ray cluster.\n",
      "2025-06-01 19:17:55,840\tINFO dag_node_operation.py:1108 -- Writing compiled graph schedule visualization to compiled_graph_schedule.png\n",
      "\u001b[32mPASSED\u001b[0m2025-06-01 19:17:55,966\tINFO compiled_dag_node.py:2774 -- Tearing down compiled DAG\n",
      "2025-06-01 19:17:55,967\tINFO compiled_dag_node.py:2779 -- Cancelling compiled worker on actor: Actor(CPUTorchTensorWorker, 9aa1d5976fceb1ae2949935301000000)\n",
      "2025-06-01 19:17:55,967\tINFO compiled_dag_node.py:2779 -- Cancelling compiled worker on actor: Actor(CPUTorchTensorWorker, 15ab151313daed7c577a74c601000000)\n",
      "2025-06-01 19:17:55,975\tINFO compiled_dag_node.py:2801 -- Waiting for worker tasks to exit\n",
      "2025-06-01 19:17:55,975\tINFO compiled_dag_node.py:2762 -- Killing actor: Actor(CPUTorchTensorWorker, 9aa1d5976fceb1ae2949935301000000)\n",
      "2025-06-01 19:17:55,975\tINFO compiled_dag_node.py:2762 -- Killing actor: Actor(CPUTorchTensorWorker, 15ab151313daed7c577a74c601000000)\n",
      "2025-06-01 19:17:55,976\tINFO compiled_dag_node.py:2804 -- Teardown complete\n",
      "\n",
      "test_cpu_communicator_dag.py::test_allreduce_basic[ray_start_cluster0] 2025-06-01 19:17:58,789\tINFO worker.py:1723 -- Connecting to existing Ray cluster at address: 128.208.3.124:64975...\n",
      "2025-06-01 19:17:58,802\tINFO worker.py:1917 -- Connected to Ray cluster.\n",
      "2025-06-01 19:18:00,812\tINFO torch_tensor_nccl_channel.py:770 -- Initializing custom NCCL group 55861f99-24b1-4a08-868f-6a0327032b1a on actors: [Actor(CPUTorchTensorWorker, 9ae4caffcbb43a85d04cd17601000000), Actor(CPUTorchTensorWorker, f47fabc859c1db2e1278fdfa01000000)]\n",
      "2025-06-01 19:18:00,816\tINFO torch_tensor_nccl_channel.py:797 -- NCCL group initialized.\n",
      "2025-06-01 19:18:00,973\tINFO dag_node_operation.py:1108 -- Writing compiled graph schedule visualization to compiled_graph_schedule.png\n",
      "\u001b[32mPASSED\u001b[0m2025-06-01 19:18:02,968\tINFO compiled_dag_node.py:2774 -- Tearing down compiled DAG\n",
      "2025-06-01 19:18:02,968\tINFO compiled_dag_node.py:2779 -- Cancelling compiled worker on actor: Actor(CPUTorchTensorWorker, 9ae4caffcbb43a85d04cd17601000000)\n",
      "2025-06-01 19:18:02,968\tINFO compiled_dag_node.py:2779 -- Cancelling compiled worker on actor: Actor(CPUTorchTensorWorker, f47fabc859c1db2e1278fdfa01000000)\n",
      "2025-06-01 19:18:02,974\tINFO compiled_dag_node.py:2801 -- Waiting for worker tasks to exit\n",
      "2025-06-01 19:18:02,974\tINFO compiled_dag_node.py:2762 -- Killing actor: Actor(CPUTorchTensorWorker, 9ae4caffcbb43a85d04cd17601000000)\n",
      "2025-06-01 19:18:02,975\tINFO compiled_dag_node.py:2762 -- Killing actor: Actor(CPUTorchTensorWorker, f47fabc859c1db2e1278fdfa01000000)\n",
      "2025-06-01 19:18:02,975\tINFO compiled_dag_node.py:2804 -- Teardown complete\n",
      "\n",
      "test_cpu_communicator_dag.py::test_allreduce_get_partial[ray_start_cluster0] 2025-06-01 19:18:05,816\tINFO worker.py:1723 -- Connecting to existing Ray cluster at address: 128.208.3.124:43725...\n",
      "2025-06-01 19:18:05,828\tINFO worker.py:1917 -- Connected to Ray cluster.\n",
      "2025-06-01 19:18:07,727\tINFO torch_tensor_nccl_channel.py:770 -- Initializing custom NCCL group 5d4ad1fe-7a22-4c38-a8e8-183fa3fb4074 on actors: [Actor(CPUTorchTensorWorker, 64827f069aaf8c007c4b5f0f01000000), Actor(CPUTorchTensorWorker, d272c631cf20e12328bf1afe01000000)]\n",
      "2025-06-01 19:18:07,731\tINFO torch_tensor_nccl_channel.py:797 -- NCCL group initialized.\n",
      "2025-06-01 19:18:07,929\tINFO dag_node_operation.py:1108 -- Writing compiled graph schedule visualization to compiled_graph_schedule.png\n",
      "\u001b[32mPASSED\u001b[0m2025-06-01 19:18:09,900\tINFO compiled_dag_node.py:2774 -- Tearing down compiled DAG\n",
      "2025-06-01 19:18:09,900\tINFO compiled_dag_node.py:2779 -- Cancelling compiled worker on actor: Actor(CPUTorchTensorWorker, 64827f069aaf8c007c4b5f0f01000000)\n",
      "2025-06-01 19:18:09,900\tINFO compiled_dag_node.py:2779 -- Cancelling compiled worker on actor: Actor(CPUTorchTensorWorker, d272c631cf20e12328bf1afe01000000)\n",
      "2025-06-01 19:18:09,909\tINFO compiled_dag_node.py:2801 -- Waiting for worker tasks to exit\n",
      "2025-06-01 19:18:09,909\tINFO compiled_dag_node.py:2762 -- Killing actor: Actor(CPUTorchTensorWorker, 64827f069aaf8c007c4b5f0f01000000)\n",
      "2025-06-01 19:18:09,910\tINFO compiled_dag_node.py:2762 -- Killing actor: Actor(CPUTorchTensorWorker, d272c631cf20e12328bf1afe01000000)\n",
      "2025-06-01 19:18:09,910\tINFO compiled_dag_node.py:2804 -- Teardown complete\n",
      "\n",
      "test_cpu_communicator_dag.py::test_allreduce_wrong_shape[ray_start_cluster0] 2025-06-01 19:18:12,825\tINFO worker.py:1723 -- Connecting to existing Ray cluster at address: 128.208.3.124:62312...\n",
      "2025-06-01 19:18:12,835\tINFO worker.py:1917 -- Connected to Ray cluster.\n",
      "2025-06-01 19:18:14,750\tINFO torch_tensor_nccl_channel.py:770 -- Initializing custom NCCL group 5e85e6a3-4c81-47bd-baec-b739b2e13d34 on actors: [Actor(CPUTorchTensorWorker, 0189c411e780bcd3cfa2e9a301000000), Actor(CPUTorchTensorWorker, 8c7d0def7747a2006afe484701000000)]\n",
      "2025-06-01 19:18:14,754\tINFO torch_tensor_nccl_channel.py:797 -- NCCL group initialized.\n",
      "2025-06-01 19:18:14,908\tINFO dag_node_operation.py:1108 -- Writing compiled graph schedule visualization to compiled_graph_schedule.png\n",
      "2025-06-01 19:18:16,883\tINFO compiled_dag_node.py:2774 -- Tearing down compiled DAG\n",
      "\u001b[32mPASSED\u001b[0m2025-06-01 19:18:16,885\tINFO compiled_dag_node.py:2779 -- Cancelling compiled worker on actor: Actor(CPUTorchTensorWorker, 0189c411e780bcd3cfa2e9a301000000)\n",
      "2025-06-01 19:18:16,885\tINFO compiled_dag_node.py:2779 -- Cancelling compiled worker on actor: Actor(CPUTorchTensorWorker, 8c7d0def7747a2006afe484701000000)\n",
      "2025-06-01 19:18:16,894\tINFO compiled_dag_node.py:2801 -- Waiting for worker tasks to exit\n",
      "2025-06-01 19:18:16,894\tINFO compiled_dag_node.py:2804 -- Teardown complete\n",
      "\n",
      "test_cpu_communicator_dag.py::test_allreduce_scheduling[ray_start_cluster0] 2025-06-01 19:18:19,619\tINFO worker.py:1723 -- Connecting to existing Ray cluster at address: 128.208.3.124:60193...\n",
      "2025-06-01 19:18:19,631\tINFO worker.py:1917 -- Connected to Ray cluster.\n",
      "2025-06-01 19:18:21,570\tINFO torch_tensor_nccl_channel.py:770 -- Initializing custom NCCL group 14bbab92-cec6-441a-9fcf-722621c10d54 on actors: [Actor(CPUTorchTensorWorker, a316057d693d4aeb9276bbfa01000000), Actor(CPUTorchTensorWorker, e4ad926781a80ee18932f69c01000000)]\n",
      "2025-06-01 19:18:21,574\tINFO torch_tensor_nccl_channel.py:797 -- NCCL group initialized.\n",
      "2025-06-01 19:18:21,759\tINFO dag_node_operation.py:1108 -- Writing compiled graph schedule visualization to compiled_graph_schedule.png\n",
      "\u001b[32mPASSED\u001b[0m2025-06-01 19:18:23,751\tINFO compiled_dag_node.py:2774 -- Tearing down compiled DAG\n",
      "2025-06-01 19:18:23,751\tINFO compiled_dag_node.py:2779 -- Cancelling compiled worker on actor: Actor(CPUTorchTensorWorker, a316057d693d4aeb9276bbfa01000000)\n",
      "2025-06-01 19:18:23,751\tINFO compiled_dag_node.py:2779 -- Cancelling compiled worker on actor: Actor(CPUTorchTensorWorker, e4ad926781a80ee18932f69c01000000)\n",
      "2025-06-01 19:18:23,760\tINFO compiled_dag_node.py:2801 -- Waiting for worker tasks to exit\n",
      "2025-06-01 19:18:23,760\tINFO compiled_dag_node.py:2762 -- Killing actor: Actor(CPUTorchTensorWorker, a316057d693d4aeb9276bbfa01000000)\n",
      "2025-06-01 19:18:23,760\tINFO compiled_dag_node.py:2762 -- Killing actor: Actor(CPUTorchTensorWorker, e4ad926781a80ee18932f69c01000000)\n",
      "2025-06-01 19:18:23,760\tINFO compiled_dag_node.py:2804 -- Teardown complete\n",
      "\n",
      "test_cpu_communicator_dag.py::test_allreduce_duplicate_actors[ray_start_cluster0] 2025-06-01 19:18:27,510\tINFO worker.py:1723 -- Connecting to existing Ray cluster at address: 128.208.3.124:65302...\n",
      "2025-06-01 19:18:27,522\tINFO worker.py:1917 -- Connected to Ray cluster.\n",
      "\u001b[32mPASSED\u001b[0m\n",
      "test_cpu_communicator_dag.py::test_allreduce_wrong_actors[ray_start_cluster0] 2025-06-01 19:18:30,515\tINFO worker.py:1723 -- Connecting to existing Ray cluster at address: 128.208.3.124:62093...\n",
      "2025-06-01 19:18:30,528\tINFO worker.py:1917 -- Connected to Ray cluster.\n",
      "\u001b[32mPASSED\u001b[0m\n",
      "\n",
      "\u001b[32m============================== \u001b[32m\u001b[1m7 passed\u001b[0m\u001b[32m in 42.74s\u001b[0m\u001b[32m ==============================\u001b[0m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!python -m pytest -v -s test_cpu_communicator_dag.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "# !python -m pytest -v -s test_execution_schedule_gpu.py"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "stbr-coll-sched-0512",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
