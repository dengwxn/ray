{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.chdir(\"/home/wxdeng/stbr-min/python/ray/dag/tests/experimental\")\n",
    "os.environ[\"RAY_DEDUP_LOGS\"] = \"0\"\n",
    "os.environ[\"RAY_PYTEST_USE_GPU\"] = \"1\"\n",
    "os.environ[\"RAY_CGRAPH_VISUALIZE_SCHEDULE\"] = \"1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "# !python -m pytest -v -s test_cpu_communicator_dag.py::test_allreduce_wrong_shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "# !python -m pytest -v -s test_torch_tensor_dag.py::test_torch_tensor_exceptions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m============================= test session starts ==============================\u001b[0m\n",
      "platform linux -- Python 3.13.3, pytest-8.3.5, pluggy-1.5.0 -- /home/wxdeng/miniconda3/envs/stbr-unify/bin/python\n",
      "cachedir: .pytest_cache\n",
      "rootdir: /home/wxdeng/stbr-min\n",
      "configfile: pytest.ini\n",
      "plugins: anyio-4.9.0\n",
      "collected 14 items                                                             \u001b[0m\n",
      "\n",
      "test_execution_schedule.py::TestSelectNextNodes::test_two_candidates_on_same_actor \u001b[32mPASSED\u001b[0m\n",
      "test_execution_schedule.py::TestSelectNextNodes::test_only_one_nccl_write \u001b[32mPASSED\u001b[0m\n",
      "test_execution_schedule.py::TestSelectNextNodes::test_two_nccl_writes \u001b[32mPASSED\u001b[0m\n",
      "test_execution_schedule.py::TestSelectNextNodes::test_only_one_nccl_collective \u001b[32mPASSED\u001b[0m\n",
      "test_execution_schedule.py::TestSelectNextNodes::test_two_nccl_collectives \u001b[32mPASSED\u001b[0m\n",
      "test_execution_schedule.py::TestBuildDAGNodeOperationGraph::test_edge_between_writer_and_reader \u001b[32mPASSED\u001b[0m\n",
      "test_execution_schedule.py::TestBuildDAGNodeOperationGraph::test_edge_between_nodes \u001b[32mPASSED\u001b[0m\n",
      "test_execution_schedule.py::TestBuildDAGNodeOperationGraph::test_two_actors \u001b[32mPASSED\u001b[0m\n",
      "test_execution_schedule.py::TestGenerateActorToExecutionSchedule::test_single_actor_1 \u001b[32mPASSED\u001b[0m\n",
      "test_execution_schedule.py::TestGenerateActorToExecutionSchedule::test_single_actor_2 \u001b[32mPASSED\u001b[0m\n",
      "test_execution_schedule.py::TestGenerateActorToExecutionSchedule::test_two_actors_no_nccl \u001b[32mPASSED\u001b[0m\n",
      "test_execution_schedule.py::TestGenerateActorToExecutionSchedule::test_two_actors_with_nccl \u001b[32mPASSED\u001b[0m\n",
      "test_execution_schedule.py::TestGenerateActorToExecutionSchedule::test_simulate_pp_2workers_2batches_1f1b_no_nccl \u001b[32mPASSED\u001b[0m\n",
      "test_execution_schedule.py::TestGenerateActorToExecutionSchedule::test_simulate_pp_2workers_2batches_1f1b_with_nccl \u001b[32mPASSED\u001b[0m\n",
      "\n",
      "\u001b[32m============================== \u001b[32m\u001b[1m14 passed\u001b[0m\u001b[32m in 0.51s\u001b[0m\u001b[32m ==============================\u001b[0m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!python -m pytest -v -s test_execution_schedule.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m============================= test session starts ==============================\u001b[0m\n",
      "platform linux -- Python 3.13.3, pytest-8.3.5, pluggy-1.5.0 -- /home/wxdeng/miniconda3/envs/stbr-unify/bin/python\n",
      "cachedir: .pytest_cache\n",
      "rootdir: /home/wxdeng/stbr-min\n",
      "configfile: pytest.ini\n",
      "plugins: anyio-4.9.0\n",
      "collected 9 items                                                              \u001b[0m\u001b[1m\n",
      "\n",
      "test_collective_dag.py::test_all_reduce_duplicate_actors[ray_start_regular0] 2025-06-02 12:13:31,844\tINFO worker.py:1929 -- Started a local Ray instance.\n",
      "\u001b[32mPASSED\u001b[0m\n",
      "test_collective_dag.py::test_all_reduce_custom_comm_wrong_actors[ray_start_regular0] 2025-06-02 12:13:34,933\tINFO worker.py:1929 -- Started a local Ray instance.\n",
      "\u001b[32mPASSED\u001b[0m\n",
      "test_collective_dag.py::test_comm_all_reduces[ray_start_regular0] 2025-06-02 12:13:38,970\tINFO worker.py:1929 -- Started a local Ray instance.\n",
      "2025-06-02 12:13:41,626\tINFO dag_node_operation.py:592 -- Writing compiled graph schedule visualization to compiled_graph_schedule.png\n",
      "2025-06-02 12:13:41,743\tINFO compiled_dag_node.py:2246 -- Tearing down compiled DAG\n",
      "2025-06-02 12:13:41,747\tINFO compiled_dag_node.py:2251 -- Cancelling compiled worker on actor: Actor(CPUTorchTensorWorker, 8a53f7b0eb875d9304e9462c01000000)\n",
      "2025-06-02 12:13:41,747\tINFO compiled_dag_node.py:2251 -- Cancelling compiled worker on actor: Actor(CPUTorchTensorWorker, 98c8213f0796dc6eea609fe001000000)\n",
      "2025-06-02 12:13:42,067\tINFO compiled_dag_node.py:2273 -- Waiting for worker tasks to exit\n",
      "2025-06-02 12:13:42,094\tINFO compiled_dag_node.py:2276 -- Teardown complete\n",
      "\u001b[32mPASSED\u001b[0m\n",
      "test_collective_dag.py::test_comm_deduplicate_all_reduces[ray_start_regular0] 2025-06-02 12:13:45,995\tINFO worker.py:1929 -- Started a local Ray instance.\n",
      "2025-06-02 12:13:48,388\tINFO dag_node_operation.py:592 -- Writing compiled graph schedule visualization to compiled_graph_schedule.png\n",
      "2025-06-02 12:13:48,512\tINFO compiled_dag_node.py:2246 -- Tearing down compiled DAG\n",
      "2025-06-02 12:13:48,516\tINFO compiled_dag_node.py:2251 -- Cancelling compiled worker on actor: Actor(CPUTorchTensorWorker, 45da74b4bca5c7fe8f1e72f701000000)\n",
      "2025-06-02 12:13:48,516\tINFO compiled_dag_node.py:2251 -- Cancelling compiled worker on actor: Actor(CPUTorchTensorWorker, 333f450d6cc075d15fb4454801000000)\n",
      "2025-06-02 12:13:48,840\tINFO compiled_dag_node.py:2273 -- Waiting for worker tasks to exit\n",
      "2025-06-02 12:13:48,869\tINFO compiled_dag_node.py:2276 -- Teardown complete\n",
      "\u001b[32mPASSED\u001b[0m\n",
      "test_collective_dag.py::test_comm_deduplicate_p2p_and_collective[ray_start_regular0] 2025-06-02 12:13:52,572\tINFO worker.py:1929 -- Started a local Ray instance.\n",
      "2025-06-02 12:13:54,951\tINFO dag_node_operation.py:592 -- Writing compiled graph schedule visualization to compiled_graph_schedule.png\n",
      "2025-06-02 12:13:55,139\tINFO compiled_dag_node.py:2246 -- Tearing down compiled DAG\n",
      "2025-06-02 12:13:55,143\tINFO compiled_dag_node.py:2251 -- Cancelling compiled worker on actor: Actor(CPUTorchTensorWorker, abda9b7707538ffbdd925c1b01000000)\n",
      "2025-06-02 12:13:55,144\tINFO compiled_dag_node.py:2251 -- Cancelling compiled worker on actor: Actor(CPUTorchTensorWorker, 1271b1934f55da6139ccb4d201000000)\n",
      "2025-06-02 12:13:55,469\tINFO compiled_dag_node.py:2273 -- Waiting for worker tasks to exit\n",
      "2025-06-02 12:13:55,496\tINFO compiled_dag_node.py:2276 -- Teardown complete\n",
      "2025-06-02 12:13:55,622\tINFO dag_node_operation.py:592 -- Writing compiled graph schedule visualization to compiled_graph_schedule.png\n",
      "2025-06-02 12:13:55,750\tINFO compiled_dag_node.py:2246 -- Tearing down compiled DAG\n",
      "2025-06-02 12:13:55,754\tINFO compiled_dag_node.py:2251 -- Cancelling compiled worker on actor: Actor(CPUTorchTensorWorker, abda9b7707538ffbdd925c1b01000000)\n",
      "2025-06-02 12:13:55,754\tINFO compiled_dag_node.py:2251 -- Cancelling compiled worker on actor: Actor(CPUTorchTensorWorker, 1271b1934f55da6139ccb4d201000000)\n",
      "2025-06-02 12:13:55,766\tINFO compiled_dag_node.py:2273 -- Waiting for worker tasks to exit\n",
      "2025-06-02 12:13:55,766\tINFO compiled_dag_node.py:2276 -- Teardown complete\n",
      "\u001b[32mPASSED\u001b[0m\n",
      "test_collective_dag.py::test_custom_comm[ray_start_regular0] 2025-06-02 12:13:58,804\tINFO worker.py:1929 -- Started a local Ray instance.\n",
      "2025-06-02 12:14:01,157\tINFO dag_node_operation.py:592 -- Writing compiled graph schedule visualization to compiled_graph_schedule.png\n",
      "2025-06-02 12:14:01,296\tINFO compiled_dag_node.py:2246 -- Tearing down compiled DAG\n",
      "2025-06-02 12:14:01,300\tINFO compiled_dag_node.py:2251 -- Cancelling compiled worker on actor: Actor(CPUTorchTensorWorker, 7db8dc01654390d281f784d201000000)\n",
      "2025-06-02 12:14:01,300\tINFO compiled_dag_node.py:2251 -- Cancelling compiled worker on actor: Actor(CPUTorchTensorWorker, b242366d49a0be336383f8c701000000)\n",
      "2025-06-02 12:14:01,616\tINFO compiled_dag_node.py:2273 -- Waiting for worker tasks to exit\n",
      "2025-06-02 12:14:01,645\tINFO compiled_dag_node.py:2276 -- Teardown complete\n",
      "2025-06-02 12:14:01,782\tINFO dag_node_operation.py:592 -- Writing compiled graph schedule visualization to compiled_graph_schedule.png\n",
      "2025-06-02 12:14:01,923\tINFO compiled_dag_node.py:2246 -- Tearing down compiled DAG\n",
      "2025-06-02 12:14:01,927\tINFO compiled_dag_node.py:2251 -- Cancelling compiled worker on actor: Actor(CPUTorchTensorWorker, 7db8dc01654390d281f784d201000000)\n",
      "2025-06-02 12:14:01,928\tINFO compiled_dag_node.py:2251 -- Cancelling compiled worker on actor: Actor(CPUTorchTensorWorker, b242366d49a0be336383f8c701000000)\n",
      "2025-06-02 12:14:01,938\tINFO compiled_dag_node.py:2273 -- Waiting for worker tasks to exit\n",
      "2025-06-02 12:14:01,938\tINFO compiled_dag_node.py:2276 -- Teardown complete\n",
      "\u001b[32mPASSED\u001b[0m\n",
      "test_collective_dag.py::test_custom_comm_init_teardown[ray_start_regular0] 2025-06-02 12:14:05,761\tINFO worker.py:1929 -- Started a local Ray instance.\n",
      "2025-06-02 12:14:08,080\tINFO dag_node_operation.py:592 -- Writing compiled graph schedule visualization to compiled_graph_schedule.png\n",
      "2025-06-02 12:14:08,208\tINFO compiled_dag_node.py:2246 -- Tearing down compiled DAG\n",
      "2025-06-02 12:14:08,212\tINFO compiled_dag_node.py:2251 -- Cancelling compiled worker on actor: Actor(CPUTorchTensorWorker, 7cab4906ce14353dc90c837401000000)\n",
      "2025-06-02 12:14:08,212\tINFO compiled_dag_node.py:2251 -- Cancelling compiled worker on actor: Actor(CPUTorchTensorWorker, 302a669959e8bc09cbdce2ef01000000)\n",
      "2025-06-02 12:14:08,532\tINFO compiled_dag_node.py:2273 -- Waiting for worker tasks to exit\n",
      "2025-06-02 12:14:08,563\tINFO compiled_dag_node.py:2276 -- Teardown complete\n",
      "2025-06-02 12:14:08,703\tINFO dag_node_operation.py:592 -- Writing compiled graph schedule visualization to compiled_graph_schedule.png\n",
      "2025-06-02 12:14:08,842\tINFO compiled_dag_node.py:2246 -- Tearing down compiled DAG\n",
      "2025-06-02 12:14:08,846\tINFO compiled_dag_node.py:2251 -- Cancelling compiled worker on actor: Actor(CPUTorchTensorWorker, 7cab4906ce14353dc90c837401000000)\n",
      "2025-06-02 12:14:08,846\tINFO compiled_dag_node.py:2251 -- Cancelling compiled worker on actor: Actor(CPUTorchTensorWorker, 302a669959e8bc09cbdce2ef01000000)\n",
      "2025-06-02 12:14:08,855\tINFO compiled_dag_node.py:2273 -- Waiting for worker tasks to exit\n",
      "2025-06-02 12:14:08,855\tINFO compiled_dag_node.py:2276 -- Teardown complete\n",
      "\u001b[32mPASSED\u001b[0m\n",
      "test_collective_dag.py::test_exec_schedules_ddp[2-ray_start_regular0] 2025-06-02 12:14:12,882\tINFO worker.py:1929 -- Started a local Ray instance.\n",
      "2025-06-02 12:14:13,551\tINFO torch_tensor_nccl_channel.py:770 -- Initializing custom NCCL group 824e5943-7487-46db-927b-0f45aede0508 on actors: [Actor(DDPWorker, 3066205e1f332028fa89410301000000), Actor(DDPWorker, 7b97ee457cd9bb782bf360c701000000)]\n",
      "2025-06-02 12:14:15,197\tINFO torch_tensor_nccl_channel.py:797 -- NCCL group initialized.\n",
      "2025-06-02 12:14:15,344\tINFO dag_node_operation.py:592 -- Writing compiled graph schedule visualization to compiled_graph_schedule.png\n",
      "\u001b[32mPASSED\u001b[0m2025-06-02 12:14:15,521\tINFO compiled_dag_node.py:2246 -- Tearing down compiled DAG\n",
      "2025-06-02 12:14:15,528\tINFO compiled_dag_node.py:2251 -- Cancelling compiled worker on actor: Actor(DDPWorker, 3066205e1f332028fa89410301000000)\n",
      "2025-06-02 12:14:15,528\tINFO compiled_dag_node.py:2251 -- Cancelling compiled worker on actor: Actor(DDPWorker, 7b97ee457cd9bb782bf360c701000000)\n",
      "2025-06-02 12:14:15,536\tINFO compiled_dag_node.py:2273 -- Waiting for worker tasks to exit\n",
      "2025-06-02 12:14:15,536\tINFO compiled_dag_node.py:2234 -- Killing actor: Actor(DDPWorker, 3066205e1f332028fa89410301000000)\n",
      "2025-06-02 12:14:15,537\tINFO compiled_dag_node.py:2234 -- Killing actor: Actor(DDPWorker, 7b97ee457cd9bb782bf360c701000000)\n",
      "2025-06-02 12:14:15,537\tINFO compiled_dag_node.py:2276 -- Teardown complete\n",
      "\n",
      "test_collective_dag.py::test_exec_schedules_ddp[4-ray_start_regular0] 2025-06-02 12:14:19,431\tINFO worker.py:1929 -- Started a local Ray instance.\n",
      "2025-06-02 12:14:20,430\tINFO torch_tensor_nccl_channel.py:770 -- Initializing custom NCCL group 02b880fa-32f1-4d25-81e5-ddc99805f251 on actors: [Actor(DDPWorker, 4abbf4b039b7f4fb2565a66701000000), Actor(DDPWorker, 3e6193a57edc7d22c7d4a0d701000000), Actor(DDPWorker, 748aa59851bd5c08ae86137c01000000), Actor(DDPWorker, de227c61b7200d7cabf7668301000000)]\n",
      "2025-06-02 12:14:22,110\tINFO torch_tensor_nccl_channel.py:797 -- NCCL group initialized.\n",
      "2025-06-02 12:14:22,440\tINFO dag_node_operation.py:592 -- Writing compiled graph schedule visualization to compiled_graph_schedule.png\n",
      "\u001b[32mPASSED\u001b[0m2025-06-02 12:14:22,679\tINFO compiled_dag_node.py:2246 -- Tearing down compiled DAG\n",
      "2025-06-02 12:14:22,689\tINFO compiled_dag_node.py:2251 -- Cancelling compiled worker on actor: Actor(DDPWorker, 4abbf4b039b7f4fb2565a66701000000)\n",
      "2025-06-02 12:14:22,689\tINFO compiled_dag_node.py:2251 -- Cancelling compiled worker on actor: Actor(DDPWorker, 3e6193a57edc7d22c7d4a0d701000000)\n",
      "2025-06-02 12:14:22,689\tINFO compiled_dag_node.py:2251 -- Cancelling compiled worker on actor: Actor(DDPWorker, 748aa59851bd5c08ae86137c01000000)\n",
      "2025-06-02 12:14:22,689\tINFO compiled_dag_node.py:2251 -- Cancelling compiled worker on actor: Actor(DDPWorker, de227c61b7200d7cabf7668301000000)\n",
      "2025-06-02 12:14:22,700\tINFO compiled_dag_node.py:2273 -- Waiting for worker tasks to exit\n",
      "2025-06-02 12:14:22,700\tINFO compiled_dag_node.py:2234 -- Killing actor: Actor(DDPWorker, 4abbf4b039b7f4fb2565a66701000000)\n",
      "2025-06-02 12:14:22,700\tINFO compiled_dag_node.py:2234 -- Killing actor: Actor(DDPWorker, 3e6193a57edc7d22c7d4a0d701000000)\n",
      "2025-06-02 12:14:22,700\tINFO compiled_dag_node.py:2234 -- Killing actor: Actor(DDPWorker, 748aa59851bd5c08ae86137c01000000)\n",
      "2025-06-02 12:14:22,701\tINFO compiled_dag_node.py:2234 -- Killing actor: Actor(DDPWorker, de227c61b7200d7cabf7668301000000)\n",
      "2025-06-02 12:14:22,701\tINFO compiled_dag_node.py:2276 -- Teardown complete\n",
      "\n",
      "\n",
      "\u001b[32m============================== \u001b[32m\u001b[1m9 passed\u001b[0m\u001b[32m in 56.65s\u001b[0m\u001b[32m ==============================\u001b[0m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!python -m pytest -v -s test_collective_dag.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m============================= test session starts ==============================\u001b[0m\n",
      "platform linux -- Python 3.13.3, pytest-8.3.5, pluggy-1.5.0 -- /home/wxdeng/miniconda3/envs/stbr-unify/bin/python\n",
      "cachedir: .pytest_cache\n",
      "rootdir: /home/wxdeng/stbr-min\n",
      "configfile: pytest.ini\n",
      "plugins: anyio-4.9.0\n",
      "collected 53 items                                                             \u001b[0m\u001b[1m\n",
      "\n",
      "test_torch_tensor_dag.py::test_torch_tensor_p2p[ray_start_regular0] 2025-06-02 12:14:29,818\tINFO worker.py:1929 -- Started a local Ray instance.\n",
      "2025-06-02 12:14:32,183\tINFO torch_tensor_nccl_channel.py:772 -- Creating NCCL group 0177522e-7d9a-4388-b245-6d45ed5d585c on actors: [Actor(TorchTensorWorker, e7cc94b82248dc0981eff57a01000000), Actor(TorchTensorWorker, 1175afbea0534272c63bb16301000000)]\n",
      "2025-06-02 12:14:33,275\tINFO torch_tensor_nccl_channel.py:797 -- NCCL group initialized.\n",
      "2025-06-02 12:14:33,444\tINFO dag_node_operation.py:592 -- Writing compiled graph schedule visualization to compiled_graph_schedule.png\n",
      "\u001b[32mPASSED\u001b[0m2025-06-02 12:14:33,624\tINFO compiled_dag_node.py:2246 -- Tearing down compiled DAG\n",
      "2025-06-02 12:14:33,625\tINFO compiled_dag_node.py:2251 -- Cancelling compiled worker on actor: Actor(TorchTensorWorker, e7cc94b82248dc0981eff57a01000000)\n",
      "2025-06-02 12:14:33,625\tINFO compiled_dag_node.py:2251 -- Cancelling compiled worker on actor: Actor(TorchTensorWorker, 1175afbea0534272c63bb16301000000)\n",
      "\u001b[36m(TorchTensorWorker pid=697404)\u001b[0m Destructing NCCL group on actor: Actor(TorchTensorWorker, 1175afbea0534272c63bb16301000000)\n",
      "\u001b[36m(TorchTensorWorker pid=697403)\u001b[0m Destructing NCCL group on actor: Actor(TorchTensorWorker, e7cc94b82248dc0981eff57a01000000)\n",
      "2025-06-02 12:14:34,141\tINFO compiled_dag_node.py:2273 -- Waiting for worker tasks to exit\n",
      "2025-06-02 12:14:34,142\tINFO compiled_dag_node.py:2234 -- Killing actor: Actor(TorchTensorWorker, e7cc94b82248dc0981eff57a01000000)\n",
      "2025-06-02 12:14:34,142\tINFO compiled_dag_node.py:2234 -- Killing actor: Actor(TorchTensorWorker, 1175afbea0534272c63bb16301000000)\n",
      "2025-06-02 12:14:34,142\tINFO compiled_dag_node.py:2276 -- Teardown complete\n",
      "\n",
      "test_torch_tensor_dag.py::test_torch_tensor_as_dag_input[ray_start_regular0] 2025-06-02 12:14:38,138\tINFO worker.py:1929 -- Started a local Ray instance.\n",
      "2025-06-02 12:14:39,988\tINFO dag_node_operation.py:592 -- Writing compiled graph schedule visualization to compiled_graph_schedule.png\n",
      "\u001b[32mPASSED\u001b[0m2025-06-02 12:14:40,295\tINFO compiled_dag_node.py:2246 -- Tearing down compiled DAG\n",
      "2025-06-02 12:14:40,295\tINFO compiled_dag_node.py:2251 -- Cancelling compiled worker on actor: Actor(TorchTensorWorker, 51722f0bd9496868cfee6c0301000000)\n",
      "2025-06-02 12:14:40,298\tINFO compiled_dag_node.py:2273 -- Waiting for worker tasks to exit\n",
      "2025-06-02 12:14:40,298\tINFO compiled_dag_node.py:2234 -- Killing actor: Actor(TorchTensorWorker, 51722f0bd9496868cfee6c0301000000)\n",
      "2025-06-02 12:14:40,299\tINFO compiled_dag_node.py:2276 -- Teardown complete\n",
      "\n",
      "test_torch_tensor_dag.py::test_torch_tensor_nccl[False-False-ray_start_regular0] 2025-06-02 12:14:43,956\tINFO worker.py:1929 -- Started a local Ray instance.\n",
      "2025-06-02 12:14:46,228\tINFO torch_tensor_nccl_channel.py:772 -- Creating NCCL group 9cb52d45-7bf6-46a9-a565-f317c8ddbc1d on actors: [Actor(TorchTensorWorker, 31be1154c6522c0fdf8b388101000000), Actor(TorchTensorWorker, 0ba380057ffa99656a8cca8e01000000)]\n",
      "2025-06-02 12:14:47,307\tINFO torch_tensor_nccl_channel.py:797 -- NCCL group initialized.\n",
      "2025-06-02 12:14:47,481\tINFO dag_node_operation.py:592 -- Writing compiled graph schedule visualization to compiled_graph_schedule.png\n",
      "2025-06-02 12:14:47,668\tINFO compiled_dag_node.py:2246 -- Tearing down compiled DAG\n",
      "2025-06-02 12:14:47,669\tINFO compiled_dag_node.py:2251 -- Cancelling compiled worker on actor: Actor(TorchTensorWorker, 31be1154c6522c0fdf8b388101000000)\n",
      "2025-06-02 12:14:47,669\tINFO compiled_dag_node.py:2251 -- Cancelling compiled worker on actor: Actor(TorchTensorWorker, 0ba380057ffa99656a8cca8e01000000)\n",
      "\u001b[36m(TorchTensorWorker pid=698774)\u001b[0m Destructing NCCL group on actor: Actor(TorchTensorWorker, 31be1154c6522c0fdf8b388101000000)\n",
      "\u001b[36m(TorchTensorWorker pid=698775)\u001b[0m Destructing NCCL group on actor: Actor(TorchTensorWorker, 0ba380057ffa99656a8cca8e01000000)\n",
      "2025-06-02 12:14:48,188\tINFO compiled_dag_node.py:2273 -- Waiting for worker tasks to exit\n",
      "2025-06-02 12:14:48,188\tINFO compiled_dag_node.py:2276 -- Teardown complete\n",
      "2025-06-02 12:14:48,199\tINFO torch_tensor_nccl_channel.py:772 -- Creating NCCL group 816dfee6-7124-49f0-9702-3736fd3765aa on actors: [Actor(TorchTensorWorker, 31be1154c6522c0fdf8b388101000000), Actor(TorchTensorWorker, 0ba380057ffa99656a8cca8e01000000)]\n",
      "2025-06-02 12:14:48,232\tINFO torch_tensor_nccl_channel.py:797 -- NCCL group initialized.\n",
      "2025-06-02 12:14:48,358\tINFO dag_node_operation.py:592 -- Writing compiled graph schedule visualization to compiled_graph_schedule.png\n",
      "\u001b[32mPASSED\u001b[0m2025-06-02 12:14:48,512\tINFO compiled_dag_node.py:2246 -- Tearing down compiled DAG\n",
      "2025-06-02 12:14:48,513\tINFO compiled_dag_node.py:2251 -- Cancelling compiled worker on actor: Actor(TorchTensorWorker, 31be1154c6522c0fdf8b388101000000)\n",
      "2025-06-02 12:14:48,513\tINFO compiled_dag_node.py:2251 -- Cancelling compiled worker on actor: Actor(TorchTensorWorker, 0ba380057ffa99656a8cca8e01000000)\n",
      "\u001b[36m(TorchTensorWorker pid=698774)\u001b[0m Destructing NCCL group on actor: Actor(TorchTensorWorker, 31be1154c6522c0fdf8b388101000000)\n",
      "\u001b[36m(TorchTensorWorker pid=698775)\u001b[0m Destructing NCCL group on actor: Actor(TorchTensorWorker, 0ba380057ffa99656a8cca8e01000000)\n",
      "2025-06-02 12:14:49,013\tINFO compiled_dag_node.py:2273 -- Waiting for worker tasks to exit\n",
      "2025-06-02 12:14:49,013\tINFO compiled_dag_node.py:2234 -- Killing actor: Actor(TorchTensorWorker, 31be1154c6522c0fdf8b388101000000)\n",
      "2025-06-02 12:14:49,014\tINFO compiled_dag_node.py:2234 -- Killing actor: Actor(TorchTensorWorker, 0ba380057ffa99656a8cca8e01000000)\n",
      "2025-06-02 12:14:49,014\tINFO compiled_dag_node.py:2276 -- Teardown complete\n",
      "\n",
      "test_torch_tensor_dag.py::test_torch_tensor_nccl[False-True-ray_start_regular0] 2025-06-02 12:14:52,064\tINFO worker.py:1929 -- Started a local Ray instance.\n",
      "2025-06-02 12:14:54,374\tINFO torch_tensor_nccl_channel.py:772 -- Creating NCCL group fe514bec-d6de-4dc8-8a6b-fac524b7aa40 on actors: [Actor(TorchTensorWorker, 69d9426f81d4121a9ff0d44301000000), Actor(TorchTensorWorker, 19d647383b72064277e161ed01000000)]\n",
      "2025-06-02 12:14:55,476\tINFO torch_tensor_nccl_channel.py:797 -- NCCL group initialized.\n",
      "2025-06-02 12:14:55,643\tINFO dag_node_operation.py:592 -- Writing compiled graph schedule visualization to compiled_graph_schedule.png\n",
      "2025-06-02 12:14:55,835\tINFO compiled_dag_node.py:2246 -- Tearing down compiled DAG\n",
      "2025-06-02 12:14:55,835\tINFO compiled_dag_node.py:2251 -- Cancelling compiled worker on actor: Actor(TorchTensorWorker, 19d647383b72064277e161ed01000000)\n",
      "2025-06-02 12:14:55,835\tINFO compiled_dag_node.py:2251 -- Cancelling compiled worker on actor: Actor(TorchTensorWorker, 69d9426f81d4121a9ff0d44301000000)\n",
      "\u001b[36m(TorchTensorWorker pid=699491)\u001b[0m Destructing NCCL group on actor: Actor(TorchTensorWorker, 19d647383b72064277e161ed01000000)\n",
      "\u001b[36m(TorchTensorWorker pid=699492)\u001b[0m Destructing NCCL group on actor: Actor(TorchTensorWorker, 69d9426f81d4121a9ff0d44301000000)\n",
      "2025-06-02 12:14:56,353\tINFO compiled_dag_node.py:2273 -- Waiting for worker tasks to exit\n",
      "2025-06-02 12:14:56,353\tINFO compiled_dag_node.py:2276 -- Teardown complete\n",
      "2025-06-02 12:14:56,365\tINFO torch_tensor_nccl_channel.py:772 -- Creating NCCL group 8a09e43f-2c29-43b5-9a20-c22d5c754134 on actors: [Actor(TorchTensorWorker, 69d9426f81d4121a9ff0d44301000000), Actor(TorchTensorWorker, 19d647383b72064277e161ed01000000)]\n",
      "2025-06-02 12:14:56,398\tINFO torch_tensor_nccl_channel.py:797 -- NCCL group initialized.\n",
      "2025-06-02 12:14:56,533\tINFO dag_node_operation.py:592 -- Writing compiled graph schedule visualization to compiled_graph_schedule.png\n",
      "\u001b[32mPASSED\u001b[0m2025-06-02 12:14:56,688\tINFO compiled_dag_node.py:2246 -- Tearing down compiled DAG\n",
      "2025-06-02 12:14:56,689\tINFO compiled_dag_node.py:2251 -- Cancelling compiled worker on actor: Actor(TorchTensorWorker, 19d647383b72064277e161ed01000000)\n",
      "2025-06-02 12:14:56,689\tINFO compiled_dag_node.py:2251 -- Cancelling compiled worker on actor: Actor(TorchTensorWorker, 69d9426f81d4121a9ff0d44301000000)\n",
      "\u001b[36m(TorchTensorWorker pid=699491)\u001b[0m Destructing NCCL group on actor: Actor(TorchTensorWorker, 19d647383b72064277e161ed01000000)\n",
      "\u001b[36m(TorchTensorWorker pid=699492)\u001b[0m Destructing NCCL group on actor: Actor(TorchTensorWorker, 69d9426f81d4121a9ff0d44301000000)\n",
      "2025-06-02 12:14:57,191\tINFO compiled_dag_node.py:2273 -- Waiting for worker tasks to exit\n",
      "2025-06-02 12:14:57,191\tINFO compiled_dag_node.py:2234 -- Killing actor: Actor(TorchTensorWorker, 19d647383b72064277e161ed01000000)\n",
      "2025-06-02 12:14:57,192\tINFO compiled_dag_node.py:2234 -- Killing actor: Actor(TorchTensorWorker, 69d9426f81d4121a9ff0d44301000000)\n",
      "2025-06-02 12:14:57,192\tINFO compiled_dag_node.py:2276 -- Teardown complete\n",
      "\n",
      "test_torch_tensor_dag.py::test_torch_tensor_nccl[True-False-ray_start_regular0] 2025-06-02 12:15:00,245\tINFO worker.py:1929 -- Started a local Ray instance.\n",
      "2025-06-02 12:15:02,573\tINFO torch_tensor_nccl_channel.py:772 -- Creating NCCL group 3282417f-d6f9-4cf7-b3f1-c341bae100cd on actors: [Actor(TorchTensorWorker, 48048f4553fc589cee0bffde01000000), Actor(TorchTensorWorker, 6cc5c3319fc9cf0a9780c17801000000)]\n",
      "2025-06-02 12:15:03,654\tINFO torch_tensor_nccl_channel.py:797 -- NCCL group initialized.\n",
      "2025-06-02 12:15:03,785\tINFO dag_node_operation.py:592 -- Writing compiled graph schedule visualization to compiled_graph_schedule.png\n",
      "2025-06-02 12:15:04,003\tINFO compiled_dag_node.py:2246 -- Tearing down compiled DAG\n",
      "2025-06-02 12:15:04,004\tINFO compiled_dag_node.py:2251 -- Cancelling compiled worker on actor: Actor(TorchTensorWorker, 48048f4553fc589cee0bffde01000000)\n",
      "2025-06-02 12:15:04,004\tINFO compiled_dag_node.py:2251 -- Cancelling compiled worker on actor: Actor(TorchTensorWorker, 6cc5c3319fc9cf0a9780c17801000000)\n",
      "\u001b[36m(TorchTensorWorker pid=700244)\u001b[0m Destructing NCCL group on actor: Actor(TorchTensorWorker, 6cc5c3319fc9cf0a9780c17801000000)\n",
      "\u001b[36m(TorchTensorWorker pid=700243)\u001b[0m Destructing NCCL group on actor: Actor(TorchTensorWorker, 48048f4553fc589cee0bffde01000000)\n",
      "2025-06-02 12:15:04,522\tINFO compiled_dag_node.py:2273 -- Waiting for worker tasks to exit\n",
      "2025-06-02 12:15:04,522\tINFO compiled_dag_node.py:2276 -- Teardown complete\n",
      "2025-06-02 12:15:04,533\tINFO torch_tensor_nccl_channel.py:772 -- Creating NCCL group 8e1a45b1-3a29-452e-904a-81a6d91e88b6 on actors: [Actor(TorchTensorWorker, 48048f4553fc589cee0bffde01000000), Actor(TorchTensorWorker, 6cc5c3319fc9cf0a9780c17801000000)]\n",
      "2025-06-02 12:15:04,565\tINFO torch_tensor_nccl_channel.py:797 -- NCCL group initialized.\n",
      "2025-06-02 12:15:04,698\tINFO dag_node_operation.py:592 -- Writing compiled graph schedule visualization to compiled_graph_schedule.png\n",
      "\u001b[32mPASSED\u001b[0m2025-06-02 12:15:04,885\tINFO compiled_dag_node.py:2246 -- Tearing down compiled DAG\n",
      "2025-06-02 12:15:04,885\tINFO compiled_dag_node.py:2251 -- Cancelling compiled worker on actor: Actor(TorchTensorWorker, 48048f4553fc589cee0bffde01000000)\n",
      "2025-06-02 12:15:04,885\tINFO compiled_dag_node.py:2251 -- Cancelling compiled worker on actor: Actor(TorchTensorWorker, 6cc5c3319fc9cf0a9780c17801000000)\n",
      "\u001b[36m(TorchTensorWorker pid=700244)\u001b[0m Destructing NCCL group on actor: Actor(TorchTensorWorker, 6cc5c3319fc9cf0a9780c17801000000)\n",
      "\u001b[36m(TorchTensorWorker pid=700243)\u001b[0m Destructing NCCL group on actor: Actor(TorchTensorWorker, 48048f4553fc589cee0bffde01000000)\n",
      "2025-06-02 12:15:05,402\tINFO compiled_dag_node.py:2273 -- Waiting for worker tasks to exit\n",
      "2025-06-02 12:15:05,403\tINFO compiled_dag_node.py:2234 -- Killing actor: Actor(TorchTensorWorker, 48048f4553fc589cee0bffde01000000)\n",
      "2025-06-02 12:15:05,403\tINFO compiled_dag_node.py:2234 -- Killing actor: Actor(TorchTensorWorker, 6cc5c3319fc9cf0a9780c17801000000)\n",
      "2025-06-02 12:15:05,403\tINFO compiled_dag_node.py:2276 -- Teardown complete\n",
      "\n",
      "test_torch_tensor_dag.py::test_torch_tensor_nccl[True-True-ray_start_regular0] 2025-06-02 12:15:08,581\tINFO worker.py:1929 -- Started a local Ray instance.\n",
      "2025-06-02 12:15:10,830\tINFO torch_tensor_nccl_channel.py:772 -- Creating NCCL group ae2db370-9533-4ac8-be8f-9bdf61004ab2 on actors: [Actor(TorchTensorWorker, 9cf48c06cc72f80c1c58409901000000), Actor(TorchTensorWorker, 7ac93ed4f8acc2d2f359fc9f01000000)]\n",
      "2025-06-02 12:15:11,915\tINFO torch_tensor_nccl_channel.py:797 -- NCCL group initialized.\n",
      "2025-06-02 12:15:12,089\tINFO dag_node_operation.py:592 -- Writing compiled graph schedule visualization to compiled_graph_schedule.png\n",
      "2025-06-02 12:15:12,278\tINFO compiled_dag_node.py:2246 -- Tearing down compiled DAG\n",
      "2025-06-02 12:15:12,279\tINFO compiled_dag_node.py:2251 -- Cancelling compiled worker on actor: Actor(TorchTensorWorker, 9cf48c06cc72f80c1c58409901000000)\n",
      "2025-06-02 12:15:12,279\tINFO compiled_dag_node.py:2251 -- Cancelling compiled worker on actor: Actor(TorchTensorWorker, 7ac93ed4f8acc2d2f359fc9f01000000)\n",
      "\u001b[36m(TorchTensorWorker pid=700995)\u001b[0m Destructing NCCL group on actor: Actor(TorchTensorWorker, 9cf48c06cc72f80c1c58409901000000)\n",
      "\u001b[36m(TorchTensorWorker pid=700994)\u001b[0m Destructing NCCL group on actor: Actor(TorchTensorWorker, 7ac93ed4f8acc2d2f359fc9f01000000)\n",
      "2025-06-02 12:15:12,798\tINFO compiled_dag_node.py:2273 -- Waiting for worker tasks to exit\n",
      "2025-06-02 12:15:12,799\tINFO compiled_dag_node.py:2276 -- Teardown complete\n",
      "2025-06-02 12:15:12,810\tINFO torch_tensor_nccl_channel.py:772 -- Creating NCCL group 76bcc430-367f-4c6e-81f3-acb3a8fde5e0 on actors: [Actor(TorchTensorWorker, 9cf48c06cc72f80c1c58409901000000), Actor(TorchTensorWorker, 7ac93ed4f8acc2d2f359fc9f01000000)]\n",
      "2025-06-02 12:15:12,843\tINFO torch_tensor_nccl_channel.py:797 -- NCCL group initialized.\n",
      "2025-06-02 12:15:12,972\tINFO dag_node_operation.py:592 -- Writing compiled graph schedule visualization to compiled_graph_schedule.png\n",
      "\u001b[32mPASSED\u001b[0m2025-06-02 12:15:13,127\tINFO compiled_dag_node.py:2246 -- Tearing down compiled DAG\n",
      "2025-06-02 12:15:13,128\tINFO compiled_dag_node.py:2251 -- Cancelling compiled worker on actor: Actor(TorchTensorWorker, 9cf48c06cc72f80c1c58409901000000)\n",
      "2025-06-02 12:15:13,128\tINFO compiled_dag_node.py:2251 -- Cancelling compiled worker on actor: Actor(TorchTensorWorker, 7ac93ed4f8acc2d2f359fc9f01000000)\n",
      "\u001b[36m(TorchTensorWorker pid=700995)\u001b[0m Destructing NCCL group on actor: Actor(TorchTensorWorker, 9cf48c06cc72f80c1c58409901000000)\n",
      "\u001b[36m(TorchTensorWorker pid=700994)\u001b[0m Destructing NCCL group on actor: Actor(TorchTensorWorker, 7ac93ed4f8acc2d2f359fc9f01000000)\n",
      "2025-06-02 12:15:13,629\tINFO compiled_dag_node.py:2273 -- Waiting for worker tasks to exit\n",
      "2025-06-02 12:15:13,629\tINFO compiled_dag_node.py:2234 -- Killing actor: Actor(TorchTensorWorker, 9cf48c06cc72f80c1c58409901000000)\n",
      "2025-06-02 12:15:13,630\tINFO compiled_dag_node.py:2234 -- Killing actor: Actor(TorchTensorWorker, 7ac93ed4f8acc2d2f359fc9f01000000)\n",
      "2025-06-02 12:15:13,630\tINFO compiled_dag_node.py:2276 -- Teardown complete\n",
      "\n",
      "test_torch_tensor_dag.py::test_torch_tensor_shm[ray_start_regular0] 2025-06-02 12:15:16,738\tINFO worker.py:1929 -- Started a local Ray instance.\n",
      "2025-06-02 12:15:18,808\tINFO dag_node_operation.py:592 -- Writing compiled graph schedule visualization to compiled_graph_schedule.png\n",
      "2025-06-02 12:15:19,681\tINFO compiled_dag_node.py:2246 -- Tearing down compiled DAG\n",
      "2025-06-02 12:15:19,681\tINFO compiled_dag_node.py:2251 -- Cancelling compiled worker on actor: Actor(TorchTensorWorker, e4f807c6642604b84824ca4e01000000)\n",
      "2025-06-02 12:15:19,682\tINFO compiled_dag_node.py:2251 -- Cancelling compiled worker on actor: Actor(TorchTensorWorker, a7288188e6d65f750696e29301000000)\n",
      "2025-06-02 12:15:19,688\tINFO compiled_dag_node.py:2273 -- Waiting for worker tasks to exit\n",
      "2025-06-02 12:15:19,688\tINFO compiled_dag_node.py:2276 -- Teardown complete\n",
      "2025-06-02 12:15:19,733\tINFO dag_node_operation.py:592 -- Writing compiled graph schedule visualization to compiled_graph_schedule.png\n",
      "2025-06-02 12:15:19,849\tINFO compiled_dag_node.py:2246 -- Tearing down compiled DAG\n",
      "2025-06-02 12:15:19,849\tINFO compiled_dag_node.py:2251 -- Cancelling compiled worker on actor: Actor(TorchTensorWorker, e4f807c6642604b84824ca4e01000000)\n",
      "2025-06-02 12:15:19,849\tINFO compiled_dag_node.py:2251 -- Cancelling compiled worker on actor: Actor(TorchTensorWorker, a7288188e6d65f750696e29301000000)\n",
      "2025-06-02 12:15:19,855\tINFO compiled_dag_node.py:2273 -- Waiting for worker tasks to exit\n",
      "2025-06-02 12:15:19,855\tINFO compiled_dag_node.py:2276 -- Teardown complete\n",
      "\u001b[32mPASSED\u001b[0m\n",
      "test_torch_tensor_dag.py::test_torch_tensor_auto[num_gpus0-ray_start_regular0] 2025-06-02 12:15:23,953\tINFO worker.py:1929 -- Started a local Ray instance.\n",
      "2025-06-02 12:15:26,066\tINFO dag_node_operation.py:592 -- Writing compiled graph schedule visualization to compiled_graph_schedule.png\n",
      "2025-06-02 12:15:26,212\tINFO compiled_dag_node.py:2246 -- Tearing down compiled DAG\n",
      "2025-06-02 12:15:26,212\tINFO compiled_dag_node.py:2251 -- Cancelling compiled worker on actor: Actor(TorchTensorWorker, 3fd3e47c127c6e5ed93afb4301000000)\n",
      "2025-06-02 12:15:26,212\tINFO compiled_dag_node.py:2251 -- Cancelling compiled worker on actor: Actor(TorchTensorWorker, 54873108da198974aa0193e901000000)\n",
      "2025-06-02 12:15:26,219\tINFO compiled_dag_node.py:2273 -- Waiting for worker tasks to exit\n",
      "2025-06-02 12:15:26,220\tINFO compiled_dag_node.py:2276 -- Teardown complete\n",
      "2025-06-02 12:15:26,342\tINFO dag_node_operation.py:592 -- Writing compiled graph schedule visualization to compiled_graph_schedule.png\n",
      "\u001b[32mPASSED\u001b[0m2025-06-02 12:15:26,485\tINFO compiled_dag_node.py:2246 -- Tearing down compiled DAG\n",
      "2025-06-02 12:15:26,486\tINFO compiled_dag_node.py:2251 -- Cancelling compiled worker on actor: Actor(TorchTensorWorker, 3fd3e47c127c6e5ed93afb4301000000)\n",
      "2025-06-02 12:15:26,486\tINFO compiled_dag_node.py:2251 -- Cancelling compiled worker on actor: Actor(TorchTensorWorker, 54873108da198974aa0193e901000000)\n",
      "2025-06-02 12:15:26,495\tINFO compiled_dag_node.py:2273 -- Waiting for worker tasks to exit\n",
      "2025-06-02 12:15:26,495\tINFO compiled_dag_node.py:2234 -- Killing actor: Actor(TorchTensorWorker, 3fd3e47c127c6e5ed93afb4301000000)\n",
      "2025-06-02 12:15:26,495\tINFO compiled_dag_node.py:2234 -- Killing actor: Actor(TorchTensorWorker, 54873108da198974aa0193e901000000)\n",
      "2025-06-02 12:15:26,496\tINFO compiled_dag_node.py:2276 -- Teardown complete\n",
      "\n",
      "test_torch_tensor_dag.py::test_torch_tensor_auto[num_gpus1-ray_start_regular0] 2025-06-02 12:15:30,359\tINFO worker.py:1929 -- Started a local Ray instance.\n",
      "2025-06-02 12:15:32,468\tINFO dag_node_operation.py:592 -- Writing compiled graph schedule visualization to compiled_graph_schedule.png\n",
      "2025-06-02 12:15:32,975\tINFO compiled_dag_node.py:2246 -- Tearing down compiled DAG\n",
      "2025-06-02 12:15:32,976\tINFO compiled_dag_node.py:2251 -- Cancelling compiled worker on actor: Actor(TorchTensorWorker, 6047e60e4a2e299088acb2b801000000)\n",
      "2025-06-02 12:15:32,976\tINFO compiled_dag_node.py:2251 -- Cancelling compiled worker on actor: Actor(TorchTensorWorker, ae77d8fcc327eb77584f422301000000)\n",
      "2025-06-02 12:15:32,982\tINFO compiled_dag_node.py:2273 -- Waiting for worker tasks to exit\n",
      "2025-06-02 12:15:32,982\tINFO compiled_dag_node.py:2276 -- Teardown complete\n",
      "2025-06-02 12:15:33,066\tINFO dag_node_operation.py:592 -- Writing compiled graph schedule visualization to compiled_graph_schedule.png\n",
      "\u001b[32mPASSED\u001b[0m2025-06-02 12:15:33,190\tINFO compiled_dag_node.py:2246 -- Tearing down compiled DAG\n",
      "2025-06-02 12:15:33,191\tINFO compiled_dag_node.py:2251 -- Cancelling compiled worker on actor: Actor(TorchTensorWorker, 6047e60e4a2e299088acb2b801000000)\n",
      "2025-06-02 12:15:33,191\tINFO compiled_dag_node.py:2251 -- Cancelling compiled worker on actor: Actor(TorchTensorWorker, ae77d8fcc327eb77584f422301000000)\n",
      "2025-06-02 12:15:33,198\tINFO compiled_dag_node.py:2273 -- Waiting for worker tasks to exit\n",
      "2025-06-02 12:15:33,198\tINFO compiled_dag_node.py:2234 -- Killing actor: Actor(TorchTensorWorker, 6047e60e4a2e299088acb2b801000000)\n",
      "2025-06-02 12:15:33,199\tINFO compiled_dag_node.py:2234 -- Killing actor: Actor(TorchTensorWorker, ae77d8fcc327eb77584f422301000000)\n",
      "2025-06-02 12:15:33,199\tINFO compiled_dag_node.py:2276 -- Teardown complete\n",
      "\n",
      "test_torch_tensor_dag.py::test_torch_tensor_auto[num_gpus2-ray_start_regular0] 2025-06-02 12:15:36,253\tINFO worker.py:1929 -- Started a local Ray instance.\n",
      "2025-06-02 12:15:38,356\tINFO dag_node_operation.py:592 -- Writing compiled graph schedule visualization to compiled_graph_schedule.png\n",
      "2025-06-02 12:15:38,646\tINFO compiled_dag_node.py:2246 -- Tearing down compiled DAG\n",
      "2025-06-02 12:15:38,647\tINFO compiled_dag_node.py:2251 -- Cancelling compiled worker on actor: Actor(TorchTensorWorker, 3ba087ffed8142ff2760fe6e01000000)\n",
      "2025-06-02 12:15:38,647\tINFO compiled_dag_node.py:2251 -- Cancelling compiled worker on actor: Actor(TorchTensorWorker, a084a0260ef8ca7653d017d101000000)\n",
      "2025-06-02 12:15:38,653\tINFO compiled_dag_node.py:2273 -- Waiting for worker tasks to exit\n",
      "2025-06-02 12:15:38,653\tINFO compiled_dag_node.py:2276 -- Teardown complete\n",
      "2025-06-02 12:15:38,754\tINFO dag_node_operation.py:592 -- Writing compiled graph schedule visualization to compiled_graph_schedule.png\n",
      "\u001b[32mPASSED\u001b[0m2025-06-02 12:15:38,881\tINFO compiled_dag_node.py:2246 -- Tearing down compiled DAG\n",
      "2025-06-02 12:15:38,882\tINFO compiled_dag_node.py:2251 -- Cancelling compiled worker on actor: Actor(TorchTensorWorker, 3ba087ffed8142ff2760fe6e01000000)\n",
      "2025-06-02 12:15:38,882\tINFO compiled_dag_node.py:2251 -- Cancelling compiled worker on actor: Actor(TorchTensorWorker, a084a0260ef8ca7653d017d101000000)\n",
      "2025-06-02 12:15:38,888\tINFO compiled_dag_node.py:2273 -- Waiting for worker tasks to exit\n",
      "2025-06-02 12:15:38,888\tINFO compiled_dag_node.py:2234 -- Killing actor: Actor(TorchTensorWorker, 3ba087ffed8142ff2760fe6e01000000)\n",
      "2025-06-02 12:15:38,889\tINFO compiled_dag_node.py:2234 -- Killing actor: Actor(TorchTensorWorker, a084a0260ef8ca7653d017d101000000)\n",
      "2025-06-02 12:15:38,889\tINFO compiled_dag_node.py:2276 -- Teardown complete\n",
      "\n",
      "test_torch_tensor_dag.py::test_torch_tensor_auto[num_gpus3-ray_start_regular0] 2025-06-02 12:15:42,780\tINFO worker.py:1929 -- Started a local Ray instance.\n",
      "2025-06-02 12:15:45,012\tINFO torch_tensor_nccl_channel.py:772 -- Creating NCCL group 66fcc290-cdc8-4fad-be04-8da71e9aab38 on actors: [Actor(TorchTensorWorker, 3b800d0230e21b3160fd314b01000000), Actor(TorchTensorWorker, 803b764c1767ea7e37322d5401000000)]\n",
      "2025-06-02 12:15:46,094\tINFO torch_tensor_nccl_channel.py:797 -- NCCL group initialized.\n",
      "2025-06-02 12:15:46,255\tINFO dag_node_operation.py:592 -- Writing compiled graph schedule visualization to compiled_graph_schedule.png\n",
      "2025-06-02 12:15:46,440\tINFO compiled_dag_node.py:2246 -- Tearing down compiled DAG\n",
      "2025-06-02 12:15:46,441\tINFO compiled_dag_node.py:2251 -- Cancelling compiled worker on actor: Actor(TorchTensorWorker, 3b800d0230e21b3160fd314b01000000)\n",
      "2025-06-02 12:15:46,441\tINFO compiled_dag_node.py:2251 -- Cancelling compiled worker on actor: Actor(TorchTensorWorker, 803b764c1767ea7e37322d5401000000)\n",
      "\u001b[36m(TorchTensorWorker pid=704639)\u001b[0m Destructing NCCL group on actor: Actor(TorchTensorWorker, 3b800d0230e21b3160fd314b01000000)\n",
      "\u001b[36m(TorchTensorWorker pid=704637)\u001b[0m Destructing NCCL group on actor: Actor(TorchTensorWorker, 803b764c1767ea7e37322d5401000000)\n",
      "2025-06-02 12:15:46,955\tINFO compiled_dag_node.py:2273 -- Waiting for worker tasks to exit\n",
      "2025-06-02 12:15:46,956\tINFO compiled_dag_node.py:2276 -- Teardown complete\n",
      "2025-06-02 12:15:46,972\tINFO torch_tensor_nccl_channel.py:772 -- Creating NCCL group 8cf15d84-9ca2-4322-bf5b-bfa96064620f on actors: [Actor(TorchTensorWorker, 3b800d0230e21b3160fd314b01000000), Actor(TorchTensorWorker, 803b764c1767ea7e37322d5401000000)]\n",
      "2025-06-02 12:15:47,004\tINFO torch_tensor_nccl_channel.py:797 -- NCCL group initialized.\n",
      "2025-06-02 12:15:47,124\tINFO dag_node_operation.py:592 -- Writing compiled graph schedule visualization to compiled_graph_schedule.png\n",
      "\u001b[32mPASSED\u001b[0m2025-06-02 12:15:47,267\tINFO compiled_dag_node.py:2246 -- Tearing down compiled DAG\n",
      "2025-06-02 12:15:47,267\tINFO compiled_dag_node.py:2251 -- Cancelling compiled worker on actor: Actor(TorchTensorWorker, 3b800d0230e21b3160fd314b01000000)\n",
      "2025-06-02 12:15:47,268\tINFO compiled_dag_node.py:2251 -- Cancelling compiled worker on actor: Actor(TorchTensorWorker, 803b764c1767ea7e37322d5401000000)\n",
      "\u001b[36m(TorchTensorWorker pid=704639)\u001b[0m Destructing NCCL group on actor: Actor(TorchTensorWorker, 3b800d0230e21b3160fd314b01000000)\n",
      "\u001b[36m(TorchTensorWorker pid=704637)\u001b[0m Destructing NCCL group on actor: Actor(TorchTensorWorker, 803b764c1767ea7e37322d5401000000)\n",
      "2025-06-02 12:15:47,770\tINFO compiled_dag_node.py:2273 -- Waiting for worker tasks to exit\n",
      "2025-06-02 12:15:47,770\tINFO compiled_dag_node.py:2234 -- Killing actor: Actor(TorchTensorWorker, 3b800d0230e21b3160fd314b01000000)\n",
      "2025-06-02 12:15:47,770\tINFO compiled_dag_node.py:2234 -- Killing actor: Actor(TorchTensorWorker, 803b764c1767ea7e37322d5401000000)\n",
      "2025-06-02 12:15:47,771\tINFO compiled_dag_node.py:2276 -- Teardown complete\n",
      "\n",
      "test_torch_tensor_dag.py::test_torch_tensor_auto[num_gpus4-ray_start_regular0] 2025-06-02 12:15:50,879\tINFO worker.py:1929 -- Started a local Ray instance.\n",
      "2025-06-02 12:15:52,933\tINFO dag_node_operation.py:592 -- Writing compiled graph schedule visualization to compiled_graph_schedule.png\n",
      "2025-06-02 12:15:53,695\tINFO compiled_dag_node.py:2246 -- Tearing down compiled DAG\n",
      "2025-06-02 12:15:53,696\tINFO compiled_dag_node.py:2251 -- Cancelling compiled worker on actor: Actor(TorchTensorWorker, a1f83f70b30846608a71d05c01000000)\n",
      "2025-06-02 12:15:53,696\tINFO compiled_dag_node.py:2251 -- Cancelling compiled worker on actor: Actor(TorchTensorWorker, a7010b6c32393fa025f35fdb01000000)\n",
      "2025-06-02 12:15:53,702\tINFO compiled_dag_node.py:2273 -- Waiting for worker tasks to exit\n",
      "2025-06-02 12:15:53,702\tINFO compiled_dag_node.py:2276 -- Teardown complete\n",
      "2025-06-02 12:15:53,787\tINFO dag_node_operation.py:592 -- Writing compiled graph schedule visualization to compiled_graph_schedule.png\n",
      "\u001b[32mPASSED\u001b[0m2025-06-02 12:15:53,909\tINFO compiled_dag_node.py:2246 -- Tearing down compiled DAG\n",
      "2025-06-02 12:15:53,909\tINFO compiled_dag_node.py:2251 -- Cancelling compiled worker on actor: Actor(TorchTensorWorker, a1f83f70b30846608a71d05c01000000)\n",
      "2025-06-02 12:15:53,909\tINFO compiled_dag_node.py:2251 -- Cancelling compiled worker on actor: Actor(TorchTensorWorker, a7010b6c32393fa025f35fdb01000000)\n",
      "2025-06-02 12:15:53,914\tINFO compiled_dag_node.py:2273 -- Waiting for worker tasks to exit\n",
      "2025-06-02 12:15:53,914\tINFO compiled_dag_node.py:2234 -- Killing actor: Actor(TorchTensorWorker, a1f83f70b30846608a71d05c01000000)\n",
      "2025-06-02 12:15:53,915\tINFO compiled_dag_node.py:2234 -- Killing actor: Actor(TorchTensorWorker, a7010b6c32393fa025f35fdb01000000)\n",
      "2025-06-02 12:15:53,915\tINFO compiled_dag_node.py:2276 -- Teardown complete\n",
      "\n",
      "test_torch_tensor_dag.py::test_torch_tensor_nccl_overlap_timed[ray_start_regular0-False] 2025-06-02 12:15:56,694\tINFO worker.py:1929 -- Started a local Ray instance.\n",
      "\u001b[31mFAILED\u001b[0m\n",
      "test_torch_tensor_dag.py::test_torch_tensor_nccl_overlap_timed[ray_start_regular1-True] 2025-06-02 12:16:00,687\tINFO worker.py:1929 -- Started a local Ray instance.\n",
      "\u001b[31mFAILED\u001b[0m\n",
      "test_torch_tensor_dag.py::test_torch_tensor_nccl_disallows_driver 2025-06-02 12:16:03,809\tINFO worker.py:1929 -- Started a local Ray instance.\n",
      "[2025-06-02 12:16:04,256 E 696941 707255] core_worker.cc:2728: Failed to register actor. Error message: SchedulingCancelled: Actor creation cancelled. actor_id=2bbd89a19510dac25c2b5d8901000000\n",
      "\u001b[32mPASSED\u001b[0m\n",
      "test_torch_tensor_dag.py::test_torch_tensor_custom_comm[ray_start_regular0] 2025-06-02 12:16:06,911\tINFO worker.py:1929 -- Started a local Ray instance.\n",
      "2025-06-02 12:16:09,164\tINFO torch_tensor_nccl_channel.py:770 -- Initializing custom NCCL group 2b76d256-f972-4bba-9b45-0dcbeb086c87 on actors: [Actor(TorchTensorWorker, 981d2f5c8cc37593072247df01000000), Actor(TorchTensorWorker, f445492050be21cf2ad5258901000000)]\n",
      "2025-06-02 12:16:10,056\tINFO torch_tensor_nccl_channel.py:797 -- NCCL group initialized.\n",
      "2025-06-02 12:16:10,244\tINFO dag_node_operation.py:592 -- Writing compiled graph schedule visualization to compiled_graph_schedule.png\n",
      "\u001b[32mPASSED\u001b[0m2025-06-02 12:16:10,477\tINFO compiled_dag_node.py:2246 -- Tearing down compiled DAG\n",
      "2025-06-02 12:16:10,478\tINFO compiled_dag_node.py:2251 -- Cancelling compiled worker on actor: Actor(TorchTensorWorker, 981d2f5c8cc37593072247df01000000)\n",
      "2025-06-02 12:16:10,478\tINFO compiled_dag_node.py:2251 -- Cancelling compiled worker on actor: Actor(TorchTensorWorker, f445492050be21cf2ad5258901000000)\n",
      "\u001b[36m(TorchTensorWorker pid=707693)\u001b[0m Destructing NCCL group on actor: Actor(TorchTensorWorker, f445492050be21cf2ad5258901000000)\n",
      "\u001b[36m(TorchTensorWorker pid=707694)\u001b[0m Destructing NCCL group on actor: Actor(TorchTensorWorker, 981d2f5c8cc37593072247df01000000)\n",
      "2025-06-02 12:16:11,001\tINFO compiled_dag_node.py:2273 -- Waiting for worker tasks to exit\n",
      "2025-06-02 12:16:11,001\tINFO compiled_dag_node.py:2234 -- Killing actor: Actor(TorchTensorWorker, 981d2f5c8cc37593072247df01000000)\n",
      "2025-06-02 12:16:11,001\tINFO compiled_dag_node.py:2234 -- Killing actor: Actor(TorchTensorWorker, f445492050be21cf2ad5258901000000)\n",
      "2025-06-02 12:16:11,002\tINFO compiled_dag_node.py:2276 -- Teardown complete\n",
      "\n",
      "test_torch_tensor_dag.py::test_torch_tensor_custom_comm_inited[ray_start_regular0] 2025-06-02 12:16:14,002\tINFO worker.py:1929 -- Started a local Ray instance.\n",
      "\u001b[33mSKIPPED\u001b[0m\n",
      "test_torch_tensor_dag.py::test_torch_tensor_default_comm[transports0-ray_start_regular0] 2025-06-02 12:16:18,119\tINFO worker.py:1929 -- Started a local Ray instance.\n",
      "\u001b[31mFAILED\u001b[0m\n",
      "test_torch_tensor_dag.py::test_torch_tensor_default_comm[transports1-ray_start_regular0] 2025-06-02 12:16:21,323\tINFO worker.py:1929 -- Started a local Ray instance.\n",
      "\u001b[31mFAILED\u001b[0m\n",
      "test_torch_tensor_dag.py::test_torch_tensor_default_comm[transports2-ray_start_regular0] 2025-06-02 12:16:25,335\tINFO worker.py:1929 -- Started a local Ray instance.\n",
      "\u001b[31mFAILED\u001b[0m\n",
      "test_torch_tensor_dag.py::test_torch_tensor_default_comm[transports3-ray_start_regular0] 2025-06-02 12:16:28,317\tINFO worker.py:1929 -- Started a local Ray instance.\n",
      "\u001b[31mFAILED\u001b[0m\n",
      "test_torch_tensor_dag.py::test_torch_tensor_invalid_custom_comm[ray_start_regular0] 2025-06-02 12:16:32,338\tINFO worker.py:1929 -- Started a local Ray instance.\n",
      "\u001b[33mSKIPPED\u001b[0m\n",
      "test_torch_tensor_dag.py::test_torch_tensor_nccl_static_shape[ray_start_regular0] 2025-06-02 12:16:35,333\tINFO worker.py:1929 -- Started a local Ray instance.\n",
      "2025-06-02 12:16:37,592\tINFO torch_tensor_nccl_channel.py:772 -- Creating NCCL group 692f1d0c-82a4-40dd-84c0-89257e2a2dbb on actors: [Actor(TorchTensorWorker, cb2a394bf8f6ee55fa2d528401000000), Actor(TorchTensorWorker, b5a09278f8a134ca71cefbfb01000000)]\n",
      "2025-06-02 12:16:38,680\tINFO torch_tensor_nccl_channel.py:797 -- NCCL group initialized.\n",
      "2025-06-02 12:16:38,836\tINFO dag_node_operation.py:592 -- Writing compiled graph schedule visualization to compiled_graph_schedule.png\n",
      "2025-06-02 12:16:39,038\tINFO compiled_dag_node.py:2246 -- Tearing down compiled DAG\n",
      "\u001b[32mPASSED\u001b[0m2025-06-02 12:16:39,039\tINFO compiled_dag_node.py:2251 -- Cancelling compiled worker on actor: Actor(TorchTensorWorker, cb2a394bf8f6ee55fa2d528401000000)\n",
      "2025-06-02 12:16:39,040\tINFO compiled_dag_node.py:2251 -- Cancelling compiled worker on actor: Actor(TorchTensorWorker, b5a09278f8a134ca71cefbfb01000000)\n",
      "\u001b[36m(TorchTensorWorker pid=712051)\u001b[0m ERROR:root:Compiled DAG task exited with exception\n",
      "\u001b[36m(TorchTensorWorker pid=712051)\u001b[0m Traceback (most recent call last):\n",
      "\u001b[36m(TorchTensorWorker pid=712051)\u001b[0m   File \"/home/wxdeng/stbr-min/python/ray/dag/compiled_dag_node.py\", line 242, in do_exec_tasks\n",
      "\u001b[36m(TorchTensorWorker pid=712051)\u001b[0m     done = tasks[operation.exec_task_idx].exec_operation(\n",
      "\u001b[36m(TorchTensorWorker pid=712051)\u001b[0m         self, overlap_gpu_communication\n",
      "\u001b[36m(TorchTensorWorker pid=712051)\u001b[0m     )\n",
      "\u001b[36m(TorchTensorWorker pid=712051)\u001b[0m   File \"/home/wxdeng/stbr-min/python/ray/dag/compiled_dag_node.py\", line 673, in exec_operation\n",
      "\u001b[36m(TorchTensorWorker pid=712051)\u001b[0m     return self.exec_operation_inner(\n",
      "\u001b[36m(TorchTensorWorker pid=712051)\u001b[0m            ~~~~~~~~~~~~~~~~~~~~~~~~~^\n",
      "\u001b[36m(TorchTensorWorker pid=712051)\u001b[0m         class_handle, overlap_gpu_communication\n",
      "\u001b[36m(TorchTensorWorker pid=712051)\u001b[0m         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[36m(TorchTensorWorker pid=712051)\u001b[0m     )\n",
      "\u001b[36m(TorchTensorWorker pid=712051)\u001b[0m     ^\n",
      "\u001b[36m(TorchTensorWorker pid=712051)\u001b[0m   File \"/home/wxdeng/stbr-min/python/ray/dag/compiled_dag_node.py\", line 736, in exec_operation_inner\n",
      "\u001b[36m(TorchTensorWorker pid=712051)\u001b[0m     raise exc\n",
      "\u001b[36m(TorchTensorWorker pid=712051)\u001b[0m   File \"/home/wxdeng/stbr-min/python/ray/dag/compiled_dag_node.py\", line 731, in exec_operation_inner\n",
      "\u001b[36m(TorchTensorWorker pid=712051)\u001b[0m     output_val = method(*input_values, **self.resolved_kwargs)\n",
      "\u001b[36m(TorchTensorWorker pid=712051)\u001b[0m   File \"/home/wxdeng/stbr-min/python/ray/dag/communication_node.py\", line 72, in execute\n",
      "\u001b[36m(TorchTensorWorker pid=712051)\u001b[0m     self.nccl_ch.write(data)\n",
      "\u001b[36m(TorchTensorWorker pid=712051)\u001b[0m     ~~~~~~~~~~~~~~~~~~^^^^^^\n",
      "\u001b[36m(TorchTensorWorker pid=712051)\u001b[0m   File \"/home/wxdeng/stbr-min/python/ray/experimental/channel/torch_tensor_nccl_channel.py\", line 270, in write\n",
      "\u001b[36m(TorchTensorWorker pid=712051)\u001b[0m     self._send_cpu_and_gpu_data(value, timeout)\n",
      "\u001b[36m(TorchTensorWorker pid=712051)\u001b[0m     ~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^\n",
      "\u001b[36m(TorchTensorWorker pid=712051)\u001b[0m   File \"/home/wxdeng/stbr-min/python/ray/experimental/channel/torch_tensor_nccl_channel.py\", line 203, in _send_cpu_and_gpu_data\n",
      "\u001b[36m(TorchTensorWorker pid=712051)\u001b[0m     self._gpu_data_channel.write(gpu_tensors)\n",
      "\u001b[36m(TorchTensorWorker pid=712051)\u001b[0m     ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^\n",
      "\u001b[36m(TorchTensorWorker pid=712051)\u001b[0m   File \"/home/wxdeng/stbr-min/python/ray/experimental/channel/torch_tensor_nccl_channel.py\", line 559, in write\n",
      "\u001b[36m(TorchTensorWorker pid=712051)\u001b[0m     metadata = self._get_send_tensors_metadata(tensors)\n",
      "\u001b[36m(TorchTensorWorker pid=712051)\u001b[0m   File \"/home/wxdeng/stbr-min/python/ray/experimental/channel/torch_tensor_nccl_channel.py\", line 514, in _get_send_tensors_metadata\n",
      "\u001b[36m(TorchTensorWorker pid=712051)\u001b[0m     raise ValueError(\n",
      "\u001b[36m(TorchTensorWorker pid=712051)\u001b[0m     ...<4 lines>...\n",
      "\u001b[36m(TorchTensorWorker pid=712051)\u001b[0m     )\n",
      "\u001b[36m(TorchTensorWorker pid=712051)\u001b[0m ValueError: Expected torch.Tensors with shapes and dtypes: [(shape=torch.Size([10]), dtype=torch.float16)], found: [(shape=torch.Size([20]), dtype=torch.float16)]. DAG will shut down.\n",
      "\u001b[36m(TorchTensorWorker pid=712051)\u001b[0m Destructing NCCL group on actor: Actor(TorchTensorWorker, cb2a394bf8f6ee55fa2d528401000000)\n",
      "\u001b[36m(TorchTensorWorker pid=712052)\u001b[0m Destructing NCCL group on actor: Actor(TorchTensorWorker, b5a09278f8a134ca71cefbfb01000000)\n",
      "2025-06-02 12:16:39,525\tINFO compiled_dag_node.py:2273 -- Waiting for worker tasks to exit\n",
      "2025-06-02 12:16:39,526\tINFO compiled_dag_node.py:2276 -- Teardown complete\n",
      "\n",
      "test_torch_tensor_dag.py::test_torch_tensor_nccl_direct_return[ray_start_regular0] 2025-06-02 12:16:42,335\tINFO worker.py:1929 -- Started a local Ray instance.\n",
      "2025-06-02 12:16:44,586\tINFO torch_tensor_nccl_channel.py:772 -- Creating NCCL group 2bd5ff33-9179-4375-a32f-c2ff8c78d8bf on actors: [Actor(TorchTensorWorker, 3ed3f77e9c5d203c1dabcc0d01000000), Actor(TorchTensorWorker, 8f3707107d59de34a3ee660601000000)]\n",
      "2025-06-02 12:16:45,678\tINFO torch_tensor_nccl_channel.py:797 -- NCCL group initialized.\n",
      "2025-06-02 12:16:45,871\tINFO dag_node_operation.py:592 -- Writing compiled graph schedule visualization to compiled_graph_schedule.png\n",
      "2025-06-02 12:16:46,101\tINFO compiled_dag_node.py:2246 -- Tearing down compiled DAG\n",
      "\u001b[32mPASSED\u001b[0m2025-06-02 12:16:46,103\tINFO compiled_dag_node.py:2251 -- Cancelling compiled worker on actor: Actor(TorchTensorWorker, 3ed3f77e9c5d203c1dabcc0d01000000)\n",
      "2025-06-02 12:16:46,103\tINFO compiled_dag_node.py:2251 -- Cancelling compiled worker on actor: Actor(TorchTensorWorker, 8f3707107d59de34a3ee660601000000)\n",
      "\u001b[36m(TorchTensorWorker pid=712805)\u001b[0m ERROR:root:Compiled DAG task exited with exception\n",
      "\u001b[36m(TorchTensorWorker pid=712805)\u001b[0m Traceback (most recent call last):\n",
      "\u001b[36m(TorchTensorWorker pid=712805)\u001b[0m   File \"/home/wxdeng/stbr-min/python/ray/dag/compiled_dag_node.py\", line 242, in do_exec_tasks\n",
      "\u001b[36m(TorchTensorWorker pid=712805)\u001b[0m     done = tasks[operation.exec_task_idx].exec_operation(\n",
      "\u001b[36m(TorchTensorWorker pid=712805)\u001b[0m         self, overlap_gpu_communication\n",
      "\u001b[36m(TorchTensorWorker pid=712805)\u001b[0m     )\n",
      "\u001b[36m(TorchTensorWorker pid=712805)\u001b[0m   File \"/home/wxdeng/stbr-min/python/ray/dag/compiled_dag_node.py\", line 673, in exec_operation\n",
      "\u001b[36m(TorchTensorWorker pid=712805)\u001b[0m     return self.exec_operation_inner(\n",
      "\u001b[36m(TorchTensorWorker pid=712805)\u001b[0m            ~~~~~~~~~~~~~~~~~~~~~~~~~^\n",
      "\u001b[36m(TorchTensorWorker pid=712805)\u001b[0m         class_handle, overlap_gpu_communication\n",
      "\u001b[36m(TorchTensorWorker pid=712805)\u001b[0m         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[36m(TorchTensorWorker pid=712805)\u001b[0m     )\n",
      "\u001b[36m(TorchTensorWorker pid=712805)\u001b[0m     ^\n",
      "\u001b[36m(TorchTensorWorker pid=712805)\u001b[0m   File \"/home/wxdeng/stbr-min/python/ray/dag/compiled_dag_node.py\", line 736, in exec_operation_inner\n",
      "\u001b[36m(TorchTensorWorker pid=712805)\u001b[0m     raise exc\n",
      "\u001b[36m(TorchTensorWorker pid=712805)\u001b[0m   File \"/home/wxdeng/stbr-min/python/ray/dag/compiled_dag_node.py\", line 731, in exec_operation_inner\n",
      "\u001b[36m(TorchTensorWorker pid=712805)\u001b[0m     output_val = method(*input_values, **self.resolved_kwargs)\n",
      "\u001b[36m(TorchTensorWorker pid=712805)\u001b[0m   File \"/home/wxdeng/stbr-min/python/ray/dag/communication_node.py\", line 72, in execute\n",
      "\u001b[36m(TorchTensorWorker pid=712805)\u001b[0m     self.nccl_ch.write(data)\n",
      "\u001b[36m(TorchTensorWorker pid=712805)\u001b[0m     ~~~~~~~~~~~~~~~~~~^^^^^^\n",
      "\u001b[36m(TorchTensorWorker pid=712805)\u001b[0m   File \"/home/wxdeng/stbr-min/python/ray/experimental/channel/torch_tensor_nccl_channel.py\", line 257, in write\n",
      "\u001b[36m(TorchTensorWorker pid=712805)\u001b[0m     raise ValueError(\n",
      "\u001b[36m(TorchTensorWorker pid=712805)\u001b[0m     ...<3 lines>...\n",
      "\u001b[36m(TorchTensorWorker pid=712805)\u001b[0m     )\n",
      "\u001b[36m(TorchTensorWorker pid=712805)\u001b[0m ValueError: Task annotated with _direct_return=True must return a CUDA torch.Tensor, instead found value `1`. DAG will shut down.\n",
      "\u001b[36m(TorchTensorWorker pid=712805)\u001b[0m Destructing NCCL group on actor: Actor(TorchTensorWorker, 3ed3f77e9c5d203c1dabcc0d01000000)\n",
      "\u001b[36m(TorchTensorWorker pid=712806)\u001b[0m Destructing NCCL group on actor: Actor(TorchTensorWorker, 8f3707107d59de34a3ee660601000000)\n",
      "2025-06-02 12:16:46,622\tINFO compiled_dag_node.py:2273 -- Waiting for worker tasks to exit\n",
      "2025-06-02 12:16:46,623\tINFO compiled_dag_node.py:2276 -- Teardown complete\n",
      "\n",
      "test_torch_tensor_dag.py::test_torch_tensor_nccl_nested_dynamic[ray_start_regular0] 2025-06-02 12:16:49,520\tINFO worker.py:1929 -- Started a local Ray instance.\n",
      "2025-06-02 12:16:51,709\tINFO torch_tensor_nccl_channel.py:772 -- Creating NCCL group e7d6b5b4-ecbc-41d0-97b6-ac2960025f27 on actors: [Actor(TorchTensorWorker, 27ab7f35dbbf1840958c122a01000000), Actor(TorchTensorWorker, 18c6a7ad54899b83d179575c01000000)]\n",
      "2025-06-02 12:16:52,802\tINFO torch_tensor_nccl_channel.py:797 -- NCCL group initialized.\n",
      "2025-06-02 12:16:52,896\tINFO dag_node_operation.py:592 -- Writing compiled graph schedule visualization to compiled_graph_schedule.png\n",
      "\u001b[32mPASSED\u001b[0m2025-06-02 12:16:53,030\tINFO compiled_dag_node.py:2246 -- Tearing down compiled DAG\n",
      "2025-06-02 12:16:53,031\tINFO compiled_dag_node.py:2251 -- Cancelling compiled worker on actor: Actor(TorchTensorWorker, 18c6a7ad54899b83d179575c01000000)\n",
      "2025-06-02 12:16:53,031\tINFO compiled_dag_node.py:2251 -- Cancelling compiled worker on actor: Actor(TorchTensorWorker, 27ab7f35dbbf1840958c122a01000000)\n",
      "\u001b[36m(TorchTensorWorker pid=713515)\u001b[0m Destructing NCCL group on actor: Actor(TorchTensorWorker, 18c6a7ad54899b83d179575c01000000)\n",
      "\u001b[36m(TorchTensorWorker pid=713516)\u001b[0m Destructing NCCL group on actor: Actor(TorchTensorWorker, 27ab7f35dbbf1840958c122a01000000)\n",
      "2025-06-02 12:16:53,808\tINFO compiled_dag_node.py:2273 -- Waiting for worker tasks to exit\n",
      "2025-06-02 12:16:53,808\tINFO compiled_dag_node.py:2234 -- Killing actor: Actor(TorchTensorWorker, 18c6a7ad54899b83d179575c01000000)\n",
      "2025-06-02 12:16:53,809\tINFO compiled_dag_node.py:2234 -- Killing actor: Actor(TorchTensorWorker, 27ab7f35dbbf1840958c122a01000000)\n",
      "2025-06-02 12:16:53,809\tINFO compiled_dag_node.py:2276 -- Teardown complete\n",
      "\n",
      "test_torch_tensor_dag.py::test_torch_tensor_exceptions[False-False-False-ray_start_regular0] 2025-06-02 12:16:56,741\tINFO worker.py:1929 -- Started a local Ray instance.\n",
      "2025-06-02 12:16:58,972\tINFO torch_tensor_nccl_channel.py:772 -- Creating NCCL group ce03a8b0-d8b5-43d4-a82f-151e5ff4e085 on actors: [Actor(TorchTensorWorker, 337ae0ec373478272d655dbb01000000), Actor(TorchTensorWorker, 8c4c6e670396de06e296305801000000)]\n",
      "2025-06-02 12:17:00,069\tINFO torch_tensor_nccl_channel.py:797 -- NCCL group initialized.\n",
      "2025-06-02 12:17:00,264\tINFO dag_node_operation.py:592 -- Writing compiled graph schedule visualization to compiled_graph_schedule.png\n",
      "\u001b[32mPASSED\u001b[0m2025-06-02 12:17:00,462\tINFO compiled_dag_node.py:2246 -- Tearing down compiled DAG\n",
      "2025-06-02 12:17:00,463\tINFO compiled_dag_node.py:2251 -- Cancelling compiled worker on actor: Actor(TorchTensorWorker, 8c4c6e670396de06e296305801000000)\n",
      "2025-06-02 12:17:00,463\tINFO compiled_dag_node.py:2251 -- Cancelling compiled worker on actor: Actor(TorchTensorWorker, 337ae0ec373478272d655dbb01000000)\n",
      "\u001b[36m(TorchTensorWorker pid=714192)\u001b[0m Destructing NCCL group on actor: Actor(TorchTensorWorker, 8c4c6e670396de06e296305801000000)\n",
      "\u001b[36m(TorchTensorWorker pid=714194)\u001b[0m Destructing NCCL group on actor: Actor(TorchTensorWorker, 337ae0ec373478272d655dbb01000000)\n",
      "2025-06-02 12:17:00,982\tINFO compiled_dag_node.py:2273 -- Waiting for worker tasks to exit\n",
      "2025-06-02 12:17:00,982\tINFO compiled_dag_node.py:2234 -- Killing actor: Actor(TorchTensorWorker, 8c4c6e670396de06e296305801000000)\n",
      "2025-06-02 12:17:00,982\tINFO compiled_dag_node.py:2234 -- Killing actor: Actor(TorchTensorWorker, 337ae0ec373478272d655dbb01000000)\n",
      "2025-06-02 12:17:00,982\tINFO compiled_dag_node.py:2276 -- Teardown complete\n",
      "\n",
      "test_torch_tensor_dag.py::test_torch_tensor_exceptions[False-False-True-ray_start_regular0] 2025-06-02 12:17:03,969\tINFO worker.py:1929 -- Started a local Ray instance.\n",
      "2025-06-02 12:17:06,175\tINFO torch_tensor_nccl_channel.py:772 -- Creating NCCL group f9452879-b75e-435d-8b92-e86f68dea5ba on actors: [Actor(TorchTensorWorker, c3518eb0a7edc902f77b231801000000), Actor(TorchTensorWorker, 7fdb9e26840c308a33aa785501000000)]\n",
      "2025-06-02 12:17:07,268\tINFO torch_tensor_nccl_channel.py:797 -- NCCL group initialized.\n",
      "2025-06-02 12:17:07,464\tINFO dag_node_operation.py:592 -- Writing compiled graph schedule visualization to compiled_graph_schedule.png\n",
      "2025-06-02 12:17:07,671\tINFO compiled_dag_node.py:2246 -- Tearing down compiled DAG\n",
      "\u001b[32mPASSED\u001b[0m2025-06-02 12:17:07,673\tINFO compiled_dag_node.py:2251 -- Cancelling compiled worker on actor: Actor(TorchTensorWorker, 7fdb9e26840c308a33aa785501000000)\n",
      "2025-06-02 12:17:07,673\tINFO compiled_dag_node.py:2251 -- Cancelling compiled worker on actor: Actor(TorchTensorWorker, c3518eb0a7edc902f77b231801000000)\n",
      "\u001b[36m(TorchTensorWorker pid=714904)\u001b[0m ERROR:root:Compiled DAG task exited with exception\n",
      "\u001b[36m(TorchTensorWorker pid=714904)\u001b[0m Traceback (most recent call last):\n",
      "\u001b[36m(TorchTensorWorker pid=714904)\u001b[0m   File \"/home/wxdeng/stbr-min/python/ray/dag/compiled_dag_node.py\", line 242, in do_exec_tasks\n",
      "\u001b[36m(TorchTensorWorker pid=714904)\u001b[0m     done = tasks[operation.exec_task_idx].exec_operation(\n",
      "\u001b[36m(TorchTensorWorker pid=714904)\u001b[0m         self, overlap_gpu_communication\n",
      "\u001b[36m(TorchTensorWorker pid=714904)\u001b[0m     )\n",
      "\u001b[36m(TorchTensorWorker pid=714904)\u001b[0m   File \"/home/wxdeng/stbr-min/python/ray/dag/compiled_dag_node.py\", line 673, in exec_operation\n",
      "\u001b[36m(TorchTensorWorker pid=714904)\u001b[0m     return self.exec_operation_inner(\n",
      "\u001b[36m(TorchTensorWorker pid=714904)\u001b[0m            ~~~~~~~~~~~~~~~~~~~~~~~~~^\n",
      "\u001b[36m(TorchTensorWorker pid=714904)\u001b[0m         class_handle, overlap_gpu_communication\n",
      "\u001b[36m(TorchTensorWorker pid=714904)\u001b[0m         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[36m(TorchTensorWorker pid=714904)\u001b[0m     )\n",
      "\u001b[36m(TorchTensorWorker pid=714904)\u001b[0m     ^\n",
      "\u001b[36m(TorchTensorWorker pid=714904)\u001b[0m   File \"/home/wxdeng/stbr-min/python/ray/dag/compiled_dag_node.py\", line 736, in exec_operation_inner\n",
      "\u001b[36m(TorchTensorWorker pid=714904)\u001b[0m     raise exc\n",
      "\u001b[36m(TorchTensorWorker pid=714904)\u001b[0m   File \"/home/wxdeng/stbr-min/python/ray/dag/compiled_dag_node.py\", line 731, in exec_operation_inner\n",
      "\u001b[36m(TorchTensorWorker pid=714904)\u001b[0m     output_val = method(*input_values, **self.resolved_kwargs)\n",
      "\u001b[36m(TorchTensorWorker pid=714904)\u001b[0m   File \"/home/wxdeng/stbr-min/python/ray/dag/communication_node.py\", line 72, in execute\n",
      "\u001b[36m(TorchTensorWorker pid=714904)\u001b[0m     self.nccl_ch.write(data)\n",
      "\u001b[36m(TorchTensorWorker pid=714904)\u001b[0m     ~~~~~~~~~~~~~~~~~~^^^^^^\n",
      "\u001b[36m(TorchTensorWorker pid=714904)\u001b[0m   File \"/home/wxdeng/stbr-min/python/ray/experimental/channel/torch_tensor_nccl_channel.py\", line 243, in write\n",
      "\u001b[36m(TorchTensorWorker pid=714904)\u001b[0m     raise value\n",
      "\u001b[36m(TorchTensorWorker pid=714904)\u001b[0m   File \"/home/wxdeng/stbr-min/python/ray/dag/compiled_dag_node.py\", line 695, in exec_operation_inner\n",
      "\u001b[36m(TorchTensorWorker pid=714904)\u001b[0m     _process_return_vals(input_data, return_single_output=False)\n",
      "\u001b[36m(TorchTensorWorker pid=714904)\u001b[0m     ~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[36m(TorchTensorWorker pid=714904)\u001b[0m   File \"/home/wxdeng/stbr-min/python/ray/experimental/compiled_dag_ref.py\", line 27, in _process_return_vals\n",
      "\u001b[36m(TorchTensorWorker pid=714904)\u001b[0m     raise val.as_instanceof_cause()\n",
      "\u001b[36m(TorchTensorWorker pid=714904)\u001b[0m ray.exceptions.RayTaskError(RuntimeError): \u001b[36mray::TorchTensorWorker.__ray_call__()\u001b[39m (pid=714904, ip=128.208.3.124)\n",
      "\u001b[36m(TorchTensorWorker pid=714904)\u001b[0m   File \"/home/wxdeng/stbr-min/python/ray/dag/tests/experimental/test_torch_tensor_dag.py\", line 67, in send_or_raise\n",
      "\u001b[36m(TorchTensorWorker pid=714904)\u001b[0m     raise RuntimeError()\n",
      "\u001b[36m(TorchTensorWorker pid=714904)\u001b[0m RuntimeError\n",
      "\u001b[36m(TorchTensorWorker pid=714904)\u001b[0m Destructing NCCL group on actor: Actor(TorchTensorWorker, 7fdb9e26840c308a33aa785501000000)\n",
      "\u001b[36m(TorchTensorWorker pid=714905)\u001b[0m Destructing NCCL group on actor: Actor(TorchTensorWorker, c3518eb0a7edc902f77b231801000000)\n",
      "2025-06-02 12:17:08,157\tINFO compiled_dag_node.py:2273 -- Waiting for worker tasks to exit\n",
      "2025-06-02 12:17:08,158\tINFO compiled_dag_node.py:2276 -- Teardown complete\n",
      "\n",
      "test_torch_tensor_dag.py::test_torch_tensor_exceptions[False-True-False-ray_start_regular0] 2025-06-02 12:17:11,050\tINFO worker.py:1929 -- Started a local Ray instance.\n",
      "2025-06-02 12:17:13,352\tINFO torch_tensor_nccl_channel.py:772 -- Creating NCCL group d3da710d-fa21-4c2b-a00d-ff2ccd63fc31 on actors: [Actor(TorchTensorWorker, b127afef5afb5ce511dc146901000000), Actor(TorchTensorWorker, ca8fb0113d9ecdc36b01d4a801000000)]\n",
      "2025-06-02 12:17:14,429\tINFO torch_tensor_nccl_channel.py:797 -- NCCL group initialized.\n",
      "2025-06-02 12:17:14,622\tINFO dag_node_operation.py:592 -- Writing compiled graph schedule visualization to compiled_graph_schedule.png\n",
      "2025-06-02 12:17:14,824\tINFO compiled_dag_node.py:2246 -- Tearing down compiled DAG\n",
      "\u001b[32mPASSED\u001b[0m2025-06-02 12:17:14,826\tINFO compiled_dag_node.py:2251 -- Cancelling compiled worker on actor: Actor(TorchTensorWorker, ca8fb0113d9ecdc36b01d4a801000000)\n",
      "2025-06-02 12:17:14,826\tINFO compiled_dag_node.py:2251 -- Cancelling compiled worker on actor: Actor(TorchTensorWorker, b127afef5afb5ce511dc146901000000)\n",
      "\u001b[36m(TorchTensorWorker pid=715639)\u001b[0m Destructing NCCL group on actor: Actor(TorchTensorWorker, b127afef5afb5ce511dc146901000000)\n",
      "\u001b[36m(TorchTensorWorker pid=715637)\u001b[0m ERROR:root:Compiled DAG task exited with exception\n",
      "\u001b[36m(TorchTensorWorker pid=715637)\u001b[0m Traceback (most recent call last):\n",
      "\u001b[36m(TorchTensorWorker pid=715637)\u001b[0m   File \"/home/wxdeng/stbr-min/python/ray/dag/compiled_dag_node.py\", line 242, in do_exec_tasks\n",
      "\u001b[36m(TorchTensorWorker pid=715637)\u001b[0m     done = tasks[operation.exec_task_idx].exec_operation(\n",
      "\u001b[36m(TorchTensorWorker pid=715637)\u001b[0m         self, overlap_gpu_communication\n",
      "\u001b[36m(TorchTensorWorker pid=715637)\u001b[0m     )\n",
      "\u001b[36m(TorchTensorWorker pid=715637)\u001b[0m   File \"/home/wxdeng/stbr-min/python/ray/dag/compiled_dag_node.py\", line 673, in exec_operation\n",
      "\u001b[36m(TorchTensorWorker pid=715637)\u001b[0m     return self.exec_operation_inner(\n",
      "\u001b[36m(TorchTensorWorker pid=715637)\u001b[0m            ~~~~~~~~~~~~~~~~~~~~~~~~~^\n",
      "\u001b[36m(TorchTensorWorker pid=715637)\u001b[0m         class_handle, overlap_gpu_communication\n",
      "\u001b[36m(TorchTensorWorker pid=715637)\u001b[0m         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[36m(TorchTensorWorker pid=715637)\u001b[0m     )\n",
      "\u001b[36m(TorchTensorWorker pid=715637)\u001b[0m     ^\n",
      "\u001b[36m(TorchTensorWorker pid=715637)\u001b[0m   File \"/home/wxdeng/stbr-min/python/ray/dag/compiled_dag_node.py\", line 736, in exec_operation_inner\n",
      "\u001b[36m(TorchTensorWorker pid=715637)\u001b[0m     raise exc\n",
      "\u001b[36m(TorchTensorWorker pid=715637)\u001b[0m   File \"/home/wxdeng/stbr-min/python/ray/dag/compiled_dag_node.py\", line 731, in exec_operation_inner\n",
      "\u001b[36m(TorchTensorWorker pid=715637)\u001b[0m     output_val = method(*input_values, **self.resolved_kwargs)\n",
      "\u001b[36m(TorchTensorWorker pid=715637)\u001b[0m   File \"/home/wxdeng/stbr-min/python/ray/dag/communication_node.py\", line 72, in execute\n",
      "\u001b[36m(TorchTensorWorker pid=715637)\u001b[0m     self.nccl_ch.write(data)\n",
      "\u001b[36m(TorchTensorWorker pid=715637)\u001b[0m     ~~~~~~~~~~~~~~~~~~^^^^^^\n",
      "\u001b[36m(TorchTensorWorker pid=715637)\u001b[0m   File \"/home/wxdeng/stbr-min/python/ray/experimental/channel/torch_tensor_nccl_channel.py\", line 243, in write\n",
      "\u001b[36m(TorchTensorWorker pid=715637)\u001b[0m     raise value\n",
      "\u001b[36m(TorchTensorWorker pid=715637)\u001b[0m   File \"/home/wxdeng/stbr-min/python/ray/dag/compiled_dag_node.py\", line 695, in exec_operation_inner\n",
      "\u001b[36m(TorchTensorWorker pid=715637)\u001b[0m     _process_return_vals(input_data, return_single_output=False)\n",
      "\u001b[36m(TorchTensorWorker pid=715637)\u001b[0m     ~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[36m(TorchTensorWorker pid=715637)\u001b[0m   File \"/home/wxdeng/stbr-min/python/ray/experimental/compiled_dag_ref.py\", line 27, in _process_return_vals\n",
      "\u001b[36m(TorchTensorWorker pid=715637)\u001b[0m     raise val.as_instanceof_cause()\n",
      "\u001b[36m(TorchTensorWorker pid=715637)\u001b[0m ray.exceptions.RayTaskError(RuntimeError): \u001b[36mray::TorchTensorWorker.__ray_call__()\u001b[39m (pid=715637, ip=128.208.3.124)\n",
      "\u001b[36m(TorchTensorWorker pid=715637)\u001b[0m   File \"/home/wxdeng/stbr-min/python/ray/dag/tests/experimental/test_torch_tensor_dag.py\", line 67, in send_or_raise\n",
      "\u001b[36m(TorchTensorWorker pid=715637)\u001b[0m     raise RuntimeError()\n",
      "\u001b[36m(TorchTensorWorker pid=715637)\u001b[0m RuntimeError\n",
      "\u001b[36m(TorchTensorWorker pid=715637)\u001b[0m Destructing NCCL group on actor: Actor(TorchTensorWorker, ca8fb0113d9ecdc36b01d4a801000000)\n",
      "2025-06-02 12:17:15,345\tINFO compiled_dag_node.py:2273 -- Waiting for worker tasks to exit\n",
      "2025-06-02 12:17:15,345\tINFO compiled_dag_node.py:2276 -- Teardown complete\n",
      "\n",
      "test_torch_tensor_dag.py::test_torch_tensor_exceptions[False-True-True-ray_start_regular0] 2025-06-02 12:17:18,407\tINFO worker.py:1929 -- Started a local Ray instance.\n",
      "2025-06-02 12:17:20,609\tINFO torch_tensor_nccl_channel.py:772 -- Creating NCCL group 73dd8c54-a20a-4492-82f5-64d591cef979 on actors: [Actor(TorchTensorWorker, 6654dab8f1cdcece13ff4d9f01000000), Actor(TorchTensorWorker, b04d21c0a86a7ee609692ae001000000)]\n",
      "2025-06-02 12:17:21,706\tINFO torch_tensor_nccl_channel.py:797 -- NCCL group initialized.\n",
      "2025-06-02 12:17:21,900\tINFO dag_node_operation.py:592 -- Writing compiled graph schedule visualization to compiled_graph_schedule.png\n",
      "\u001b[36m(TorchTensorWorker pid=716336)\u001b[0m ERROR:root:Compiled DAG task exited with exception\n",
      "\u001b[36m(TorchTensorWorker pid=716336)\u001b[0m Traceback (most recent call last):\n",
      "\u001b[36m(TorchTensorWorker pid=716336)\u001b[0m   File \"/home/wxdeng/stbr-min/python/ray/dag/compiled_dag_node.py\", line 242, in do_exec_tasks\n",
      "\u001b[36m(TorchTensorWorker pid=716336)\u001b[0m     done = tasks[operation.exec_task_idx].exec_operation(\n",
      "\u001b[36m(TorchTensorWorker pid=716336)\u001b[0m         self, overlap_gpu_communication\n",
      "\u001b[36m(TorchTensorWorker pid=716336)\u001b[0m     )\n",
      "\u001b[36m(TorchTensorWorker pid=716336)\u001b[0m   File \"/home/wxdeng/stbr-min/python/ray/dag/compiled_dag_node.py\", line 673, in exec_operation\n",
      "\u001b[36m(TorchTensorWorker pid=716336)\u001b[0m     return self.exec_operation_inner(\n",
      "\u001b[36m(TorchTensorWorker pid=716336)\u001b[0m            ~~~~~~~~~~~~~~~~~~~~~~~~~^\n",
      "\u001b[36m(TorchTensorWorker pid=716336)\u001b[0m         class_handle, overlap_gpu_communication\n",
      "\u001b[36m(TorchTensorWorker pid=716336)\u001b[0m         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[36m(TorchTensorWorker pid=716336)\u001b[0m     )\n",
      "\u001b[36m(TorchTensorWorker pid=716336)\u001b[0m     ^\n",
      "\u001b[36m(TorchTensorWorker pid=716336)\u001b[0m   File \"/home/wxdeng/stbr-min/python/ray/dag/compiled_dag_node.py\", line 736, in exec_operation_inner\n",
      "\u001b[36m(TorchTensorWorker pid=716336)\u001b[0m     raise exc\n",
      "\u001b[36m(TorchTensorWorker pid=716336)\u001b[0m   File \"/home/wxdeng/stbr-min/python/ray/dag/compiled_dag_node.py\", line 731, in exec_operation_inner\n",
      "\u001b[36m(TorchTensorWorker pid=716336)\u001b[0m     output_val = method(*input_values, **self.resolved_kwargs)\n",
      "\u001b[36m(TorchTensorWorker pid=716336)\u001b[0m   File \"/home/wxdeng/stbr-min/python/ray/dag/communication_node.py\", line 72, in execute\n",
      "\u001b[36m(TorchTensorWorker pid=716336)\u001b[0m     self.nccl_ch.write(data)\n",
      "\u001b[36m(TorchTensorWorker pid=716336)\u001b[0m     ~~~~~~~~~~~~~~~~~~^^^^^^\n",
      "\u001b[36m(TorchTensorWorker pid=716336)\u001b[0m   File \"/home/wxdeng/stbr-min/python/ray/experimental/channel/torch_tensor_nccl_channel.py\", line 243, in write\n",
      "\u001b[36m(TorchTensorWorker pid=716336)\u001b[0m     raise value\n",
      "\u001b[36m(TorchTensorWorker pid=716336)\u001b[0m   File \"/home/wxdeng/stbr-min/python/ray/dag/compiled_dag_node.py\", line 695, in exec_operation_inner\n",
      "\u001b[36m(TorchTensorWorker pid=716336)\u001b[0m     _process_return_vals(input_data, return_single_output=False)\n",
      "\u001b[36m(TorchTensorWorker pid=716336)\u001b[0m     ~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[36m(TorchTensorWorker pid=716336)\u001b[0m   File \"/home/wxdeng/stbr-min/python/ray/experimental/compiled_dag_ref.py\", line 27, in _process_return_vals\n",
      "\u001b[36m(TorchTensorWorker pid=716336)\u001b[0m     raise val.as_instanceof_cause()\n",
      "\u001b[36m(TorchTensorWorker pid=716336)\u001b[0m ray.exceptions.RayTaskError(RuntimeError): \u001b[36mray::TorchTensorWorker.__ray_call__()\u001b[39m (pid=716336, ip=128.208.3.124)\n",
      "\u001b[36m(TorchTensorWorker pid=716336)\u001b[0m   File \"/home/wxdeng/stbr-min/python/ray/dag/tests/experimental/test_torch_tensor_dag.py\", line 67, in send_or_raise\n",
      "\u001b[36m(TorchTensorWorker pid=716336)\u001b[0m     raise RuntimeError()\n",
      "\u001b[36m(TorchTensorWorker pid=716336)\u001b[0m RuntimeError\n",
      "2025-06-02 12:17:22,099\tINFO compiled_dag_node.py:2246 -- Tearing down compiled DAG\n",
      "\u001b[32mPASSED\u001b[0m2025-06-02 12:17:22,101\tINFO compiled_dag_node.py:2251 -- Cancelling compiled worker on actor: Actor(TorchTensorWorker, 6654dab8f1cdcece13ff4d9f01000000)\n",
      "2025-06-02 12:17:22,101\tINFO compiled_dag_node.py:2251 -- Cancelling compiled worker on actor: Actor(TorchTensorWorker, b04d21c0a86a7ee609692ae001000000)\n",
      "\u001b[36m(TorchTensorWorker pid=716336)\u001b[0m Destructing NCCL group on actor: Actor(TorchTensorWorker, 6654dab8f1cdcece13ff4d9f01000000)\n",
      "\u001b[36m(TorchTensorWorker pid=716337)\u001b[0m Destructing NCCL group on actor: Actor(TorchTensorWorker, b04d21c0a86a7ee609692ae001000000)\n",
      "2025-06-02 12:17:22,588\tINFO compiled_dag_node.py:2273 -- Waiting for worker tasks to exit\n",
      "2025-06-02 12:17:22,589\tINFO compiled_dag_node.py:2276 -- Teardown complete\n",
      "\n",
      "test_torch_tensor_dag.py::test_torch_tensor_exceptions[True-False-False-ray_start_regular0] 2025-06-02 12:17:26,530\tINFO worker.py:1929 -- Started a local Ray instance.\n",
      "2025-06-02 12:17:28,852\tINFO torch_tensor_nccl_channel.py:772 -- Creating NCCL group f906de65-c9b2-4766-b2b4-dac50bf5263b on actors: [Actor(TorchTensorWorker, df9826a040c1d2cd1080800501000000), Actor(TorchTensorWorker, fd4c154653dddf6484be6c5501000000)]\n",
      "2025-06-02 12:17:29,942\tINFO torch_tensor_nccl_channel.py:797 -- NCCL group initialized.\n",
      "2025-06-02 12:17:30,139\tINFO dag_node_operation.py:592 -- Writing compiled graph schedule visualization to compiled_graph_schedule.png\n",
      "\u001b[32mPASSED\u001b[0m2025-06-02 12:17:30,330\tINFO compiled_dag_node.py:2246 -- Tearing down compiled DAG\n",
      "2025-06-02 12:17:30,331\tINFO compiled_dag_node.py:2251 -- Cancelling compiled worker on actor: Actor(TorchTensorWorker, df9826a040c1d2cd1080800501000000)\n",
      "2025-06-02 12:17:30,331\tINFO compiled_dag_node.py:2251 -- Cancelling compiled worker on actor: Actor(TorchTensorWorker, fd4c154653dddf6484be6c5501000000)\n",
      "\u001b[36m(TorchTensorWorker pid=717120)\u001b[0m Destructing NCCL group on actor: Actor(TorchTensorWorker, fd4c154653dddf6484be6c5501000000)\n",
      "\u001b[36m(TorchTensorWorker pid=717119)\u001b[0m Destructing NCCL group on actor: Actor(TorchTensorWorker, df9826a040c1d2cd1080800501000000)\n",
      "2025-06-02 12:17:30,850\tINFO compiled_dag_node.py:2273 -- Waiting for worker tasks to exit\n",
      "2025-06-02 12:17:30,851\tINFO compiled_dag_node.py:2234 -- Killing actor: Actor(TorchTensorWorker, df9826a040c1d2cd1080800501000000)\n",
      "2025-06-02 12:17:30,851\tINFO compiled_dag_node.py:2234 -- Killing actor: Actor(TorchTensorWorker, fd4c154653dddf6484be6c5501000000)\n",
      "2025-06-02 12:17:30,852\tINFO compiled_dag_node.py:2276 -- Teardown complete\n",
      "\n",
      "test_torch_tensor_dag.py::test_torch_tensor_exceptions[True-False-True-ray_start_regular0] 2025-06-02 12:17:34,855\tINFO worker.py:1929 -- Started a local Ray instance.\n",
      "2025-06-02 12:17:37,051\tINFO torch_tensor_nccl_channel.py:772 -- Creating NCCL group e21aa003-d702-4c58-a1d4-52468d731651 on actors: [Actor(TorchTensorWorker, 0549c7d8a3a22168bc95634001000000), Actor(TorchTensorWorker, 4cd01c0a9a32bfc9c5d633fd01000000)]\n",
      "2025-06-02 12:17:38,144\tINFO torch_tensor_nccl_channel.py:797 -- NCCL group initialized.\n",
      "2025-06-02 12:17:38,338\tINFO dag_node_operation.py:592 -- Writing compiled graph schedule visualization to compiled_graph_schedule.png\n",
      "2025-06-02 12:17:38,566\tINFO compiled_dag_node.py:2246 -- Tearing down compiled DAG\n",
      "\u001b[32mPASSED\u001b[0m2025-06-02 12:17:38,568\tINFO compiled_dag_node.py:2251 -- Cancelling compiled worker on actor: Actor(TorchTensorWorker, 4cd01c0a9a32bfc9c5d633fd01000000)\n",
      "2025-06-02 12:17:38,568\tINFO compiled_dag_node.py:2251 -- Cancelling compiled worker on actor: Actor(TorchTensorWorker, 0549c7d8a3a22168bc95634001000000)\n",
      "\u001b[36m(TorchTensorWorker pid=717849)\u001b[0m Destructing NCCL group on actor: Actor(TorchTensorWorker, 0549c7d8a3a22168bc95634001000000)\n",
      "\u001b[36m(TorchTensorWorker pid=717848)\u001b[0m ERROR:root:Compiled DAG task exited with exception\n",
      "\u001b[36m(TorchTensorWorker pid=717848)\u001b[0m Traceback (most recent call last):\n",
      "\u001b[36m(TorchTensorWorker pid=717848)\u001b[0m   File \"/home/wxdeng/stbr-min/python/ray/dag/compiled_dag_node.py\", line 242, in do_exec_tasks\n",
      "\u001b[36m(TorchTensorWorker pid=717848)\u001b[0m     done = tasks[operation.exec_task_idx].exec_operation(\n",
      "\u001b[36m(TorchTensorWorker pid=717848)\u001b[0m         self, overlap_gpu_communication\n",
      "\u001b[36m(TorchTensorWorker pid=717848)\u001b[0m     )\n",
      "\u001b[36m(TorchTensorWorker pid=717848)\u001b[0m   File \"/home/wxdeng/stbr-min/python/ray/dag/compiled_dag_node.py\", line 673, in exec_operation\n",
      "\u001b[36m(TorchTensorWorker pid=717848)\u001b[0m     return self.exec_operation_inner(\n",
      "\u001b[36m(TorchTensorWorker pid=717848)\u001b[0m            ~~~~~~~~~~~~~~~~~~~~~~~~~^\n",
      "\u001b[36m(TorchTensorWorker pid=717848)\u001b[0m         class_handle, overlap_gpu_communication\n",
      "\u001b[36m(TorchTensorWorker pid=717848)\u001b[0m         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[36m(TorchTensorWorker pid=717848)\u001b[0m     )\n",
      "\u001b[36m(TorchTensorWorker pid=717848)\u001b[0m     ^\n",
      "\u001b[36m(TorchTensorWorker pid=717848)\u001b[0m   File \"/home/wxdeng/stbr-min/python/ray/dag/compiled_dag_node.py\", line 736, in exec_operation_inner\n",
      "\u001b[36m(TorchTensorWorker pid=717848)\u001b[0m     raise exc\n",
      "\u001b[36m(TorchTensorWorker pid=717848)\u001b[0m   File \"/home/wxdeng/stbr-min/python/ray/dag/compiled_dag_node.py\", line 731, in exec_operation_inner\n",
      "\u001b[36m(TorchTensorWorker pid=717848)\u001b[0m     output_val = method(*input_values, **self.resolved_kwargs)\n",
      "\u001b[36m(TorchTensorWorker pid=717848)\u001b[0m   File \"/home/wxdeng/stbr-min/python/ray/dag/communication_node.py\", line 72, in execute\n",
      "\u001b[36m(TorchTensorWorker pid=717848)\u001b[0m     self.nccl_ch.write(data)\n",
      "\u001b[36m(TorchTensorWorker pid=717848)\u001b[0m     ~~~~~~~~~~~~~~~~~~^^^^^^\n",
      "\u001b[36m(TorchTensorWorker pid=717848)\u001b[0m   File \"/home/wxdeng/stbr-min/python/ray/experimental/channel/torch_tensor_nccl_channel.py\", line 243, in write\n",
      "\u001b[36m(TorchTensorWorker pid=717848)\u001b[0m     raise value\n",
      "\u001b[36m(TorchTensorWorker pid=717848)\u001b[0m   File \"/home/wxdeng/stbr-min/python/ray/dag/compiled_dag_node.py\", line 695, in exec_operation_inner\n",
      "\u001b[36m(TorchTensorWorker pid=717848)\u001b[0m     _process_return_vals(input_data, return_single_output=False)\n",
      "\u001b[36m(TorchTensorWorker pid=717848)\u001b[0m     ~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[36m(TorchTensorWorker pid=717848)\u001b[0m   File \"/home/wxdeng/stbr-min/python/ray/experimental/compiled_dag_ref.py\", line 27, in _process_return_vals\n",
      "\u001b[36m(TorchTensorWorker pid=717848)\u001b[0m     raise val.as_instanceof_cause()\n",
      "\u001b[36m(TorchTensorWorker pid=717848)\u001b[0m ray.exceptions.RayTaskError(RuntimeError): \u001b[36mray::TorchTensorWorker.__ray_call__()\u001b[39m (pid=717848, ip=128.208.3.124)\n",
      "\u001b[36m(TorchTensorWorker pid=717848)\u001b[0m   File \"/home/wxdeng/stbr-min/python/ray/dag/tests/experimental/test_torch_tensor_dag.py\", line 67, in send_or_raise\n",
      "\u001b[36m(TorchTensorWorker pid=717848)\u001b[0m     raise RuntimeError()\n",
      "\u001b[36m(TorchTensorWorker pid=717848)\u001b[0m RuntimeError\n",
      "\u001b[36m(TorchTensorWorker pid=717848)\u001b[0m Destructing NCCL group on actor: Actor(TorchTensorWorker, 4cd01c0a9a32bfc9c5d633fd01000000)\n",
      "2025-06-02 12:17:39,087\tINFO compiled_dag_node.py:2273 -- Waiting for worker tasks to exit\n",
      "2025-06-02 12:17:39,087\tINFO compiled_dag_node.py:2276 -- Teardown complete\n",
      "\n",
      "test_torch_tensor_dag.py::test_torch_tensor_exceptions[True-True-False-ray_start_regular0] 2025-06-02 12:17:43,183\tINFO worker.py:1929 -- Started a local Ray instance.\n",
      "2025-06-02 12:17:45,494\tINFO torch_tensor_nccl_channel.py:772 -- Creating NCCL group 8a81d3bf-ab59-44cd-8802-26bdc0f43740 on actors: [Actor(TorchTensorWorker, f0a041b2347efb41256c52a001000000), Actor(TorchTensorWorker, 9ab70a64ea2fcfddef7acf0e01000000)]\n",
      "2025-06-02 12:17:46,575\tINFO torch_tensor_nccl_channel.py:797 -- NCCL group initialized.\n",
      "2025-06-02 12:17:46,769\tINFO dag_node_operation.py:592 -- Writing compiled graph schedule visualization to compiled_graph_schedule.png\n",
      "2025-06-02 12:17:46,962\tINFO compiled_dag_node.py:2246 -- Tearing down compiled DAG\n",
      "\u001b[32mPASSED\u001b[0m2025-06-02 12:17:46,964\tINFO compiled_dag_node.py:2251 -- Cancelling compiled worker on actor: Actor(TorchTensorWorker, f0a041b2347efb41256c52a001000000)\n",
      "2025-06-02 12:17:46,964\tINFO compiled_dag_node.py:2251 -- Cancelling compiled worker on actor: Actor(TorchTensorWorker, 9ab70a64ea2fcfddef7acf0e01000000)\n",
      "\u001b[36m(TorchTensorWorker pid=718616)\u001b[0m ERROR:root:Compiled DAG task exited with exception\n",
      "\u001b[36m(TorchTensorWorker pid=718616)\u001b[0m Traceback (most recent call last):\n",
      "\u001b[36m(TorchTensorWorker pid=718616)\u001b[0m   File \"/home/wxdeng/stbr-min/python/ray/dag/compiled_dag_node.py\", line 242, in do_exec_tasks\n",
      "\u001b[36m(TorchTensorWorker pid=718616)\u001b[0m     done = tasks[operation.exec_task_idx].exec_operation(\n",
      "\u001b[36m(TorchTensorWorker pid=718616)\u001b[0m         self, overlap_gpu_communication\n",
      "\u001b[36m(TorchTensorWorker pid=718616)\u001b[0m     )\n",
      "\u001b[36m(TorchTensorWorker pid=718616)\u001b[0m   File \"/home/wxdeng/stbr-min/python/ray/dag/compiled_dag_node.py\", line 673, in exec_operation\n",
      "\u001b[36m(TorchTensorWorker pid=718616)\u001b[0m     return self.exec_operation_inner(\n",
      "\u001b[36m(TorchTensorWorker pid=718616)\u001b[0m            ~~~~~~~~~~~~~~~~~~~~~~~~~^\n",
      "\u001b[36m(TorchTensorWorker pid=718616)\u001b[0m         class_handle, overlap_gpu_communication\n",
      "\u001b[36m(TorchTensorWorker pid=718616)\u001b[0m         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[36m(TorchTensorWorker pid=718616)\u001b[0m     )\n",
      "\u001b[36m(TorchTensorWorker pid=718616)\u001b[0m     ^\n",
      "\u001b[36m(TorchTensorWorker pid=718616)\u001b[0m   File \"/home/wxdeng/stbr-min/python/ray/dag/compiled_dag_node.py\", line 736, in exec_operation_inner\n",
      "\u001b[36m(TorchTensorWorker pid=718616)\u001b[0m     raise exc\n",
      "\u001b[36m(TorchTensorWorker pid=718616)\u001b[0m   File \"/home/wxdeng/stbr-min/python/ray/dag/compiled_dag_node.py\", line 731, in exec_operation_inner\n",
      "\u001b[36m(TorchTensorWorker pid=718616)\u001b[0m     output_val = method(*input_values, **self.resolved_kwargs)\n",
      "\u001b[36m(TorchTensorWorker pid=718616)\u001b[0m   File \"/home/wxdeng/stbr-min/python/ray/dag/communication_node.py\", line 72, in execute\n",
      "\u001b[36m(TorchTensorWorker pid=718616)\u001b[0m     self.nccl_ch.write(data)\n",
      "\u001b[36m(TorchTensorWorker pid=718616)\u001b[0m     ~~~~~~~~~~~~~~~~~~^^^^^^\n",
      "\u001b[36m(TorchTensorWorker pid=718616)\u001b[0m   File \"/home/wxdeng/stbr-min/python/ray/experimental/channel/torch_tensor_nccl_channel.py\", line 243, in write\n",
      "\u001b[36m(TorchTensorWorker pid=718616)\u001b[0m     raise value\n",
      "\u001b[36m(TorchTensorWorker pid=718616)\u001b[0m   File \"/home/wxdeng/stbr-min/python/ray/dag/compiled_dag_node.py\", line 695, in exec_operation_inner\n",
      "\u001b[36m(TorchTensorWorker pid=718616)\u001b[0m     _process_return_vals(input_data, return_single_output=False)\n",
      "\u001b[36m(TorchTensorWorker pid=718616)\u001b[0m     ~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[36m(TorchTensorWorker pid=718616)\u001b[0m   File \"/home/wxdeng/stbr-min/python/ray/experimental/compiled_dag_ref.py\", line 27, in _process_return_vals\n",
      "\u001b[36m(TorchTensorWorker pid=718616)\u001b[0m     raise val.as_instanceof_cause()\n",
      "\u001b[36m(TorchTensorWorker pid=718616)\u001b[0m ray.exceptions.RayTaskError(RuntimeError): \u001b[36mray::TorchTensorWorker.__ray_call__()\u001b[39m (pid=718616, ip=128.208.3.124)\n",
      "\u001b[36m(TorchTensorWorker pid=718616)\u001b[0m   File \"/home/wxdeng/stbr-min/python/ray/dag/tests/experimental/test_torch_tensor_dag.py\", line 67, in send_or_raise\n",
      "\u001b[36m(TorchTensorWorker pid=718616)\u001b[0m     raise RuntimeError()\n",
      "\u001b[36m(TorchTensorWorker pid=718616)\u001b[0m RuntimeError\n",
      "\u001b[36m(TorchTensorWorker pid=718616)\u001b[0m Destructing NCCL group on actor: Actor(TorchTensorWorker, f0a041b2347efb41256c52a001000000)\n",
      "\u001b[36m(TorchTensorWorker pid=718615)\u001b[0m Destructing NCCL group on actor: Actor(TorchTensorWorker, 9ab70a64ea2fcfddef7acf0e01000000)\n",
      "2025-06-02 12:17:47,453\tINFO compiled_dag_node.py:2273 -- Waiting for worker tasks to exit\n",
      "2025-06-02 12:17:47,453\tINFO compiled_dag_node.py:2276 -- Teardown complete\n",
      "\n",
      "test_torch_tensor_dag.py::test_torch_tensor_exceptions[True-True-True-ray_start_regular0] 2025-06-02 12:17:50,435\tINFO worker.py:1929 -- Started a local Ray instance.\n",
      "2025-06-02 12:17:52,602\tINFO torch_tensor_nccl_channel.py:772 -- Creating NCCL group 48b7a732-662e-4e9b-b71b-8c92bdd978f6 on actors: [Actor(TorchTensorWorker, 95a388bbdd180b26dcb778f101000000), Actor(TorchTensorWorker, 14e7433fd3c711e04f47c1f501000000)]\n",
      "2025-06-02 12:17:53,689\tINFO torch_tensor_nccl_channel.py:797 -- NCCL group initialized.\n",
      "2025-06-02 12:17:53,863\tINFO dag_node_operation.py:592 -- Writing compiled graph schedule visualization to compiled_graph_schedule.png\n",
      "2025-06-02 12:17:54,067\tINFO compiled_dag_node.py:2246 -- Tearing down compiled DAG\n",
      "\u001b[32mPASSED\u001b[0m2025-06-02 12:17:54,070\tINFO compiled_dag_node.py:2251 -- Cancelling compiled worker on actor: Actor(TorchTensorWorker, 14e7433fd3c711e04f47c1f501000000)\n",
      "2025-06-02 12:17:54,070\tINFO compiled_dag_node.py:2251 -- Cancelling compiled worker on actor: Actor(TorchTensorWorker, 95a388bbdd180b26dcb778f101000000)\n",
      "\u001b[36m(TorchTensorWorker pid=719320)\u001b[0m ERROR:root:Compiled DAG task exited with exception\n",
      "\u001b[36m(TorchTensorWorker pid=719320)\u001b[0m Traceback (most recent call last):\n",
      "\u001b[36m(TorchTensorWorker pid=719320)\u001b[0m   File \"/home/wxdeng/stbr-min/python/ray/dag/compiled_dag_node.py\", line 242, in do_exec_tasks\n",
      "\u001b[36m(TorchTensorWorker pid=719320)\u001b[0m     done = tasks[operation.exec_task_idx].exec_operation(\n",
      "\u001b[36m(TorchTensorWorker pid=719320)\u001b[0m         self, overlap_gpu_communication\n",
      "\u001b[36m(TorchTensorWorker pid=719320)\u001b[0m     )\n",
      "\u001b[36m(TorchTensorWorker pid=719320)\u001b[0m   File \"/home/wxdeng/stbr-min/python/ray/dag/compiled_dag_node.py\", line 673, in exec_operation\n",
      "\u001b[36m(TorchTensorWorker pid=719320)\u001b[0m     return self.exec_operation_inner(\n",
      "\u001b[36m(TorchTensorWorker pid=719320)\u001b[0m            ~~~~~~~~~~~~~~~~~~~~~~~~~^\n",
      "\u001b[36m(TorchTensorWorker pid=719320)\u001b[0m         class_handle, overlap_gpu_communication\n",
      "\u001b[36m(TorchTensorWorker pid=719320)\u001b[0m         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[36m(TorchTensorWorker pid=719320)\u001b[0m     )\n",
      "\u001b[36m(TorchTensorWorker pid=719320)\u001b[0m     ^\n",
      "\u001b[36m(TorchTensorWorker pid=719320)\u001b[0m   File \"/home/wxdeng/stbr-min/python/ray/dag/compiled_dag_node.py\", line 736, in exec_operation_inner\n",
      "\u001b[36m(TorchTensorWorker pid=719320)\u001b[0m     raise exc\n",
      "\u001b[36m(TorchTensorWorker pid=719320)\u001b[0m   File \"/home/wxdeng/stbr-min/python/ray/dag/compiled_dag_node.py\", line 731, in exec_operation_inner\n",
      "\u001b[36m(TorchTensorWorker pid=719320)\u001b[0m     output_val = method(*input_values, **self.resolved_kwargs)\n",
      "\u001b[36m(TorchTensorWorker pid=719320)\u001b[0m   File \"/home/wxdeng/stbr-min/python/ray/dag/communication_node.py\", line 72, in execute\n",
      "\u001b[36m(TorchTensorWorker pid=719320)\u001b[0m     self.nccl_ch.write(data)\n",
      "\u001b[36m(TorchTensorWorker pid=719320)\u001b[0m     ~~~~~~~~~~~~~~~~~~^^^^^^\n",
      "\u001b[36m(TorchTensorWorker pid=719320)\u001b[0m   File \"/home/wxdeng/stbr-min/python/ray/experimental/channel/torch_tensor_nccl_channel.py\", line 243, in write\n",
      "\u001b[36m(TorchTensorWorker pid=719320)\u001b[0m     raise value\n",
      "\u001b[36m(TorchTensorWorker pid=719320)\u001b[0m   File \"/home/wxdeng/stbr-min/python/ray/dag/compiled_dag_node.py\", line 695, in exec_operation_inner\n",
      "\u001b[36m(TorchTensorWorker pid=719320)\u001b[0m     _process_return_vals(input_data, return_single_output=False)\n",
      "\u001b[36m(TorchTensorWorker pid=719320)\u001b[0m     ~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[36m(TorchTensorWorker pid=719320)\u001b[0m   File \"/home/wxdeng/stbr-min/python/ray/experimental/compiled_dag_ref.py\", line 27, in _process_return_vals\n",
      "\u001b[36m(TorchTensorWorker pid=719320)\u001b[0m     raise val.as_instanceof_cause()\n",
      "\u001b[36m(TorchTensorWorker pid=719320)\u001b[0m ray.exceptions.RayTaskError(RuntimeError): \u001b[36mray::TorchTensorWorker.__ray_call__()\u001b[39m (pid=719320, ip=128.208.3.124)\n",
      "\u001b[36m(TorchTensorWorker pid=719320)\u001b[0m   File \"/home/wxdeng/stbr-min/python/ray/dag/tests/experimental/test_torch_tensor_dag.py\", line 67, in send_or_raise\n",
      "\u001b[36m(TorchTensorWorker pid=719320)\u001b[0m     raise RuntimeError()\n",
      "\u001b[36m(TorchTensorWorker pid=719320)\u001b[0m RuntimeError\n",
      "\u001b[36m(TorchTensorWorker pid=719320)\u001b[0m Destructing NCCL group on actor: Actor(TorchTensorWorker, 14e7433fd3c711e04f47c1f501000000)\n",
      "\u001b[36m(TorchTensorWorker pid=719319)\u001b[0m Destructing NCCL group on actor: Actor(TorchTensorWorker, 95a388bbdd180b26dcb778f101000000)\n",
      "2025-06-02 12:17:54,551\tINFO compiled_dag_node.py:2273 -- Waiting for worker tasks to exit\n",
      "2025-06-02 12:17:54,551\tINFO compiled_dag_node.py:2276 -- Teardown complete\n",
      "\n",
      "test_torch_tensor_dag.py::test_torch_tensor_exceptions2[ray_start_regular0] 2025-06-02 12:17:57,645\tINFO worker.py:1929 -- Started a local Ray instance.\n",
      "2025-06-02 12:17:59,861\tINFO torch_tensor_nccl_channel.py:772 -- Creating NCCL group 35fb4de9-ac30-4c0d-835a-cd1dd48f74e8 on actors: [Actor(TorchTensorWorker, 2914ca080594a3ec0a023b1501000000), Actor(TorchTensorWorker, 96939df4f3ef187de192013901000000)]\n",
      "2025-06-02 12:18:00,941\tINFO torch_tensor_nccl_channel.py:797 -- NCCL group initialized.\n",
      "2025-06-02 12:18:01,041\tINFO dag_node_operation.py:592 -- Writing compiled graph schedule visualization to compiled_graph_schedule.png\n",
      "2025-06-02 12:18:01,173\tINFO compiled_dag_node.py:2246 -- Tearing down compiled DAG\n",
      "\u001b[32mPASSED\u001b[0m2025-06-02 12:18:01,175\tINFO compiled_dag_node.py:2251 -- Cancelling compiled worker on actor: Actor(TorchTensorWorker, 96939df4f3ef187de192013901000000)\n",
      "2025-06-02 12:18:01,176\tINFO compiled_dag_node.py:2251 -- Cancelling compiled worker on actor: Actor(TorchTensorWorker, 2914ca080594a3ec0a023b1501000000)\n",
      "\u001b[36m(TorchTensorWorker pid=720071)\u001b[0m ERROR:root:Compiled DAG task exited with exception\n",
      "\u001b[36m(TorchTensorWorker pid=720071)\u001b[0m Traceback (most recent call last):\n",
      "\u001b[36m(TorchTensorWorker pid=720071)\u001b[0m   File \"/home/wxdeng/stbr-min/python/ray/dag/compiled_dag_node.py\", line 242, in do_exec_tasks\n",
      "\u001b[36m(TorchTensorWorker pid=720071)\u001b[0m     done = tasks[operation.exec_task_idx].exec_operation(\n",
      "\u001b[36m(TorchTensorWorker pid=720071)\u001b[0m         self, overlap_gpu_communication\n",
      "\u001b[36m(TorchTensorWorker pid=720071)\u001b[0m     )\n",
      "\u001b[36m(TorchTensorWorker pid=720071)\u001b[0m   File \"/home/wxdeng/stbr-min/python/ray/dag/compiled_dag_node.py\", line 673, in exec_operation\n",
      "\u001b[36m(TorchTensorWorker pid=720071)\u001b[0m     return self.exec_operation_inner(\n",
      "\u001b[36m(TorchTensorWorker pid=720071)\u001b[0m            ~~~~~~~~~~~~~~~~~~~~~~~~~^\n",
      "\u001b[36m(TorchTensorWorker pid=720071)\u001b[0m         class_handle, overlap_gpu_communication\n",
      "\u001b[36m(TorchTensorWorker pid=720071)\u001b[0m         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[36m(TorchTensorWorker pid=720071)\u001b[0m     )\n",
      "\u001b[36m(TorchTensorWorker pid=720071)\u001b[0m     ^\n",
      "\u001b[36m(TorchTensorWorker pid=720071)\u001b[0m   File \"/home/wxdeng/stbr-min/python/ray/dag/compiled_dag_node.py\", line 736, in exec_operation_inner\n",
      "\u001b[36m(TorchTensorWorker pid=720071)\u001b[0m     raise exc\n",
      "\u001b[36m(TorchTensorWorker pid=720071)\u001b[0m   File \"/home/wxdeng/stbr-min/python/ray/dag/compiled_dag_node.py\", line 731, in exec_operation_inner\n",
      "\u001b[36m(TorchTensorWorker pid=720071)\u001b[0m     output_val = method(*input_values, **self.resolved_kwargs)\n",
      "\u001b[36m(TorchTensorWorker pid=720071)\u001b[0m   File \"/home/wxdeng/stbr-min/python/ray/dag/communication_node.py\", line 72, in execute\n",
      "\u001b[36m(TorchTensorWorker pid=720071)\u001b[0m     self.nccl_ch.write(data)\n",
      "\u001b[36m(TorchTensorWorker pid=720071)\u001b[0m     ~~~~~~~~~~~~~~~~~~^^^^^^\n",
      "\u001b[36m(TorchTensorWorker pid=720071)\u001b[0m   File \"/home/wxdeng/stbr-min/python/ray/experimental/channel/torch_tensor_nccl_channel.py\", line 257, in write\n",
      "\u001b[36m(TorchTensorWorker pid=720071)\u001b[0m     raise ValueError(\n",
      "\u001b[36m(TorchTensorWorker pid=720071)\u001b[0m     ...<3 lines>...\n",
      "\u001b[36m(TorchTensorWorker pid=720071)\u001b[0m     )\n",
      "\u001b[36m(TorchTensorWorker pid=720071)\u001b[0m ValueError: Task annotated with _direct_return=True must return a CUDA torch.Tensor, instead found value `1`. DAG will shut down.\n",
      "\u001b[36m(TorchTensorWorker pid=720071)\u001b[0m Destructing NCCL group on actor: Actor(TorchTensorWorker, 96939df4f3ef187de192013901000000)\n",
      "\u001b[36m(TorchTensorWorker pid=720070)\u001b[0m Destructing NCCL group on actor: Actor(TorchTensorWorker, 2914ca080594a3ec0a023b1501000000)\n",
      "2025-06-02 12:18:01,946\tINFO compiled_dag_node.py:2273 -- Waiting for worker tasks to exit\n",
      "2025-06-02 12:18:01,946\tINFO compiled_dag_node.py:2276 -- Teardown complete\n",
      "\n",
      "test_torch_tensor_dag.py::test_torch_tensor_explicit_communicator[ray_start_regular0] 2025-06-02 12:18:04,978\tINFO worker.py:1929 -- Started a local Ray instance.\n",
      "\u001b[32mPASSED\u001b[0m\n",
      "test_torch_tensor_dag.py::test_torch_tensor_nccl_collective_ops[operation0-None-ray_start_regular0] 2025-06-02 12:18:09,568\tINFO worker.py:1929 -- Started a local Ray instance.\n",
      "2025-06-02 12:18:11,873\tINFO torch_tensor_nccl_channel.py:772 -- Creating NCCL group 9c2c39f1-f260-4de1-bc80-547ba7a6a903 on actors: [Actor(TorchTensorWorker, b215a16ba585cd3a034d58fc01000000), Actor(TorchTensorWorker, 4ba2074047ce0a047c028efe01000000)]\n",
      "2025-06-02 12:18:12,958\tINFO torch_tensor_nccl_channel.py:797 -- NCCL group initialized.\n",
      "2025-06-02 12:18:13,075\tINFO dag_node_operation.py:592 -- Writing compiled graph schedule visualization to compiled_graph_schedule.png\n",
      "\u001b[32mPASSED\u001b[0m2025-06-02 12:18:13,531\tINFO compiled_dag_node.py:2246 -- Tearing down compiled DAG\n",
      "2025-06-02 12:18:13,532\tINFO compiled_dag_node.py:2251 -- Cancelling compiled worker on actor: Actor(TorchTensorWorker, b215a16ba585cd3a034d58fc01000000)\n",
      "2025-06-02 12:18:13,532\tINFO compiled_dag_node.py:2251 -- Cancelling compiled worker on actor: Actor(TorchTensorWorker, 4ba2074047ce0a047c028efe01000000)\n",
      "\u001b[36m(TorchTensorWorker pid=721475)\u001b[0m Destructing NCCL group on actor: Actor(TorchTensorWorker, b215a16ba585cd3a034d58fc01000000)\n",
      "\u001b[36m(TorchTensorWorker pid=721476)\u001b[0m Destructing NCCL group on actor: Actor(TorchTensorWorker, 4ba2074047ce0a047c028efe01000000)\n",
      "2025-06-02 12:18:14,051\tINFO compiled_dag_node.py:2273 -- Waiting for worker tasks to exit\n",
      "2025-06-02 12:18:14,051\tINFO compiled_dag_node.py:2234 -- Killing actor: Actor(TorchTensorWorker, b215a16ba585cd3a034d58fc01000000)\n",
      "2025-06-02 12:18:14,052\tINFO compiled_dag_node.py:2234 -- Killing actor: Actor(TorchTensorWorker, 4ba2074047ce0a047c028efe01000000)\n",
      "2025-06-02 12:18:14,052\tINFO compiled_dag_node.py:2276 -- Teardown complete\n",
      "\n",
      "test_torch_tensor_dag.py::test_torch_tensor_nccl_collective_ops[operation1-ReduceOp.SUM-ray_start_regular0] 2025-06-02 12:18:17,091\tINFO worker.py:1929 -- Started a local Ray instance.\n",
      "2025-06-02 12:18:19,323\tINFO torch_tensor_nccl_channel.py:772 -- Creating NCCL group 30f671ab-05c8-43d1-969b-343861ef7b18 on actors: [Actor(TorchTensorWorker, 99aaff3acab399c182abaab701000000), Actor(TorchTensorWorker, 7228b9205d564f19fb2a465101000000)]\n",
      "2025-06-02 12:18:20,182\tINFO torch_tensor_nccl_channel.py:797 -- NCCL group initialized.\n",
      "2025-06-02 12:18:20,347\tINFO dag_node_operation.py:592 -- Writing compiled graph schedule visualization to compiled_graph_schedule.png\n",
      "\u001b[32mPASSED\u001b[0m2025-06-02 12:18:20,536\tINFO compiled_dag_node.py:2246 -- Tearing down compiled DAG\n",
      "2025-06-02 12:18:20,536\tINFO compiled_dag_node.py:2251 -- Cancelling compiled worker on actor: Actor(TorchTensorWorker, 99aaff3acab399c182abaab701000000)\n",
      "2025-06-02 12:18:20,537\tINFO compiled_dag_node.py:2251 -- Cancelling compiled worker on actor: Actor(TorchTensorWorker, 7228b9205d564f19fb2a465101000000)\n",
      "\u001b[36m(TorchTensorWorker pid=722230)\u001b[0m Destructing NCCL group on actor: Actor(TorchTensorWorker, 7228b9205d564f19fb2a465101000000)\n",
      "\u001b[36m(TorchTensorWorker pid=722229)\u001b[0m Destructing NCCL group on actor: Actor(TorchTensorWorker, 99aaff3acab399c182abaab701000000)\n",
      "2025-06-02 12:18:21,025\tINFO compiled_dag_node.py:2273 -- Waiting for worker tasks to exit\n",
      "2025-06-02 12:18:21,026\tINFO compiled_dag_node.py:2234 -- Killing actor: Actor(TorchTensorWorker, 99aaff3acab399c182abaab701000000)\n",
      "2025-06-02 12:18:21,026\tINFO compiled_dag_node.py:2234 -- Killing actor: Actor(TorchTensorWorker, 7228b9205d564f19fb2a465101000000)\n",
      "2025-06-02 12:18:21,026\tINFO compiled_dag_node.py:2276 -- Teardown complete\n",
      "\n",
      "test_torch_tensor_dag.py::test_torch_tensor_nccl_collective_ops[operation2-ReduceOp.PRODUCT-ray_start_regular0] 2025-06-02 12:18:23,897\tINFO worker.py:1929 -- Started a local Ray instance.\n",
      "2025-06-02 12:18:26,137\tINFO torch_tensor_nccl_channel.py:772 -- Creating NCCL group 8130ec2c-bb0b-45d8-b685-a26a7aec9b3b on actors: [Actor(TorchTensorWorker, d40609d5ddb47ee023d74ab801000000), Actor(TorchTensorWorker, b15d2963d036a0ca83eb6c5e01000000)]\n",
      "2025-06-02 12:18:26,996\tINFO torch_tensor_nccl_channel.py:797 -- NCCL group initialized.\n",
      "2025-06-02 12:18:27,162\tINFO dag_node_operation.py:592 -- Writing compiled graph schedule visualization to compiled_graph_schedule.png\n",
      "\u001b[32mPASSED\u001b[0m2025-06-02 12:18:27,353\tINFO compiled_dag_node.py:2246 -- Tearing down compiled DAG\n",
      "2025-06-02 12:18:27,353\tINFO compiled_dag_node.py:2251 -- Cancelling compiled worker on actor: Actor(TorchTensorWorker, d40609d5ddb47ee023d74ab801000000)\n",
      "2025-06-02 12:18:27,353\tINFO compiled_dag_node.py:2251 -- Cancelling compiled worker on actor: Actor(TorchTensorWorker, b15d2963d036a0ca83eb6c5e01000000)\n",
      "\u001b[36m(TorchTensorWorker pid=722957)\u001b[0m Destructing NCCL group on actor: Actor(TorchTensorWorker, d40609d5ddb47ee023d74ab801000000)\n",
      "\u001b[36m(TorchTensorWorker pid=722958)\u001b[0m Destructing NCCL group on actor: Actor(TorchTensorWorker, b15d2963d036a0ca83eb6c5e01000000)\n",
      "2025-06-02 12:18:27,842\tINFO compiled_dag_node.py:2273 -- Waiting for worker tasks to exit\n",
      "2025-06-02 12:18:27,842\tINFO compiled_dag_node.py:2234 -- Killing actor: Actor(TorchTensorWorker, d40609d5ddb47ee023d74ab801000000)\n",
      "2025-06-02 12:18:27,843\tINFO compiled_dag_node.py:2234 -- Killing actor: Actor(TorchTensorWorker, b15d2963d036a0ca83eb6c5e01000000)\n",
      "2025-06-02 12:18:27,843\tINFO compiled_dag_node.py:2276 -- Teardown complete\n",
      "\n",
      "test_torch_tensor_dag.py::test_torch_tensor_nccl_collective_ops[operation3-ReduceOp.MIN-ray_start_regular0] 2025-06-02 12:18:30,805\tINFO worker.py:1929 -- Started a local Ray instance.\n",
      "2025-06-02 12:18:33,038\tINFO torch_tensor_nccl_channel.py:772 -- Creating NCCL group 23db8b4c-1b42-46f6-8e3a-45b6eca18b5d on actors: [Actor(TorchTensorWorker, c791c2ddc5d1798f79df7d4901000000), Actor(TorchTensorWorker, eaa56bd5b3377d5a0cb3c7bc01000000)]\n",
      "2025-06-02 12:18:33,901\tINFO torch_tensor_nccl_channel.py:797 -- NCCL group initialized.\n",
      "2025-06-02 12:18:34,080\tINFO dag_node_operation.py:592 -- Writing compiled graph schedule visualization to compiled_graph_schedule.png\n",
      "\u001b[32mPASSED\u001b[0m2025-06-02 12:18:34,272\tINFO compiled_dag_node.py:2246 -- Tearing down compiled DAG\n",
      "2025-06-02 12:18:34,272\tINFO compiled_dag_node.py:2251 -- Cancelling compiled worker on actor: Actor(TorchTensorWorker, c791c2ddc5d1798f79df7d4901000000)\n",
      "2025-06-02 12:18:34,272\tINFO compiled_dag_node.py:2251 -- Cancelling compiled worker on actor: Actor(TorchTensorWorker, eaa56bd5b3377d5a0cb3c7bc01000000)\n",
      "\u001b[36m(TorchTensorWorker pid=723652)\u001b[0m Destructing NCCL group on actor: Actor(TorchTensorWorker, c791c2ddc5d1798f79df7d4901000000)\n",
      "\u001b[36m(TorchTensorWorker pid=723653)\u001b[0m Destructing NCCL group on actor: Actor(TorchTensorWorker, eaa56bd5b3377d5a0cb3c7bc01000000)\n",
      "2025-06-02 12:18:34,760\tINFO compiled_dag_node.py:2273 -- Waiting for worker tasks to exit\n",
      "2025-06-02 12:18:34,761\tINFO compiled_dag_node.py:2234 -- Killing actor: Actor(TorchTensorWorker, c791c2ddc5d1798f79df7d4901000000)\n",
      "2025-06-02 12:18:34,761\tINFO compiled_dag_node.py:2234 -- Killing actor: Actor(TorchTensorWorker, eaa56bd5b3377d5a0cb3c7bc01000000)\n",
      "2025-06-02 12:18:34,761\tINFO compiled_dag_node.py:2276 -- Teardown complete\n",
      "\n",
      "test_torch_tensor_dag.py::test_torch_tensor_nccl_collective_ops[operation4-ReduceOp.MAX-ray_start_regular0] 2025-06-02 12:18:38,834\tINFO worker.py:1929 -- Started a local Ray instance.\n",
      "2025-06-02 12:18:41,096\tINFO torch_tensor_nccl_channel.py:772 -- Creating NCCL group 3257cebc-f606-4d18-9f5b-9c41a8a19a7d on actors: [Actor(TorchTensorWorker, 2ddc05642b1ff7acf55158e701000000), Actor(TorchTensorWorker, aaca100f77bc626f4ce5ef2801000000)]\n",
      "2025-06-02 12:18:41,961\tINFO torch_tensor_nccl_channel.py:797 -- NCCL group initialized.\n",
      "2025-06-02 12:18:42,136\tINFO dag_node_operation.py:592 -- Writing compiled graph schedule visualization to compiled_graph_schedule.png\n",
      "\u001b[32mPASSED\u001b[0m2025-06-02 12:18:42,328\tINFO compiled_dag_node.py:2246 -- Tearing down compiled DAG\n",
      "2025-06-02 12:18:42,329\tINFO compiled_dag_node.py:2251 -- Cancelling compiled worker on actor: Actor(TorchTensorWorker, 2ddc05642b1ff7acf55158e701000000)\n",
      "2025-06-02 12:18:42,329\tINFO compiled_dag_node.py:2251 -- Cancelling compiled worker on actor: Actor(TorchTensorWorker, aaca100f77bc626f4ce5ef2801000000)\n",
      "\u001b[36m(TorchTensorWorker pid=724459)\u001b[0m Destructing NCCL group on actor: Actor(TorchTensorWorker, 2ddc05642b1ff7acf55158e701000000)\n",
      "\u001b[36m(TorchTensorWorker pid=724460)\u001b[0m Destructing NCCL group on actor: Actor(TorchTensorWorker, aaca100f77bc626f4ce5ef2801000000)\n",
      "2025-06-02 12:18:42,817\tINFO compiled_dag_node.py:2273 -- Waiting for worker tasks to exit\n",
      "2025-06-02 12:18:42,817\tINFO compiled_dag_node.py:2234 -- Killing actor: Actor(TorchTensorWorker, 2ddc05642b1ff7acf55158e701000000)\n",
      "2025-06-02 12:18:42,818\tINFO compiled_dag_node.py:2234 -- Killing actor: Actor(TorchTensorWorker, aaca100f77bc626f4ce5ef2801000000)\n",
      "2025-06-02 12:18:42,818\tINFO compiled_dag_node.py:2276 -- Teardown complete\n",
      "\n",
      "test_torch_tensor_dag.py::test_torch_tensor_nccl_collective_ops[operation5-ReduceOp.SUM-ray_start_regular0] 2025-06-02 12:18:45,768\tINFO worker.py:1929 -- Started a local Ray instance.\n",
      "2025-06-02 12:18:47,984\tINFO torch_tensor_nccl_channel.py:772 -- Creating NCCL group 2009b9c8-6180-470f-b4fb-44079ad5fb9a on actors: [Actor(TorchTensorWorker, ff002bb640b14c04ba7fd2f001000000), Actor(TorchTensorWorker, 641a9b3dd643064784e4b1b201000000)]\n",
      "2025-06-02 12:18:48,843\tINFO torch_tensor_nccl_channel.py:797 -- NCCL group initialized.\n",
      "2025-06-02 12:18:48,985\tINFO dag_node_operation.py:592 -- Writing compiled graph schedule visualization to compiled_graph_schedule.png\n",
      "\u001b[32mPASSED\u001b[0m2025-06-02 12:18:49,155\tINFO compiled_dag_node.py:2246 -- Tearing down compiled DAG\n",
      "2025-06-02 12:18:49,155\tINFO compiled_dag_node.py:2251 -- Cancelling compiled worker on actor: Actor(TorchTensorWorker, ff002bb640b14c04ba7fd2f001000000)\n",
      "2025-06-02 12:18:49,155\tINFO compiled_dag_node.py:2251 -- Cancelling compiled worker on actor: Actor(TorchTensorWorker, 641a9b3dd643064784e4b1b201000000)\n",
      "\u001b[36m(TorchTensorWorker pid=725159)\u001b[0m Destructing NCCL group on actor: Actor(TorchTensorWorker, 641a9b3dd643064784e4b1b201000000)\n",
      "\u001b[36m(TorchTensorWorker pid=725157)\u001b[0m Destructing NCCL group on actor: Actor(TorchTensorWorker, ff002bb640b14c04ba7fd2f001000000)\n",
      "2025-06-02 12:18:49,646\tINFO compiled_dag_node.py:2273 -- Waiting for worker tasks to exit\n",
      "2025-06-02 12:18:49,646\tINFO compiled_dag_node.py:2234 -- Killing actor: Actor(TorchTensorWorker, ff002bb640b14c04ba7fd2f001000000)\n",
      "2025-06-02 12:18:49,647\tINFO compiled_dag_node.py:2234 -- Killing actor: Actor(TorchTensorWorker, 641a9b3dd643064784e4b1b201000000)\n",
      "2025-06-02 12:18:49,647\tINFO compiled_dag_node.py:2276 -- Teardown complete\n",
      "\n",
      "test_torch_tensor_dag.py::test_torch_tensor_nccl_collective_ops[operation6-ReduceOp.PRODUCT-ray_start_regular0] 2025-06-02 12:18:53,491\tINFO worker.py:1929 -- Started a local Ray instance.\n",
      "2025-06-02 12:18:55,737\tINFO torch_tensor_nccl_channel.py:772 -- Creating NCCL group 52bbf657-0e07-401e-8852-7806a7b04dca on actors: [Actor(TorchTensorWorker, 9d5917bb81f586a94224945501000000), Actor(TorchTensorWorker, 0713b9fc9be4c3f21cab6bf801000000)]\n",
      "2025-06-02 12:18:56,602\tINFO torch_tensor_nccl_channel.py:797 -- NCCL group initialized.\n",
      "2025-06-02 12:18:56,746\tINFO dag_node_operation.py:592 -- Writing compiled graph schedule visualization to compiled_graph_schedule.png\n",
      "\u001b[32mPASSED\u001b[0m2025-06-02 12:18:56,939\tINFO compiled_dag_node.py:2246 -- Tearing down compiled DAG\n",
      "2025-06-02 12:18:56,940\tINFO compiled_dag_node.py:2251 -- Cancelling compiled worker on actor: Actor(TorchTensorWorker, 9d5917bb81f586a94224945501000000)\n",
      "2025-06-02 12:18:56,940\tINFO compiled_dag_node.py:2251 -- Cancelling compiled worker on actor: Actor(TorchTensorWorker, 0713b9fc9be4c3f21cab6bf801000000)\n",
      "\u001b[36m(TorchTensorWorker pid=725929)\u001b[0m Destructing NCCL group on actor: Actor(TorchTensorWorker, 9d5917bb81f586a94224945501000000)\n",
      "\u001b[36m(TorchTensorWorker pid=725931)\u001b[0m Destructing NCCL group on actor: Actor(TorchTensorWorker, 0713b9fc9be4c3f21cab6bf801000000)\n",
      "2025-06-02 12:18:57,429\tINFO compiled_dag_node.py:2273 -- Waiting for worker tasks to exit\n",
      "2025-06-02 12:18:57,429\tINFO compiled_dag_node.py:2234 -- Killing actor: Actor(TorchTensorWorker, 9d5917bb81f586a94224945501000000)\n",
      "2025-06-02 12:18:57,430\tINFO compiled_dag_node.py:2234 -- Killing actor: Actor(TorchTensorWorker, 0713b9fc9be4c3f21cab6bf801000000)\n",
      "2025-06-02 12:18:57,430\tINFO compiled_dag_node.py:2276 -- Teardown complete\n",
      "\n",
      "test_torch_tensor_dag.py::test_torch_tensor_nccl_collective_ops[operation7-ReduceOp.MIN-ray_start_regular0] 2025-06-02 12:19:00,477\tINFO worker.py:1929 -- Started a local Ray instance.\n",
      "2025-06-02 12:19:02,720\tINFO torch_tensor_nccl_channel.py:772 -- Creating NCCL group 11f1d951-d5c7-444d-80a6-ff48bdfa6f91 on actors: [Actor(TorchTensorWorker, 874a27b79aa8e0be389c444c01000000), Actor(TorchTensorWorker, 6a3715a3bdaad19741101d1001000000)]\n",
      "2025-06-02 12:19:03,590\tINFO torch_tensor_nccl_channel.py:797 -- NCCL group initialized.\n",
      "2025-06-02 12:19:03,757\tINFO dag_node_operation.py:592 -- Writing compiled graph schedule visualization to compiled_graph_schedule.png\n",
      "\u001b[32mPASSED\u001b[0m2025-06-02 12:19:03,946\tINFO compiled_dag_node.py:2246 -- Tearing down compiled DAG\n",
      "2025-06-02 12:19:03,946\tINFO compiled_dag_node.py:2251 -- Cancelling compiled worker on actor: Actor(TorchTensorWorker, 874a27b79aa8e0be389c444c01000000)\n",
      "2025-06-02 12:19:03,946\tINFO compiled_dag_node.py:2251 -- Cancelling compiled worker on actor: Actor(TorchTensorWorker, 6a3715a3bdaad19741101d1001000000)\n",
      "\u001b[36m(TorchTensorWorker pid=726595)\u001b[0m Destructing NCCL group on actor: Actor(TorchTensorWorker, 874a27b79aa8e0be389c444c01000000)\n",
      "\u001b[36m(TorchTensorWorker pid=726597)\u001b[0m Destructing NCCL group on actor: Actor(TorchTensorWorker, 6a3715a3bdaad19741101d1001000000)\n",
      "2025-06-02 12:19:04,468\tINFO compiled_dag_node.py:2273 -- Waiting for worker tasks to exit\n",
      "2025-06-02 12:19:04,468\tINFO compiled_dag_node.py:2234 -- Killing actor: Actor(TorchTensorWorker, 874a27b79aa8e0be389c444c01000000)\n",
      "2025-06-02 12:19:04,469\tINFO compiled_dag_node.py:2234 -- Killing actor: Actor(TorchTensorWorker, 6a3715a3bdaad19741101d1001000000)\n",
      "2025-06-02 12:19:04,469\tINFO compiled_dag_node.py:2276 -- Teardown complete\n",
      "\n",
      "test_torch_tensor_dag.py::test_torch_tensor_nccl_collective_ops[operation8-ReduceOp.MAX-ray_start_regular0] 2025-06-02 12:19:08,360\tINFO worker.py:1929 -- Started a local Ray instance.\n",
      "2025-06-02 12:19:10,620\tINFO torch_tensor_nccl_channel.py:772 -- Creating NCCL group f215f7fa-5347-46dc-83e9-7708504ae2f4 on actors: [Actor(TorchTensorWorker, 75dad5a03378a367a0ecbd9901000000), Actor(TorchTensorWorker, 86f8820de3ab554f7e0d72ef01000000)]\n",
      "2025-06-02 12:19:11,479\tINFO torch_tensor_nccl_channel.py:797 -- NCCL group initialized.\n",
      "2025-06-02 12:19:11,656\tINFO dag_node_operation.py:592 -- Writing compiled graph schedule visualization to compiled_graph_schedule.png\n",
      "\u001b[32mPASSED\u001b[0m2025-06-02 12:19:11,879\tINFO compiled_dag_node.py:2246 -- Tearing down compiled DAG\n",
      "2025-06-02 12:19:11,879\tINFO compiled_dag_node.py:2251 -- Cancelling compiled worker on actor: Actor(TorchTensorWorker, 75dad5a03378a367a0ecbd9901000000)\n",
      "2025-06-02 12:19:11,879\tINFO compiled_dag_node.py:2251 -- Cancelling compiled worker on actor: Actor(TorchTensorWorker, 86f8820de3ab554f7e0d72ef01000000)\n",
      "\u001b[36m(TorchTensorWorker pid=727370)\u001b[0m Destructing NCCL group on actor: Actor(TorchTensorWorker, 86f8820de3ab554f7e0d72ef01000000)\n",
      "\u001b[36m(TorchTensorWorker pid=727369)\u001b[0m Destructing NCCL group on actor: Actor(TorchTensorWorker, 75dad5a03378a367a0ecbd9901000000)\n",
      "2025-06-02 12:19:12,402\tINFO compiled_dag_node.py:2273 -- Waiting for worker tasks to exit\n",
      "2025-06-02 12:19:12,402\tINFO compiled_dag_node.py:2234 -- Killing actor: Actor(TorchTensorWorker, 75dad5a03378a367a0ecbd9901000000)\n",
      "2025-06-02 12:19:12,403\tINFO compiled_dag_node.py:2234 -- Killing actor: Actor(TorchTensorWorker, 86f8820de3ab554f7e0d72ef01000000)\n",
      "2025-06-02 12:19:12,403\tINFO compiled_dag_node.py:2276 -- Teardown complete\n",
      "\n",
      "test_torch_tensor_dag.py::test_torch_tensor_nccl_all_reduce_get_partial[ray_start_regular0] 2025-06-02 12:19:16,439\tINFO worker.py:1929 -- Started a local Ray instance.\n",
      "2025-06-02 12:19:18,695\tINFO torch_tensor_nccl_channel.py:772 -- Creating NCCL group 1f5f9b79-bf89-4a38-9058-7c31b2c2c4a8 on actors: [Actor(TorchTensorWorker, f4a77e8ab1ef28d8258c3f0201000000), Actor(TorchTensorWorker, 2675364410d835dd8dbed4f401000000)]\n",
      "2025-06-02 12:19:19,555\tINFO torch_tensor_nccl_channel.py:797 -- NCCL group initialized.\n",
      "2025-06-02 12:19:19,757\tINFO dag_node_operation.py:592 -- Writing compiled graph schedule visualization to compiled_graph_schedule.png\n",
      "\u001b[32mPASSED\u001b[0m2025-06-02 12:19:20,004\tINFO compiled_dag_node.py:2246 -- Tearing down compiled DAG\n",
      "2025-06-02 12:19:20,004\tINFO compiled_dag_node.py:2251 -- Cancelling compiled worker on actor: Actor(TorchTensorWorker, f4a77e8ab1ef28d8258c3f0201000000)\n",
      "2025-06-02 12:19:20,004\tINFO compiled_dag_node.py:2251 -- Cancelling compiled worker on actor: Actor(TorchTensorWorker, 2675364410d835dd8dbed4f401000000)\n",
      "\u001b[36m(TorchTensorWorker pid=728096)\u001b[0m Destructing NCCL group on actor: Actor(TorchTensorWorker, f4a77e8ab1ef28d8258c3f0201000000)\n",
      "\u001b[36m(TorchTensorWorker pid=728098)\u001b[0m Destructing NCCL group on actor: Actor(TorchTensorWorker, 2675364410d835dd8dbed4f401000000)\n",
      "2025-06-02 12:19:20,489\tINFO compiled_dag_node.py:2273 -- Waiting for worker tasks to exit\n",
      "2025-06-02 12:19:20,489\tINFO compiled_dag_node.py:2234 -- Killing actor: Actor(TorchTensorWorker, f4a77e8ab1ef28d8258c3f0201000000)\n",
      "2025-06-02 12:19:20,489\tINFO compiled_dag_node.py:2234 -- Killing actor: Actor(TorchTensorWorker, 2675364410d835dd8dbed4f401000000)\n",
      "2025-06-02 12:19:20,489\tINFO compiled_dag_node.py:2276 -- Teardown complete\n",
      "\n",
      "test_torch_tensor_dag.py::test_torch_tensor_nccl_all_reduce_wrong_shape[ray_start_regular0] 2025-06-02 12:19:23,565\tINFO worker.py:1929 -- Started a local Ray instance.\n",
      "2025-06-02 12:19:25,800\tINFO torch_tensor_nccl_channel.py:772 -- Creating NCCL group 3e39aa60-90b6-4a7d-9ffb-c8e799d44b9c on actors: [Actor(TorchTensorWorker, a3aea2f25ef7a8c08facd0bd01000000), Actor(TorchTensorWorker, 26d4708631b1ede248b1043c01000000)]\n",
      "2025-06-02 12:19:26,657\tINFO torch_tensor_nccl_channel.py:797 -- NCCL group initialized.\n",
      "2025-06-02 12:19:26,796\tINFO dag_node_operation.py:592 -- Writing compiled graph schedule visualization to compiled_graph_schedule.png\n",
      "\u001b[32mPASSED\u001b[0m2025-06-02 12:19:47,001\tINFO compiled_dag_node.py:2246 -- Tearing down compiled DAG\n",
      "2025-06-02 12:19:47,001\tINFO compiled_dag_node.py:2251 -- Cancelling compiled worker on actor: Actor(TorchTensorWorker, a3aea2f25ef7a8c08facd0bd01000000)\n",
      "2025-06-02 12:19:47,001\tINFO compiled_dag_node.py:2251 -- Cancelling compiled worker on actor: Actor(TorchTensorWorker, 26d4708631b1ede248b1043c01000000)\n",
      "\u001b[36m(TorchTensorWorker pid=728818)\u001b[0m Destructing NCCL group on actor: Actor(TorchTensorWorker, a3aea2f25ef7a8c08facd0bd01000000)\n",
      "\u001b[36m(TorchTensorWorker pid=728819)\u001b[0m Destructing NCCL group on actor: Actor(TorchTensorWorker, 26d4708631b1ede248b1043c01000000)\n",
      "2025-06-02 12:19:47,095\tINFO compiled_dag_node.py:2273 -- Waiting for worker tasks to exit\n",
      "2025-06-02 12:19:47,095\tINFO compiled_dag_node.py:2234 -- Killing actor: Actor(TorchTensorWorker, a3aea2f25ef7a8c08facd0bd01000000)\n",
      "2025-06-02 12:19:47,096\tINFO compiled_dag_node.py:2234 -- Killing actor: Actor(TorchTensorWorker, 26d4708631b1ede248b1043c01000000)\n",
      "2025-06-02 12:19:47,096\tINFO compiled_dag_node.py:2276 -- Teardown complete\n",
      "\n",
      "test_torch_tensor_dag.py::test_torch_tensor_nccl_all_reduce_custom_comm[ray_start_regular0] 2025-06-02 12:19:50,232\tINFO worker.py:1929 -- Started a local Ray instance.\n",
      "2025-06-02 12:19:52,476\tINFO torch_tensor_nccl_channel.py:770 -- Initializing custom NCCL group bba171f2-35c3-4a85-9312-89d10be4ab0a on actors: [Actor(TorchTensorWorker, b8121bdc9e8da54e4f6a0a2601000000), Actor(TorchTensorWorker, 30f593a05fd810569dd5c15601000000)]\n",
      "2025-06-02 12:19:53,249\tINFO torch_tensor_nccl_channel.py:797 -- NCCL group initialized.\n",
      "2025-06-02 12:19:53,388\tINFO dag_node_operation.py:592 -- Writing compiled graph schedule visualization to compiled_graph_schedule.png\n",
      "\u001b[32mPASSED\u001b[0m2025-06-02 12:19:53,622\tINFO compiled_dag_node.py:2246 -- Tearing down compiled DAG\n",
      "2025-06-02 12:19:53,622\tINFO compiled_dag_node.py:2251 -- Cancelling compiled worker on actor: Actor(TorchTensorWorker, b8121bdc9e8da54e4f6a0a2601000000)\n",
      "2025-06-02 12:19:53,622\tINFO compiled_dag_node.py:2251 -- Cancelling compiled worker on actor: Actor(TorchTensorWorker, 30f593a05fd810569dd5c15601000000)\n",
      "2025-06-02 12:19:53,633\tINFO compiled_dag_node.py:2273 -- Waiting for worker tasks to exit\n",
      "2025-06-02 12:19:53,633\tINFO compiled_dag_node.py:2234 -- Killing actor: Actor(TorchTensorWorker, b8121bdc9e8da54e4f6a0a2601000000)\n",
      "2025-06-02 12:19:53,633\tINFO compiled_dag_node.py:2234 -- Killing actor: Actor(TorchTensorWorker, 30f593a05fd810569dd5c15601000000)\n",
      "2025-06-02 12:19:53,633\tINFO compiled_dag_node.py:2276 -- Teardown complete\n",
      "\n",
      "test_torch_tensor_dag.py::test_torch_tensor_nccl_all_reduce_scheduling[ray_start_regular0] 2025-06-02 12:19:57,665\tINFO worker.py:1929 -- Started a local Ray instance.\n",
      "2025-06-02 12:19:59,890\tINFO torch_tensor_nccl_channel.py:772 -- Creating NCCL group e890df77-cb81-46e9-abfc-17c09a1a6e4c on actors: [Actor(TorchTensorWorker, a28d4e8ed7b7efd7945e4bf201000000), Actor(TorchTensorWorker, e89ede80bfb2d1127b6b702b01000000)]\n",
      "2025-06-02 12:20:00,797\tINFO torch_tensor_nccl_channel.py:797 -- NCCL group initialized.\n",
      "2025-06-02 12:20:00,983\tINFO dag_node_operation.py:592 -- Writing compiled graph schedule visualization to compiled_graph_schedule.png\n",
      "\u001b[32mPASSED\u001b[0m2025-06-02 12:20:01,217\tINFO compiled_dag_node.py:2246 -- Tearing down compiled DAG\n",
      "2025-06-02 12:20:01,218\tINFO compiled_dag_node.py:2251 -- Cancelling compiled worker on actor: Actor(TorchTensorWorker, a28d4e8ed7b7efd7945e4bf201000000)\n",
      "2025-06-02 12:20:01,218\tINFO compiled_dag_node.py:2251 -- Cancelling compiled worker on actor: Actor(TorchTensorWorker, e89ede80bfb2d1127b6b702b01000000)\n",
      "\u001b[36m(TorchTensorWorker pid=730376)\u001b[0m Destructing NCCL group on actor: Actor(TorchTensorWorker, e89ede80bfb2d1127b6b702b01000000)\n",
      "\u001b[36m(TorchTensorWorker pid=730377)\u001b[0m Destructing NCCL group on actor: Actor(TorchTensorWorker, a28d4e8ed7b7efd7945e4bf201000000)\n",
      "2025-06-02 12:20:01,743\tINFO compiled_dag_node.py:2273 -- Waiting for worker tasks to exit\n",
      "2025-06-02 12:20:01,744\tINFO compiled_dag_node.py:2234 -- Killing actor: Actor(TorchTensorWorker, a28d4e8ed7b7efd7945e4bf201000000)\n",
      "2025-06-02 12:20:01,744\tINFO compiled_dag_node.py:2234 -- Killing actor: Actor(TorchTensorWorker, e89ede80bfb2d1127b6b702b01000000)\n",
      "2025-06-02 12:20:01,744\tINFO compiled_dag_node.py:2276 -- Teardown complete\n",
      "\n",
      "test_torch_tensor_dag.py::test_nccl_all_reduce_with_class_method_output_node[ray_start_regular0] 2025-06-02 12:20:04,802\tINFO worker.py:1929 -- Started a local Ray instance.\n",
      "2025-06-02 12:20:07,084\tINFO torch_tensor_nccl_channel.py:772 -- Creating NCCL group dcd2d479-5bbd-4c62-be8e-bd45bbe3a6c4 on actors: [Actor(TorchTensorWorker, a2cd142c4a3afe962d70c87701000000), Actor(TorchTensorWorker, 4e9766c0e170b8353f62ba4601000000)]\n",
      "2025-06-02 12:20:07,941\tINFO torch_tensor_nccl_channel.py:797 -- NCCL group initialized.\n",
      "2025-06-02 12:20:08,224\tINFO dag_node_operation.py:592 -- Writing compiled graph schedule visualization to compiled_graph_schedule.png\n",
      "\u001b[32mPASSED\u001b[0m2025-06-02 12:20:08,459\tINFO compiled_dag_node.py:2246 -- Tearing down compiled DAG\n",
      "2025-06-02 12:20:08,460\tINFO compiled_dag_node.py:2251 -- Cancelling compiled worker on actor: Actor(TorchTensorWorker, a2cd142c4a3afe962d70c87701000000)\n",
      "2025-06-02 12:20:08,460\tINFO compiled_dag_node.py:2251 -- Cancelling compiled worker on actor: Actor(TorchTensorWorker, 4e9766c0e170b8353f62ba4601000000)\n",
      "\u001b[36m(TorchTensorWorker pid=731081)\u001b[0m Destructing NCCL group on actor: Actor(TorchTensorWorker, 4e9766c0e170b8353f62ba4601000000)\n",
      "\u001b[36m(TorchTensorWorker pid=731080)\u001b[0m Destructing NCCL group on actor: Actor(TorchTensorWorker, a2cd142c4a3afe962d70c87701000000)\n",
      "2025-06-02 12:20:08,983\tINFO compiled_dag_node.py:2273 -- Waiting for worker tasks to exit\n",
      "2025-06-02 12:20:08,984\tINFO compiled_dag_node.py:2234 -- Killing actor: Actor(TorchTensorWorker, a2cd142c4a3afe962d70c87701000000)\n",
      "2025-06-02 12:20:08,984\tINFO compiled_dag_node.py:2234 -- Killing actor: Actor(TorchTensorWorker, 4e9766c0e170b8353f62ba4601000000)\n",
      "2025-06-02 12:20:08,984\tINFO compiled_dag_node.py:2276 -- Teardown complete\n",
      "\n",
      "test_torch_tensor_dag.py::test_tensor_writable_warning_suppressed[ray_start_regular0] 2025-06-02 12:20:13,052\tINFO worker.py:1929 -- Started a local Ray instance.\n",
      "2025-06-02 12:20:13,610\tINFO dag_node_operation.py:592 -- Writing compiled graph schedule visualization to compiled_graph_schedule.png\n",
      "2025-06-02 12:20:18,088\tINFO compiled_dag_node.py:2246 -- Tearing down compiled DAG\n",
      "2025-06-02 12:20:18,089\tINFO compiled_dag_node.py:2251 -- Cancelling compiled worker on actor: Actor(test_tensor_writable_warning_suppressed.<locals>.A, d9a578dd4325fd3e73659f0401000000)\n",
      "2025-06-02 12:20:18,094\tINFO compiled_dag_node.py:2273 -- Waiting for worker tasks to exit\n",
      "2025-06-02 12:20:18,094\tINFO compiled_dag_node.py:2276 -- Teardown complete\n",
      "\u001b[32mPASSED\u001b[0m\n",
      "test_torch_tensor_dag.py::test_torch_nccl_channel_with_local_reader[ray_start_regular0] 2025-06-02 12:20:20,919\tINFO worker.py:1929 -- Started a local Ray instance.\n",
      "2025-06-02 12:20:23,199\tINFO torch_tensor_nccl_channel.py:772 -- Creating NCCL group 062e664f-27b6-4d13-bdaf-edf071c96895 on actors: [Actor(TorchTensorWorker, f150fc70abb693425acdcc1201000000), Actor(TorchTensorWorker, 4975d2cc0615a24280c46cde01000000)]\n",
      "2025-06-02 12:20:24,054\tINFO torch_tensor_nccl_channel.py:797 -- NCCL group initialized.\n",
      "2025-06-02 12:20:24,270\tINFO dag_node_operation.py:592 -- Writing compiled graph schedule visualization to compiled_graph_schedule.png\n",
      "\u001b[32mPASSED\u001b[0m2025-06-02 12:20:24,453\tINFO compiled_dag_node.py:2246 -- Tearing down compiled DAG\n",
      "2025-06-02 12:20:24,453\tINFO compiled_dag_node.py:2251 -- Cancelling compiled worker on actor: Actor(TorchTensorWorker, 4975d2cc0615a24280c46cde01000000)\n",
      "2025-06-02 12:20:24,453\tINFO compiled_dag_node.py:2251 -- Cancelling compiled worker on actor: Actor(TorchTensorWorker, f150fc70abb693425acdcc1201000000)\n",
      "\u001b[36m(TorchTensorWorker pid=732394)\u001b[0m Destructing NCCL group on actor: Actor(TorchTensorWorker, 4975d2cc0615a24280c46cde01000000)\n",
      "\u001b[36m(TorchTensorWorker pid=732396)\u001b[0m Destructing NCCL group on actor: Actor(TorchTensorWorker, f150fc70abb693425acdcc1201000000)\n",
      "2025-06-02 12:20:24,972\tINFO compiled_dag_node.py:2273 -- Waiting for worker tasks to exit\n",
      "2025-06-02 12:20:24,972\tINFO compiled_dag_node.py:2234 -- Killing actor: Actor(TorchTensorWorker, 4975d2cc0615a24280c46cde01000000)\n",
      "2025-06-02 12:20:24,972\tINFO compiled_dag_node.py:2234 -- Killing actor: Actor(TorchTensorWorker, f150fc70abb693425acdcc1201000000)\n",
      "2025-06-02 12:20:24,973\tINFO compiled_dag_node.py:2276 -- Teardown complete\n",
      "\n",
      "test_torch_tensor_dag.py::test_torch_nccl_channel_with_two_local_readers[ray_start_regular0] 2025-06-02 12:20:28,852\tINFO worker.py:1929 -- Started a local Ray instance.\n",
      "2025-06-02 12:20:31,063\tINFO torch_tensor_nccl_channel.py:772 -- Creating NCCL group 76a5a401-2271-4422-a6dc-864995bac53c on actors: [Actor(TorchTensorWorker, 34f61263b156561935108d2201000000), Actor(TorchTensorWorker, a02dfc7de47382429ec2b9dd01000000)]\n",
      "2025-06-02 12:20:31,965\tINFO torch_tensor_nccl_channel.py:797 -- NCCL group initialized.\n",
      "2025-06-02 12:20:32,178\tINFO dag_node_operation.py:592 -- Writing compiled graph schedule visualization to compiled_graph_schedule.png\n",
      "\u001b[32mPASSED\u001b[0m2025-06-02 12:20:32,380\tINFO compiled_dag_node.py:2246 -- Tearing down compiled DAG\n",
      "2025-06-02 12:20:32,380\tINFO compiled_dag_node.py:2251 -- Cancelling compiled worker on actor: Actor(TorchTensorWorker, a02dfc7de47382429ec2b9dd01000000)\n",
      "2025-06-02 12:20:32,380\tINFO compiled_dag_node.py:2251 -- Cancelling compiled worker on actor: Actor(TorchTensorWorker, 34f61263b156561935108d2201000000)\n",
      "\u001b[36m(TorchTensorWorker pid=733149)\u001b[0m Destructing NCCL group on actor: Actor(TorchTensorWorker, 34f61263b156561935108d2201000000)\n",
      "\u001b[36m(TorchTensorWorker pid=733148)\u001b[0m Destructing NCCL group on actor: Actor(TorchTensorWorker, a02dfc7de47382429ec2b9dd01000000)\n",
      "2025-06-02 12:20:32,873\tINFO compiled_dag_node.py:2273 -- Waiting for worker tasks to exit\n",
      "2025-06-02 12:20:32,874\tINFO compiled_dag_node.py:2234 -- Killing actor: Actor(TorchTensorWorker, a02dfc7de47382429ec2b9dd01000000)\n",
      "2025-06-02 12:20:32,874\tINFO compiled_dag_node.py:2234 -- Killing actor: Actor(TorchTensorWorker, 34f61263b156561935108d2201000000)\n",
      "2025-06-02 12:20:32,874\tINFO compiled_dag_node.py:2276 -- Teardown complete\n",
      "\n",
      "test_torch_tensor_dag.py::test_torch_nccl_channel_with_all_local_readers[ray_start_regular0] 2025-06-02 12:20:35,996\tINFO worker.py:1929 -- Started a local Ray instance.\n",
      "2025-06-02 12:20:38,106\tINFO torch_tensor_nccl_channel.py:772 -- Creating NCCL group 5fb48983-d6bf-42fd-86f5-1b7bd79f0b7e on actors: [Actor(TorchTensorWorker, 67335842f15e081aa5374c1101000000)]\n",
      "2025-06-02 12:20:38,595\tINFO torch_tensor_nccl_channel.py:797 -- NCCL group initialized.\n",
      "\u001b[32mPASSED\u001b[0m\n",
      "\n",
      "=================================== FAILURES ===================================\n",
      "\u001b[31m\u001b[1m________ test_torch_tensor_nccl_overlap_timed[ray_start_regular0-False] ________\u001b[0m\n",
      "\n",
      "ray_start_regular = RayContext(dashboard_url='', python_version='3.13.3', ray_version='3.0.0.dev0', ray_commit='{{RAY_COMMIT_SHA}}')\n",
      "overlap_gpu_communication = False\n",
      "\n",
      "    \u001b[0m\u001b[37m@pytest\u001b[39;49;00m.mark.skipif(\u001b[95mnot\u001b[39;49;00m USE_GPU, reason=\u001b[33m\"\u001b[39;49;00m\u001b[33mSkipping GPU Test\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[33m\"\u001b[39;49;00m\u001b[33mray_start_regular, overlap_gpu_communication\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m,\u001b[90m\u001b[39;49;00m\n",
      "        [({\u001b[33m\"\u001b[39;49;00m\u001b[33mnum_cpus\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m: \u001b[94m4\u001b[39;49;00m}, \u001b[94mFalse\u001b[39;49;00m), ({\u001b[33m\"\u001b[39;49;00m\u001b[33mnum_cpus\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m: \u001b[94m4\u001b[39;49;00m}, \u001b[94mTrue\u001b[39;49;00m)],\u001b[90m\u001b[39;49;00m\n",
      "        indirect=[\u001b[33m\"\u001b[39;49;00m\u001b[33mray_start_regular\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m],\u001b[90m\u001b[39;49;00m\n",
      "    )\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mtest_torch_tensor_nccl_overlap_timed\u001b[39;49;00m(ray_start_regular, overlap_gpu_communication):\u001b[90m\u001b[39;49;00m\n",
      ">       \u001b[94massert\u001b[39;49;00m (\u001b[90m\u001b[39;49;00m\n",
      "            \u001b[96msum\u001b[39;49;00m(node[\u001b[33m\"\u001b[39;49;00m\u001b[33mResources\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m].get(\u001b[33m\"\u001b[39;49;00m\u001b[33mGPU\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[94m0\u001b[39;49;00m) \u001b[94mfor\u001b[39;49;00m node \u001b[95min\u001b[39;49;00m ray.nodes()) >= \u001b[94m4\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      "        ), \u001b[33m\"\u001b[39;49;00m\u001b[33mThis test requires at least 4 GPUs\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      "\u001b[1m\u001b[31mE       AssertionError: This test requires at least 4 GPUs\u001b[0m\n",
      "\u001b[1m\u001b[31mE       assert 2.0 >= 4\u001b[0m\n",
      "\u001b[1m\u001b[31mE        +  where 2.0 = sum(<generator object test_torch_tensor_nccl_overlap_timed.<locals>.<genexpr> at 0x745a2839fbc0>)\u001b[0m\n",
      "\n",
      "\u001b[1m\u001b[31mtest_torch_tensor_dag.py\u001b[0m:379: AssertionError\n",
      "\u001b[31m\u001b[1m________ test_torch_tensor_nccl_overlap_timed[ray_start_regular1-True] _________\u001b[0m\n",
      "\n",
      "ray_start_regular = RayContext(dashboard_url='', python_version='3.13.3', ray_version='3.0.0.dev0', ray_commit='{{RAY_COMMIT_SHA}}')\n",
      "overlap_gpu_communication = True\n",
      "\n",
      "    \u001b[0m\u001b[37m@pytest\u001b[39;49;00m.mark.skipif(\u001b[95mnot\u001b[39;49;00m USE_GPU, reason=\u001b[33m\"\u001b[39;49;00m\u001b[33mSkipping GPU Test\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[33m\"\u001b[39;49;00m\u001b[33mray_start_regular, overlap_gpu_communication\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m,\u001b[90m\u001b[39;49;00m\n",
      "        [({\u001b[33m\"\u001b[39;49;00m\u001b[33mnum_cpus\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m: \u001b[94m4\u001b[39;49;00m}, \u001b[94mFalse\u001b[39;49;00m), ({\u001b[33m\"\u001b[39;49;00m\u001b[33mnum_cpus\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m: \u001b[94m4\u001b[39;49;00m}, \u001b[94mTrue\u001b[39;49;00m)],\u001b[90m\u001b[39;49;00m\n",
      "        indirect=[\u001b[33m\"\u001b[39;49;00m\u001b[33mray_start_regular\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m],\u001b[90m\u001b[39;49;00m\n",
      "    )\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mtest_torch_tensor_nccl_overlap_timed\u001b[39;49;00m(ray_start_regular, overlap_gpu_communication):\u001b[90m\u001b[39;49;00m\n",
      ">       \u001b[94massert\u001b[39;49;00m (\u001b[90m\u001b[39;49;00m\n",
      "            \u001b[96msum\u001b[39;49;00m(node[\u001b[33m\"\u001b[39;49;00m\u001b[33mResources\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m].get(\u001b[33m\"\u001b[39;49;00m\u001b[33mGPU\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[94m0\u001b[39;49;00m) \u001b[94mfor\u001b[39;49;00m node \u001b[95min\u001b[39;49;00m ray.nodes()) >= \u001b[94m4\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      "        ), \u001b[33m\"\u001b[39;49;00m\u001b[33mThis test requires at least 4 GPUs\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      "\u001b[1m\u001b[31mE       AssertionError: This test requires at least 4 GPUs\u001b[0m\n",
      "\u001b[1m\u001b[31mE       assert 2.0 >= 4\u001b[0m\n",
      "\u001b[1m\u001b[31mE        +  where 2.0 = sum(<generator object test_torch_tensor_nccl_overlap_timed.<locals>.<genexpr> at 0x745a2839fae0>)\u001b[0m\n",
      "\n",
      "\u001b[1m\u001b[31mtest_torch_tensor_dag.py\u001b[0m:379: AssertionError\n",
      "\u001b[31m\u001b[1m________ test_torch_tensor_default_comm[transports0-ray_start_regular0] ________\u001b[0m\n",
      "\n",
      "ray_start_regular = RayContext(dashboard_url='', python_version='3.13.3', ray_version='3.0.0.dev0', ray_commit='{{RAY_COMMIT_SHA}}')\n",
      "transports = ['auto', 'nccl']\n",
      "\n",
      "    \u001b[0m\u001b[37m@pytest\u001b[39;49;00m.mark.skipif(\u001b[95mnot\u001b[39;49;00m USE_GPU, reason=\u001b[33m\"\u001b[39;49;00m\u001b[33mSkipping GPU Test\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mray_start_regular\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [{\u001b[33m\"\u001b[39;49;00m\u001b[33mnum_cpus\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m: \u001b[94m4\u001b[39;49;00m}], indirect=\u001b[94mTrue\u001b[39;49;00m)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[33m\"\u001b[39;49;00m\u001b[33mtransports\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m,\u001b[90m\u001b[39;49;00m\n",
      "        [[\u001b[33m\"\u001b[39;49;00m\u001b[33mauto\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33mnccl\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m], [\u001b[33m\"\u001b[39;49;00m\u001b[33mcustom\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33mnccl\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m], [\u001b[33m\"\u001b[39;49;00m\u001b[33mauto\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33mnccl\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m], [\u001b[33m\"\u001b[39;49;00m\u001b[33mcustom\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33mcustom\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m]],\u001b[90m\u001b[39;49;00m\n",
      "    )\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mtest_torch_tensor_default_comm\u001b[39;49;00m(ray_start_regular, transports):\u001b[90m\u001b[39;49;00m\n",
      ">       \u001b[94massert\u001b[39;49;00m (\u001b[90m\u001b[39;49;00m\n",
      "            \u001b[96msum\u001b[39;49;00m(node[\u001b[33m\"\u001b[39;49;00m\u001b[33mResources\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m].get(\u001b[33m\"\u001b[39;49;00m\u001b[33mGPU\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[94m0\u001b[39;49;00m) \u001b[94mfor\u001b[39;49;00m node \u001b[95min\u001b[39;49;00m ray.nodes()) > \u001b[94m2\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      "        ), \u001b[33m\"\u001b[39;49;00m\u001b[33mThis test requires at least 3 GPUs\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      "\u001b[1m\u001b[31mE       AssertionError: This test requires at least 3 GPUs\u001b[0m\n",
      "\u001b[1m\u001b[31mE       assert 2.0 > 2\u001b[0m\n",
      "\u001b[1m\u001b[31mE        +  where 2.0 = sum(<generator object test_torch_tensor_default_comm.<locals>.<genexpr> at 0x745a2695f060>)\u001b[0m\n",
      "\n",
      "\u001b[1m\u001b[31mtest_torch_tensor_dag.py\u001b[0m:751: AssertionError\n",
      "\u001b[31m\u001b[1m________ test_torch_tensor_default_comm[transports1-ray_start_regular0] ________\u001b[0m\n",
      "\n",
      "ray_start_regular = RayContext(dashboard_url='', python_version='3.13.3', ray_version='3.0.0.dev0', ray_commit='{{RAY_COMMIT_SHA}}')\n",
      "transports = ['custom', 'nccl']\n",
      "\n",
      "    \u001b[0m\u001b[37m@pytest\u001b[39;49;00m.mark.skipif(\u001b[95mnot\u001b[39;49;00m USE_GPU, reason=\u001b[33m\"\u001b[39;49;00m\u001b[33mSkipping GPU Test\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mray_start_regular\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [{\u001b[33m\"\u001b[39;49;00m\u001b[33mnum_cpus\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m: \u001b[94m4\u001b[39;49;00m}], indirect=\u001b[94mTrue\u001b[39;49;00m)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[33m\"\u001b[39;49;00m\u001b[33mtransports\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m,\u001b[90m\u001b[39;49;00m\n",
      "        [[\u001b[33m\"\u001b[39;49;00m\u001b[33mauto\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33mnccl\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m], [\u001b[33m\"\u001b[39;49;00m\u001b[33mcustom\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33mnccl\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m], [\u001b[33m\"\u001b[39;49;00m\u001b[33mauto\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33mnccl\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m], [\u001b[33m\"\u001b[39;49;00m\u001b[33mcustom\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33mcustom\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m]],\u001b[90m\u001b[39;49;00m\n",
      "    )\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mtest_torch_tensor_default_comm\u001b[39;49;00m(ray_start_regular, transports):\u001b[90m\u001b[39;49;00m\n",
      ">       \u001b[94massert\u001b[39;49;00m (\u001b[90m\u001b[39;49;00m\n",
      "            \u001b[96msum\u001b[39;49;00m(node[\u001b[33m\"\u001b[39;49;00m\u001b[33mResources\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m].get(\u001b[33m\"\u001b[39;49;00m\u001b[33mGPU\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[94m0\u001b[39;49;00m) \u001b[94mfor\u001b[39;49;00m node \u001b[95min\u001b[39;49;00m ray.nodes()) > \u001b[94m2\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      "        ), \u001b[33m\"\u001b[39;49;00m\u001b[33mThis test requires at least 3 GPUs\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      "\u001b[1m\u001b[31mE       AssertionError: This test requires at least 3 GPUs\u001b[0m\n",
      "\u001b[1m\u001b[31mE       assert 2.0 > 2\u001b[0m\n",
      "\u001b[1m\u001b[31mE        +  where 2.0 = sum(<generator object test_torch_tensor_default_comm.<locals>.<genexpr> at 0x745a2695fa00>)\u001b[0m\n",
      "\n",
      "\u001b[1m\u001b[31mtest_torch_tensor_dag.py\u001b[0m:751: AssertionError\n",
      "\u001b[31m\u001b[1m________ test_torch_tensor_default_comm[transports2-ray_start_regular0] ________\u001b[0m\n",
      "\n",
      "ray_start_regular = RayContext(dashboard_url='', python_version='3.13.3', ray_version='3.0.0.dev0', ray_commit='{{RAY_COMMIT_SHA}}')\n",
      "transports = ['auto', 'nccl']\n",
      "\n",
      "    \u001b[0m\u001b[37m@pytest\u001b[39;49;00m.mark.skipif(\u001b[95mnot\u001b[39;49;00m USE_GPU, reason=\u001b[33m\"\u001b[39;49;00m\u001b[33mSkipping GPU Test\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mray_start_regular\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [{\u001b[33m\"\u001b[39;49;00m\u001b[33mnum_cpus\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m: \u001b[94m4\u001b[39;49;00m}], indirect=\u001b[94mTrue\u001b[39;49;00m)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[33m\"\u001b[39;49;00m\u001b[33mtransports\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m,\u001b[90m\u001b[39;49;00m\n",
      "        [[\u001b[33m\"\u001b[39;49;00m\u001b[33mauto\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33mnccl\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m], [\u001b[33m\"\u001b[39;49;00m\u001b[33mcustom\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33mnccl\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m], [\u001b[33m\"\u001b[39;49;00m\u001b[33mauto\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33mnccl\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m], [\u001b[33m\"\u001b[39;49;00m\u001b[33mcustom\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33mcustom\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m]],\u001b[90m\u001b[39;49;00m\n",
      "    )\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mtest_torch_tensor_default_comm\u001b[39;49;00m(ray_start_regular, transports):\u001b[90m\u001b[39;49;00m\n",
      ">       \u001b[94massert\u001b[39;49;00m (\u001b[90m\u001b[39;49;00m\n",
      "            \u001b[96msum\u001b[39;49;00m(node[\u001b[33m\"\u001b[39;49;00m\u001b[33mResources\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m].get(\u001b[33m\"\u001b[39;49;00m\u001b[33mGPU\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[94m0\u001b[39;49;00m) \u001b[94mfor\u001b[39;49;00m node \u001b[95min\u001b[39;49;00m ray.nodes()) > \u001b[94m2\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      "        ), \u001b[33m\"\u001b[39;49;00m\u001b[33mThis test requires at least 3 GPUs\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      "\u001b[1m\u001b[31mE       AssertionError: This test requires at least 3 GPUs\u001b[0m\n",
      "\u001b[1m\u001b[31mE       assert 2.0 > 2\u001b[0m\n",
      "\u001b[1m\u001b[31mE        +  where 2.0 = sum(<generator object test_torch_tensor_default_comm.<locals>.<genexpr> at 0x745a2695edc0>)\u001b[0m\n",
      "\n",
      "\u001b[1m\u001b[31mtest_torch_tensor_dag.py\u001b[0m:751: AssertionError\n",
      "\u001b[31m\u001b[1m________ test_torch_tensor_default_comm[transports3-ray_start_regular0] ________\u001b[0m\n",
      "\n",
      "ray_start_regular = RayContext(dashboard_url='', python_version='3.13.3', ray_version='3.0.0.dev0', ray_commit='{{RAY_COMMIT_SHA}}')\n",
      "transports = ['custom', 'custom']\n",
      "\n",
      "    \u001b[0m\u001b[37m@pytest\u001b[39;49;00m.mark.skipif(\u001b[95mnot\u001b[39;49;00m USE_GPU, reason=\u001b[33m\"\u001b[39;49;00m\u001b[33mSkipping GPU Test\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mray_start_regular\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [{\u001b[33m\"\u001b[39;49;00m\u001b[33mnum_cpus\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m: \u001b[94m4\u001b[39;49;00m}], indirect=\u001b[94mTrue\u001b[39;49;00m)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[33m\"\u001b[39;49;00m\u001b[33mtransports\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m,\u001b[90m\u001b[39;49;00m\n",
      "        [[\u001b[33m\"\u001b[39;49;00m\u001b[33mauto\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33mnccl\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m], [\u001b[33m\"\u001b[39;49;00m\u001b[33mcustom\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33mnccl\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m], [\u001b[33m\"\u001b[39;49;00m\u001b[33mauto\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33mnccl\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m], [\u001b[33m\"\u001b[39;49;00m\u001b[33mcustom\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33mcustom\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m]],\u001b[90m\u001b[39;49;00m\n",
      "    )\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mtest_torch_tensor_default_comm\u001b[39;49;00m(ray_start_regular, transports):\u001b[90m\u001b[39;49;00m\n",
      ">       \u001b[94massert\u001b[39;49;00m (\u001b[90m\u001b[39;49;00m\n",
      "            \u001b[96msum\u001b[39;49;00m(node[\u001b[33m\"\u001b[39;49;00m\u001b[33mResources\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m].get(\u001b[33m\"\u001b[39;49;00m\u001b[33mGPU\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[94m0\u001b[39;49;00m) \u001b[94mfor\u001b[39;49;00m node \u001b[95min\u001b[39;49;00m ray.nodes()) > \u001b[94m2\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      "        ), \u001b[33m\"\u001b[39;49;00m\u001b[33mThis test requires at least 3 GPUs\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      "\u001b[1m\u001b[31mE       AssertionError: This test requires at least 3 GPUs\u001b[0m\n",
      "\u001b[1m\u001b[31mE       assert 2.0 > 2\u001b[0m\n",
      "\u001b[1m\u001b[31mE        +  where 2.0 = sum(<generator object test_torch_tensor_default_comm.<locals>.<genexpr> at 0x745a268411c0>)\u001b[0m\n",
      "\n",
      "\u001b[1m\u001b[31mtest_torch_tensor_dag.py\u001b[0m:751: AssertionError\n",
      "\u001b[36m\u001b[1m=========================== short test summary info ============================\u001b[0m\n",
      "\u001b[31mFAILED\u001b[0m test_torch_tensor_dag.py::\u001b[1mtest_torch_tensor_nccl_overlap_timed[ray_start_regular0-False]\u001b[0m - AssertionError: This test requires at least 4 GPUs\n",
      "\u001b[31mFAILED\u001b[0m test_torch_tensor_dag.py::\u001b[1mtest_torch_tensor_nccl_overlap_timed[ray_start_regular1-True]\u001b[0m - AssertionError: This test requires at least 4 GPUs\n",
      "\u001b[31mFAILED\u001b[0m test_torch_tensor_dag.py::\u001b[1mtest_torch_tensor_default_comm[transports0-ray_start_regular0]\u001b[0m - AssertionError: This test requires at least 3 GPUs\n",
      "\u001b[31mFAILED\u001b[0m test_torch_tensor_dag.py::\u001b[1mtest_torch_tensor_default_comm[transports1-ray_start_regular0]\u001b[0m - AssertionError: This test requires at least 3 GPUs\n",
      "\u001b[31mFAILED\u001b[0m test_torch_tensor_dag.py::\u001b[1mtest_torch_tensor_default_comm[transports2-ray_start_regular0]\u001b[0m - AssertionError: This test requires at least 3 GPUs\n",
      "\u001b[31mFAILED\u001b[0m test_torch_tensor_dag.py::\u001b[1mtest_torch_tensor_default_comm[transports3-ray_start_regular0]\u001b[0m - AssertionError: This test requires at least 3 GPUs\n",
      "\u001b[31m============= \u001b[31m\u001b[1m6 failed\u001b[0m, \u001b[32m45 passed\u001b[0m, \u001b[33m2 skipped\u001b[0m\u001b[31m in 374.79s (0:06:14)\u001b[0m\u001b[31m ==============\u001b[0m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!python -m pytest -v -s test_torch_tensor_dag.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "# !python -m pytest -v -s test_compiled_graphs.py::test_event_profiling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "# !python -m pytest -v -s test_cpu_communicator_dag.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "# !python -m pytest -v -s test_execution_schedule_gpu.py"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "stbr-coll-sched-0512",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
