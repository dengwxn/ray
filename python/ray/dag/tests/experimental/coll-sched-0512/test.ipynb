{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.chdir(\"/home/wxdeng/stbr-min/python/ray/dag/tests/experimental\")\n",
    "os.environ[\"RAY_DEDUP_LOGS\"] = \"0\"\n",
    "os.environ[\"RAY_PYTEST_USE_GPU\"] = \"1\"\n",
    "os.environ[\"RAY_CGRAPH_VISUALIZE_SCHEDULE\"] = \"1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "# !python -m coll-sched-0512.ddp --name ddp_bench"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "# !python -m coll-sched-0512.ddp --name ddp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "# !python -m pytest -v -s test_coll_sched.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m============================= test session starts ==============================\u001b[0m\n",
      "platform linux -- Python 3.13.3, pytest-8.3.5, pluggy-1.5.0 -- /home/wxdeng/miniconda3/envs/stbr-unify/bin/python\n",
      "cachedir: .pytest_cache\n",
      "rootdir: /home/wxdeng/stbr-min\n",
      "configfile: pytest.ini\n",
      "plugins: anyio-4.9.0\n",
      "collected 2 items                                                              \u001b[0m\u001b[1m\n",
      "\n",
      "test_collective_dag.py::test_exec_schedules_ddp[2-ray_start_regular0] 2025-05-14 14:21:10,633\tINFO worker.py:1879 -- Started a local Ray instance. View the dashboard at \u001b[1m\u001b[32mhttp://127.0.0.1:8265 \u001b[39m\u001b[22m\n",
      "2025-05-14 14:21:11,343\tINFO torch_tensor_nccl_channel.py:770 -- Initializing custom NCCL group cee0b33d-d806-4c01-8220-cf20f2618f88 on actors: [Actor(DDPWorker, edd9abf10ca84191f4085d1601000000), Actor(DDPWorker, 4d9cb0d14146328f1a43d59801000000)]\n",
      "2025-05-14 14:21:12,975\tINFO torch_tensor_nccl_channel.py:797 -- NCCL group initialized.\n",
      "2025-05-14 14:21:13,152\tINFO dag_node_operation.py:618 -- Writing compiled graph schedule visualization to compiled_graph_schedule.png\n",
      "\u001b[32mPASSED\u001b[0m2025-05-14 14:21:13,460\tINFO compiled_dag_node.py:2173 -- Tearing down compiled DAG\n",
      "2025-05-14 14:21:13,467\tINFO compiled_dag_node.py:2178 -- Cancelling compiled worker on actor: Actor(DDPWorker, edd9abf10ca84191f4085d1601000000)\n",
      "2025-05-14 14:21:13,467\tINFO compiled_dag_node.py:2178 -- Cancelling compiled worker on actor: Actor(DDPWorker, 4d9cb0d14146328f1a43d59801000000)\n",
      "2025-05-14 14:21:13,475\tINFO compiled_dag_node.py:2200 -- Waiting for worker tasks to exit\n",
      "2025-05-14 14:21:13,475\tINFO compiled_dag_node.py:2161 -- Killing actor: Actor(DDPWorker, edd9abf10ca84191f4085d1601000000)\n",
      "2025-05-14 14:21:13,475\tINFO compiled_dag_node.py:2161 -- Killing actor: Actor(DDPWorker, 4d9cb0d14146328f1a43d59801000000)\n",
      "2025-05-14 14:21:13,475\tINFO compiled_dag_node.py:2203 -- Teardown complete\n",
      "\n",
      "test_collective_dag.py::test_exec_schedules_ddp[4-ray_start_regular0] 2025-05-14 14:21:16,818\tINFO worker.py:1879 -- Started a local Ray instance. View the dashboard at \u001b[1m\u001b[32mhttp://127.0.0.1:8265 \u001b[39m\u001b[22m\n",
      "2025-05-14 14:21:17,796\tINFO torch_tensor_nccl_channel.py:770 -- Initializing custom NCCL group dd23a438-59db-438e-b055-d22e2919fa0c on actors: [Actor(DDPWorker, 12613a5c377887c20440b0a301000000), Actor(DDPWorker, fb5b890af1d0720a888a735601000000), Actor(DDPWorker, de07f8b50e94fbb225b5b31901000000), Actor(DDPWorker, feadb4b3f7e7f93d2bdaa04201000000)]\n",
      "2025-05-14 14:21:19,443\tINFO torch_tensor_nccl_channel.py:797 -- NCCL group initialized.\n",
      "2025-05-14 14:21:19,712\tINFO dag_node_operation.py:618 -- Writing compiled graph schedule visualization to compiled_graph_schedule.png\n",
      "\u001b[32mPASSED\u001b[0m2025-05-14 14:21:20,178\tINFO compiled_dag_node.py:2173 -- Tearing down compiled DAG\n",
      "2025-05-14 14:21:20,188\tINFO compiled_dag_node.py:2178 -- Cancelling compiled worker on actor: Actor(DDPWorker, 12613a5c377887c20440b0a301000000)\n",
      "2025-05-14 14:21:20,188\tINFO compiled_dag_node.py:2178 -- Cancelling compiled worker on actor: Actor(DDPWorker, fb5b890af1d0720a888a735601000000)\n",
      "2025-05-14 14:21:20,188\tINFO compiled_dag_node.py:2178 -- Cancelling compiled worker on actor: Actor(DDPWorker, de07f8b50e94fbb225b5b31901000000)\n",
      "2025-05-14 14:21:20,188\tINFO compiled_dag_node.py:2178 -- Cancelling compiled worker on actor: Actor(DDPWorker, feadb4b3f7e7f93d2bdaa04201000000)\n",
      "2025-05-14 14:21:20,199\tINFO compiled_dag_node.py:2200 -- Waiting for worker tasks to exit\n",
      "2025-05-14 14:21:20,199\tINFO compiled_dag_node.py:2161 -- Killing actor: Actor(DDPWorker, 12613a5c377887c20440b0a301000000)\n",
      "2025-05-14 14:21:20,199\tINFO compiled_dag_node.py:2161 -- Killing actor: Actor(DDPWorker, fb5b890af1d0720a888a735601000000)\n",
      "2025-05-14 14:21:20,199\tINFO compiled_dag_node.py:2161 -- Killing actor: Actor(DDPWorker, de07f8b50e94fbb225b5b31901000000)\n",
      "2025-05-14 14:21:20,200\tINFO compiled_dag_node.py:2161 -- Killing actor: Actor(DDPWorker, feadb4b3f7e7f93d2bdaa04201000000)\n",
      "2025-05-14 14:21:20,200\tINFO compiled_dag_node.py:2203 -- Teardown complete\n",
      "\n",
      "\n",
      "\u001b[32m============================== \u001b[32m\u001b[1m2 passed\u001b[0m\u001b[32m in 14.95s\u001b[0m\u001b[32m ==============================\u001b[0m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!python -m pytest -v -s test_collective_dag.py::test_exec_schedules_ddp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "# !python -m pytest -v -s test_execution_schedule.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m============================= test session starts ==============================\u001b[0m\n",
      "platform linux -- Python 3.9.22, pytest-8.3.5, pluggy-1.5.0 -- /home/wxdeng/miniconda3/envs/coll-sched-0512-py39/bin/python\n",
      "cachedir: .pytest_cache\n",
      "rootdir: /home/wxdeng/stbr-min\n",
      "configfile: pytest.ini\n",
      "plugins: anyio-4.9.0\n",
      "collected 7 items                                                              \u001b[0m\u001b[1m\n",
      "\n",
      "test_collective_dag.py::test_all_reduce_duplicate_actors[ray_start_regular0] 2025-05-14 12:24:28,457\tINFO worker.py:1879 -- Started a local Ray instance. View the dashboard at \u001b[1m\u001b[32m127.0.0.1:8265 \u001b[39m\u001b[22m\n",
      "\u001b[32mPASSED\u001b[0m\n",
      "test_collective_dag.py::test_all_reduce_custom_comm_wrong_actors[ray_start_regular0] 2025-05-14 12:24:32,266\tINFO worker.py:1879 -- Started a local Ray instance. View the dashboard at \u001b[1m\u001b[32m127.0.0.1:8265 \u001b[39m\u001b[22m\n",
      "\u001b[32mPASSED\u001b[0m\n",
      "test_collective_dag.py::test_comm_all_reduces[ray_start_regular0] 2025-05-14 12:24:36,989\tINFO worker.py:1879 -- Started a local Ray instance. View the dashboard at \u001b[1m\u001b[32m127.0.0.1:8265 \u001b[39m\u001b[22m\n",
      "2025-05-14 12:24:39,861\tINFO dag_node_operation.py:618 -- Writing compiled graph schedule visualization to compiled_graph_schedule.png\n",
      "2025-05-14 12:24:40,086\tINFO compiled_dag_node.py:2173 -- Tearing down compiled DAG\n",
      "2025-05-14 12:24:40,091\tINFO compiled_dag_node.py:2178 -- Cancelling compiled worker on actor: Actor(CPUTorchTensorWorker, e4bf4b89907e121a03c2b87501000000)\n",
      "2025-05-14 12:24:40,091\tINFO compiled_dag_node.py:2178 -- Cancelling compiled worker on actor: Actor(CPUTorchTensorWorker, d81c9d365840e9cc6880bf9801000000)\n",
      "2025-05-14 12:24:40,419\tINFO compiled_dag_node.py:2200 -- Waiting for worker tasks to exit\n",
      "2025-05-14 12:24:40,446\tINFO compiled_dag_node.py:2203 -- Teardown complete\n",
      "\u001b[32mPASSED\u001b[0m\n",
      "test_collective_dag.py::test_comm_deduplicate_all_reduces[ray_start_regular0] 2025-05-14 12:24:44,967\tINFO worker.py:1879 -- Started a local Ray instance. View the dashboard at \u001b[1m\u001b[32m127.0.0.1:8265 \u001b[39m\u001b[22m\n",
      "2025-05-14 12:24:47,373\tINFO dag_node_operation.py:618 -- Writing compiled graph schedule visualization to compiled_graph_schedule.png\n",
      "2025-05-14 12:24:47,643\tINFO compiled_dag_node.py:2173 -- Tearing down compiled DAG\n",
      "2025-05-14 12:24:47,647\tINFO compiled_dag_node.py:2178 -- Cancelling compiled worker on actor: Actor(CPUTorchTensorWorker, cc0fdddfb96c8be91a6a86b001000000)\n",
      "2025-05-14 12:24:47,647\tINFO compiled_dag_node.py:2178 -- Cancelling compiled worker on actor: Actor(CPUTorchTensorWorker, a0121b498f6b227217fce03901000000)\n",
      "2025-05-14 12:24:47,970\tINFO compiled_dag_node.py:2200 -- Waiting for worker tasks to exit\n",
      "2025-05-14 12:24:47,998\tINFO compiled_dag_node.py:2203 -- Teardown complete\n",
      "\u001b[32mPASSED\u001b[0m\n",
      "test_collective_dag.py::test_comm_deduplicate_p2p_and_collective[ray_start_regular0] 2025-05-14 12:24:51,660\tINFO worker.py:1879 -- Started a local Ray instance. View the dashboard at \u001b[1m\u001b[32m127.0.0.1:8265 \u001b[39m\u001b[22m\n",
      "2025-05-14 12:24:54,121\tINFO dag_node_operation.py:618 -- Writing compiled graph schedule visualization to compiled_graph_schedule.png\n",
      "2025-05-14 12:24:54,377\tINFO compiled_dag_node.py:2173 -- Tearing down compiled DAG\n",
      "2025-05-14 12:24:54,382\tINFO compiled_dag_node.py:2178 -- Cancelling compiled worker on actor: Actor(CPUTorchTensorWorker, 124c03e347494ee58f607e9a01000000)\n",
      "2025-05-14 12:24:54,382\tINFO compiled_dag_node.py:2178 -- Cancelling compiled worker on actor: Actor(CPUTorchTensorWorker, c4fee5b0c844aafa02b50e0401000000)\n",
      "2025-05-14 12:24:54,695\tINFO compiled_dag_node.py:2200 -- Waiting for worker tasks to exit\n",
      "2025-05-14 12:24:54,721\tINFO compiled_dag_node.py:2203 -- Teardown complete\n",
      "2025-05-14 12:24:54,851\tINFO dag_node_operation.py:618 -- Writing compiled graph schedule visualization to compiled_graph_schedule.png\n",
      "2025-05-14 12:24:55,085\tINFO compiled_dag_node.py:2173 -- Tearing down compiled DAG\n",
      "2025-05-14 12:24:55,089\tINFO compiled_dag_node.py:2178 -- Cancelling compiled worker on actor: Actor(CPUTorchTensorWorker, 124c03e347494ee58f607e9a01000000)\n",
      "2025-05-14 12:24:55,089\tINFO compiled_dag_node.py:2178 -- Cancelling compiled worker on actor: Actor(CPUTorchTensorWorker, c4fee5b0c844aafa02b50e0401000000)\n",
      "2025-05-14 12:24:55,120\tINFO compiled_dag_node.py:2200 -- Waiting for worker tasks to exit\n",
      "2025-05-14 12:24:55,121\tINFO compiled_dag_node.py:2203 -- Teardown complete\n",
      "\u001b[32mPASSED\u001b[0m\n",
      "test_collective_dag.py::test_custom_comm[ray_start_regular0] 2025-05-14 12:24:58,683\tINFO worker.py:1879 -- Started a local Ray instance. View the dashboard at \u001b[1m\u001b[32m127.0.0.1:8265 \u001b[39m\u001b[22m\n",
      "\u001b[33m(raylet)\u001b[0m [2025-05-14 12:24:58,650 E 1352505 1352533] (raylet) file_system_monitor.cc:116: /tmp/ray/session_2025-05-14_12-24-56_756468_1345615 is over 95% full, available space: 81.2405 GB; capacity: 1830.63 GB. Object creation will fail if spilling is required.\n",
      "2025-05-14 12:25:01,267\tINFO dag_node_operation.py:618 -- Writing compiled graph schedule visualization to compiled_graph_schedule.png\n",
      "2025-05-14 12:25:01,604\tINFO compiled_dag_node.py:2173 -- Tearing down compiled DAG\n",
      "2025-05-14 12:25:01,608\tINFO compiled_dag_node.py:2178 -- Cancelling compiled worker on actor: Actor(CPUTorchTensorWorker, cee3d2c5d3a1dc95c7c7a82101000000)\n",
      "2025-05-14 12:25:01,609\tINFO compiled_dag_node.py:2178 -- Cancelling compiled worker on actor: Actor(CPUTorchTensorWorker, 56945bb31956a1dbcea1551d01000000)\n",
      "2025-05-14 12:25:01,924\tINFO compiled_dag_node.py:2200 -- Waiting for worker tasks to exit\n",
      "2025-05-14 12:25:01,952\tINFO compiled_dag_node.py:2203 -- Teardown complete\n",
      "2025-05-14 12:25:02,089\tINFO dag_node_operation.py:618 -- Writing compiled graph schedule visualization to compiled_graph_schedule.png\n",
      "2025-05-14 12:25:02,429\tINFO compiled_dag_node.py:2173 -- Tearing down compiled DAG\n",
      "2025-05-14 12:25:02,433\tINFO compiled_dag_node.py:2178 -- Cancelling compiled worker on actor: Actor(CPUTorchTensorWorker, cee3d2c5d3a1dc95c7c7a82101000000)\n",
      "2025-05-14 12:25:02,433\tINFO compiled_dag_node.py:2178 -- Cancelling compiled worker on actor: Actor(CPUTorchTensorWorker, 56945bb31956a1dbcea1551d01000000)\n",
      "2025-05-14 12:25:02,457\tINFO compiled_dag_node.py:2200 -- Waiting for worker tasks to exit\n",
      "2025-05-14 12:25:02,458\tINFO compiled_dag_node.py:2203 -- Teardown complete\n",
      "\u001b[32mPASSED\u001b[0m\n",
      "test_collective_dag.py::test_custom_comm_init_teardown[ray_start_regular0] 2025-05-14 12:25:06,900\tINFO worker.py:1879 -- Started a local Ray instance. View the dashboard at \u001b[1m\u001b[32m127.0.0.1:8265 \u001b[39m\u001b[22m\n",
      "2025-05-14 12:25:09,329\tINFO dag_node_operation.py:618 -- Writing compiled graph schedule visualization to compiled_graph_schedule.png\n",
      "2025-05-14 12:25:09,546\tINFO compiled_dag_node.py:2173 -- Tearing down compiled DAG\n",
      "2025-05-14 12:25:09,551\tINFO compiled_dag_node.py:2178 -- Cancelling compiled worker on actor: Actor(CPUTorchTensorWorker, 8b77bcd6c2191752a74f60f501000000)\n",
      "2025-05-14 12:25:09,551\tINFO compiled_dag_node.py:2178 -- Cancelling compiled worker on actor: Actor(CPUTorchTensorWorker, 0ee77568b600ea2f55b1e0a601000000)\n",
      "2025-05-14 12:25:09,869\tINFO compiled_dag_node.py:2200 -- Waiting for worker tasks to exit\n",
      "2025-05-14 12:25:09,899\tINFO compiled_dag_node.py:2203 -- Teardown complete\n",
      "2025-05-14 12:25:10,032\tINFO dag_node_operation.py:618 -- Writing compiled graph schedule visualization to compiled_graph_schedule.png\n",
      "2025-05-14 12:25:10,371\tINFO compiled_dag_node.py:2173 -- Tearing down compiled DAG\n",
      "2025-05-14 12:25:10,375\tINFO compiled_dag_node.py:2178 -- Cancelling compiled worker on actor: Actor(CPUTorchTensorWorker, 8b77bcd6c2191752a74f60f501000000)\n",
      "2025-05-14 12:25:10,375\tINFO compiled_dag_node.py:2178 -- Cancelling compiled worker on actor: Actor(CPUTorchTensorWorker, 0ee77568b600ea2f55b1e0a601000000)\n",
      "2025-05-14 12:25:10,403\tINFO compiled_dag_node.py:2200 -- Waiting for worker tasks to exit\n",
      "2025-05-14 12:25:10,405\tINFO compiled_dag_node.py:2203 -- Teardown complete\n",
      "\u001b[32mPASSED\u001b[0m\n",
      "\n",
      "\u001b[32m============================== \u001b[32m\u001b[1m7 passed\u001b[0m\u001b[32m in 47.16s\u001b[0m\u001b[32m ==============================\u001b[0m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "# !python -m pytest -v -s test_collective_dag.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m============================= test session starts ==============================\u001b[0m\n",
      "platform linux -- Python 3.9.22, pytest-8.3.5, pluggy-1.5.0 -- /home/wxdeng/miniconda3/envs/coll-sched-0512-py39/bin/python\n",
      "cachedir: .pytest_cache\n",
      "rootdir: /home/wxdeng/stbr-min\n",
      "configfile: pytest.ini\n",
      "plugins: anyio-4.9.0\n",
      "collected 53 items                                                             \u001b[0m\u001b[1m\n",
      "\n",
      "test_torch_tensor_dag.py::test_torch_tensor_p2p[ray_start_regular0] 2025-05-14 11:54:22,907\tINFO worker.py:1879 -- Started a local Ray instance. View the dashboard at \u001b[1m\u001b[32m127.0.0.1:8265 \u001b[39m\u001b[22m\n",
      "2025-05-14 11:54:25,343\tINFO torch_tensor_nccl_channel.py:772 -- Creating NCCL group c4559e10-7989-4bc8-9637-e74be25a3b76 on actors: [Actor(TorchTensorWorker, d1a21632413a0d30bc6e1cec01000000), Actor(TorchTensorWorker, 17bb59fe75e070a4cb8abbfd01000000)]\n",
      "2025-05-14 11:54:26,530\tINFO torch_tensor_nccl_channel.py:797 -- NCCL group initialized.\n",
      "2025-05-14 11:54:26,701\tINFO dag_node_operation.py:618 -- Writing compiled graph schedule visualization to compiled_graph_schedule.png\n",
      "\u001b[32mPASSED\u001b[0m2025-05-14 11:54:26,933\tINFO compiled_dag_node.py:2173 -- Tearing down compiled DAG\n",
      "2025-05-14 11:54:26,933\tINFO compiled_dag_node.py:2178 -- Cancelling compiled worker on actor: Actor(TorchTensorWorker, d1a21632413a0d30bc6e1cec01000000)\n",
      "2025-05-14 11:54:26,933\tINFO compiled_dag_node.py:2178 -- Cancelling compiled worker on actor: Actor(TorchTensorWorker, 17bb59fe75e070a4cb8abbfd01000000)\n",
      "\u001b[36m(TorchTensorWorker pid=1087529)\u001b[0m Destructing NCCL group on actor: Actor(TorchTensorWorker, d1a21632413a0d30bc6e1cec01000000)\n",
      "\u001b[36m(TorchTensorWorker pid=1087531)\u001b[0m Destructing NCCL group on actor: Actor(TorchTensorWorker, 17bb59fe75e070a4cb8abbfd01000000)\n",
      "2025-05-14 11:54:27,452\tINFO compiled_dag_node.py:2200 -- Waiting for worker tasks to exit\n",
      "2025-05-14 11:54:27,452\tINFO compiled_dag_node.py:2161 -- Killing actor: Actor(TorchTensorWorker, d1a21632413a0d30bc6e1cec01000000)\n",
      "2025-05-14 11:54:27,452\tINFO compiled_dag_node.py:2161 -- Killing actor: Actor(TorchTensorWorker, 17bb59fe75e070a4cb8abbfd01000000)\n",
      "2025-05-14 11:54:27,453\tINFO compiled_dag_node.py:2203 -- Teardown complete\n",
      "\n",
      "test_torch_tensor_dag.py::test_torch_tensor_as_dag_input[ray_start_regular0] 2025-05-14 11:54:31,306\tINFO worker.py:1879 -- Started a local Ray instance. View the dashboard at \u001b[1m\u001b[32m127.0.0.1:8265 \u001b[39m\u001b[22m\n",
      "2025-05-14 11:54:33,139\tINFO dag_node_operation.py:618 -- Writing compiled graph schedule visualization to compiled_graph_schedule.png\n",
      "\u001b[32mPASSED\u001b[0m2025-05-14 11:54:33,509\tINFO compiled_dag_node.py:2173 -- Tearing down compiled DAG\n",
      "2025-05-14 11:54:33,509\tINFO compiled_dag_node.py:2178 -- Cancelling compiled worker on actor: Actor(TorchTensorWorker, 4b872700bea42c830a370aa701000000)\n",
      "2025-05-14 11:54:33,512\tINFO compiled_dag_node.py:2200 -- Waiting for worker tasks to exit\n",
      "2025-05-14 11:54:33,512\tINFO compiled_dag_node.py:2161 -- Killing actor: Actor(TorchTensorWorker, 4b872700bea42c830a370aa701000000)\n",
      "2025-05-14 11:54:33,512\tINFO compiled_dag_node.py:2203 -- Teardown complete\n",
      "\n",
      "test_torch_tensor_dag.py::test_torch_tensor_nccl[False-False-ray_start_regular0] 2025-05-14 11:54:36,778\tINFO worker.py:1879 -- Started a local Ray instance. View the dashboard at \u001b[1m\u001b[32m127.0.0.1:8265 \u001b[39m\u001b[22m\n",
      "\u001b[33m(raylet)\u001b[0m [2025-05-14 11:54:36,753 E 1089860 1089888] (raylet) file_system_monitor.cc:116: /tmp/ray/session_2025-05-14_11-54-34_949478_1086541 is over 95% full, available space: 81.3829 GB; capacity: 1830.63 GB. Object creation will fail if spilling is required.\n",
      "2025-05-14 11:54:39,058\tINFO torch_tensor_nccl_channel.py:772 -- Creating NCCL group 1a804cbc-6487-44f3-b06b-c2208f208799 on actors: [Actor(TorchTensorWorker, a589a6baf15e955ab6f5f58401000000), Actor(TorchTensorWorker, ba14fcf273b2f73aac418e6801000000)]\n",
      "2025-05-14 11:54:40,252\tINFO torch_tensor_nccl_channel.py:797 -- NCCL group initialized.\n",
      "2025-05-14 11:54:40,417\tINFO dag_node_operation.py:618 -- Writing compiled graph schedule visualization to compiled_graph_schedule.png\n",
      "2025-05-14 11:54:40,645\tINFO compiled_dag_node.py:2173 -- Tearing down compiled DAG\n",
      "2025-05-14 11:54:40,646\tINFO compiled_dag_node.py:2178 -- Cancelling compiled worker on actor: Actor(TorchTensorWorker, ba14fcf273b2f73aac418e6801000000)\n",
      "2025-05-14 11:54:40,646\tINFO compiled_dag_node.py:2178 -- Cancelling compiled worker on actor: Actor(TorchTensorWorker, a589a6baf15e955ab6f5f58401000000)\n",
      "\u001b[36m(TorchTensorWorker pid=1089950)\u001b[0m Destructing NCCL group on actor: Actor(TorchTensorWorker, a589a6baf15e955ab6f5f58401000000)\n",
      "\u001b[36m(TorchTensorWorker pid=1089951)\u001b[0m Destructing NCCL group on actor: Actor(TorchTensorWorker, ba14fcf273b2f73aac418e6801000000)\n",
      "2025-05-14 11:54:41,161\tINFO compiled_dag_node.py:2200 -- Waiting for worker tasks to exit\n",
      "2025-05-14 11:54:41,161\tINFO compiled_dag_node.py:2203 -- Teardown complete\n",
      "2025-05-14 11:54:41,176\tINFO torch_tensor_nccl_channel.py:772 -- Creating NCCL group 4222380e-f16a-41fd-9e5d-513b48898b01 on actors: [Actor(TorchTensorWorker, a589a6baf15e955ab6f5f58401000000), Actor(TorchTensorWorker, ba14fcf273b2f73aac418e6801000000)]\n",
      "2025-05-14 11:54:41,217\tINFO torch_tensor_nccl_channel.py:797 -- NCCL group initialized.\n",
      "2025-05-14 11:54:41,341\tINFO dag_node_operation.py:618 -- Writing compiled graph schedule visualization to compiled_graph_schedule.png\n",
      "\u001b[32mPASSED\u001b[0m2025-05-14 11:54:41,536\tINFO compiled_dag_node.py:2173 -- Tearing down compiled DAG\n",
      "2025-05-14 11:54:41,537\tINFO compiled_dag_node.py:2178 -- Cancelling compiled worker on actor: Actor(TorchTensorWorker, ba14fcf273b2f73aac418e6801000000)\n",
      "2025-05-14 11:54:41,538\tINFO compiled_dag_node.py:2178 -- Cancelling compiled worker on actor: Actor(TorchTensorWorker, a589a6baf15e955ab6f5f58401000000)\n",
      "\u001b[36m(TorchTensorWorker pid=1089950)\u001b[0m Destructing NCCL group on actor: Actor(TorchTensorWorker, a589a6baf15e955ab6f5f58401000000)\n",
      "\u001b[36m(TorchTensorWorker pid=1089951)\u001b[0m Destructing NCCL group on actor: Actor(TorchTensorWorker, ba14fcf273b2f73aac418e6801000000)\n",
      "2025-05-14 11:54:42,038\tINFO compiled_dag_node.py:2200 -- Waiting for worker tasks to exit\n",
      "2025-05-14 11:54:42,038\tINFO compiled_dag_node.py:2161 -- Killing actor: Actor(TorchTensorWorker, ba14fcf273b2f73aac418e6801000000)\n",
      "2025-05-14 11:54:42,039\tINFO compiled_dag_node.py:2161 -- Killing actor: Actor(TorchTensorWorker, a589a6baf15e955ab6f5f58401000000)\n",
      "2025-05-14 11:54:42,039\tINFO compiled_dag_node.py:2203 -- Teardown complete\n",
      "\n",
      "test_torch_tensor_dag.py::test_torch_tensor_nccl[False-True-ray_start_regular0] 2025-05-14 11:54:46,783\tINFO worker.py:1879 -- Started a local Ray instance. View the dashboard at \u001b[1m\u001b[32m127.0.0.1:8265 \u001b[39m\u001b[22m\n",
      "2025-05-14 11:54:49,079\tINFO torch_tensor_nccl_channel.py:772 -- Creating NCCL group d1c90e59-aaf8-48f5-a609-cda0f85842af on actors: [Actor(TorchTensorWorker, 19feb080b3d62b4c91dc099d01000000), Actor(TorchTensorWorker, ae12cf8de13300c1fa18a6a001000000)]\n",
      "2025-05-14 11:54:50,275\tINFO torch_tensor_nccl_channel.py:797 -- NCCL group initialized.\n",
      "2025-05-14 11:54:50,439\tINFO dag_node_operation.py:618 -- Writing compiled graph schedule visualization to compiled_graph_schedule.png\n",
      "2025-05-14 11:54:50,664\tINFO compiled_dag_node.py:2173 -- Tearing down compiled DAG\n",
      "2025-05-14 11:54:50,665\tINFO compiled_dag_node.py:2178 -- Cancelling compiled worker on actor: Actor(TorchTensorWorker, 19feb080b3d62b4c91dc099d01000000)\n",
      "2025-05-14 11:54:50,665\tINFO compiled_dag_node.py:2178 -- Cancelling compiled worker on actor: Actor(TorchTensorWorker, ae12cf8de13300c1fa18a6a001000000)\n",
      "\u001b[36m(TorchTensorWorker pid=1091247)\u001b[0m Destructing NCCL group on actor: Actor(TorchTensorWorker, ae12cf8de13300c1fa18a6a001000000)\n",
      "\u001b[36m(TorchTensorWorker pid=1091246)\u001b[0m Destructing NCCL group on actor: Actor(TorchTensorWorker, 19feb080b3d62b4c91dc099d01000000)\n",
      "2025-05-14 11:54:51,163\tINFO compiled_dag_node.py:2200 -- Waiting for worker tasks to exit\n",
      "2025-05-14 11:54:51,164\tINFO compiled_dag_node.py:2203 -- Teardown complete\n",
      "2025-05-14 11:54:51,177\tINFO torch_tensor_nccl_channel.py:772 -- Creating NCCL group 52b0a5be-55e8-470f-8623-1c7d374b8eb7 on actors: [Actor(TorchTensorWorker, 19feb080b3d62b4c91dc099d01000000), Actor(TorchTensorWorker, ae12cf8de13300c1fa18a6a001000000)]\n",
      "2025-05-14 11:54:51,216\tINFO torch_tensor_nccl_channel.py:797 -- NCCL group initialized.\n",
      "2025-05-14 11:54:51,335\tINFO dag_node_operation.py:618 -- Writing compiled graph schedule visualization to compiled_graph_schedule.png\n",
      "\u001b[32mPASSED\u001b[0m2025-05-14 11:54:51,533\tINFO compiled_dag_node.py:2173 -- Tearing down compiled DAG\n",
      "2025-05-14 11:54:51,534\tINFO compiled_dag_node.py:2178 -- Cancelling compiled worker on actor: Actor(TorchTensorWorker, 19feb080b3d62b4c91dc099d01000000)\n",
      "2025-05-14 11:54:51,534\tINFO compiled_dag_node.py:2178 -- Cancelling compiled worker on actor: Actor(TorchTensorWorker, ae12cf8de13300c1fa18a6a001000000)\n",
      "\u001b[36m(TorchTensorWorker pid=1091247)\u001b[0m Destructing NCCL group on actor: Actor(TorchTensorWorker, ae12cf8de13300c1fa18a6a001000000)\n",
      "\u001b[36m(TorchTensorWorker pid=1091246)\u001b[0m Destructing NCCL group on actor: Actor(TorchTensorWorker, 19feb080b3d62b4c91dc099d01000000)\n",
      "2025-05-14 11:54:52,033\tINFO compiled_dag_node.py:2200 -- Waiting for worker tasks to exit\n",
      "2025-05-14 11:54:52,033\tINFO compiled_dag_node.py:2161 -- Killing actor: Actor(TorchTensorWorker, 19feb080b3d62b4c91dc099d01000000)\n",
      "2025-05-14 11:54:52,034\tINFO compiled_dag_node.py:2161 -- Killing actor: Actor(TorchTensorWorker, ae12cf8de13300c1fa18a6a001000000)\n",
      "2025-05-14 11:54:52,034\tINFO compiled_dag_node.py:2203 -- Teardown complete\n",
      "\n",
      "test_torch_tensor_dag.py::test_torch_tensor_nccl[True-False-ray_start_regular0] 2025-05-14 11:54:56,773\tINFO worker.py:1879 -- Started a local Ray instance. View the dashboard at \u001b[1m\u001b[32m127.0.0.1:8265 \u001b[39m\u001b[22m\n",
      "2025-05-14 11:54:59,180\tINFO torch_tensor_nccl_channel.py:772 -- Creating NCCL group 0fed489f-9db1-44bb-8ec2-d5e714a7899f on actors: [Actor(TorchTensorWorker, bf39958a6df51a14ef7459e401000000), Actor(TorchTensorWorker, 7462d30a09b1ca95f06eca7301000000)]\n",
      "2025-05-14 11:55:00,391\tINFO torch_tensor_nccl_channel.py:797 -- NCCL group initialized.\n",
      "2025-05-14 11:55:00,560\tINFO dag_node_operation.py:618 -- Writing compiled graph schedule visualization to compiled_graph_schedule.png\n",
      "2025-05-14 11:55:00,789\tINFO compiled_dag_node.py:2173 -- Tearing down compiled DAG\n",
      "2025-05-14 11:55:00,790\tINFO compiled_dag_node.py:2178 -- Cancelling compiled worker on actor: Actor(TorchTensorWorker, 7462d30a09b1ca95f06eca7301000000)\n",
      "2025-05-14 11:55:00,790\tINFO compiled_dag_node.py:2178 -- Cancelling compiled worker on actor: Actor(TorchTensorWorker, bf39958a6df51a14ef7459e401000000)\n",
      "\u001b[36m(TorchTensorWorker pid=1092553)\u001b[0m Destructing NCCL group on actor: Actor(TorchTensorWorker, bf39958a6df51a14ef7459e401000000)\n",
      "\u001b[36m(TorchTensorWorker pid=1092551)\u001b[0m Destructing NCCL group on actor: Actor(TorchTensorWorker, 7462d30a09b1ca95f06eca7301000000)\n",
      "2025-05-14 11:55:01,307\tINFO compiled_dag_node.py:2200 -- Waiting for worker tasks to exit\n",
      "2025-05-14 11:55:01,308\tINFO compiled_dag_node.py:2203 -- Teardown complete\n",
      "2025-05-14 11:55:01,321\tINFO torch_tensor_nccl_channel.py:772 -- Creating NCCL group 1b79c507-ef55-4514-9bc4-0d0e4a20f387 on actors: [Actor(TorchTensorWorker, bf39958a6df51a14ef7459e401000000), Actor(TorchTensorWorker, 7462d30a09b1ca95f06eca7301000000)]\n",
      "2025-05-14 11:55:01,355\tINFO torch_tensor_nccl_channel.py:797 -- NCCL group initialized.\n",
      "2025-05-14 11:55:01,481\tINFO dag_node_operation.py:618 -- Writing compiled graph schedule visualization to compiled_graph_schedule.png\n",
      "\u001b[32mPASSED\u001b[0m2025-05-14 11:55:01,675\tINFO compiled_dag_node.py:2173 -- Tearing down compiled DAG\n",
      "2025-05-14 11:55:01,676\tINFO compiled_dag_node.py:2178 -- Cancelling compiled worker on actor: Actor(TorchTensorWorker, 7462d30a09b1ca95f06eca7301000000)\n",
      "2025-05-14 11:55:01,676\tINFO compiled_dag_node.py:2178 -- Cancelling compiled worker on actor: Actor(TorchTensorWorker, bf39958a6df51a14ef7459e401000000)\n",
      "\u001b[36m(TorchTensorWorker pid=1092551)\u001b[0m Destructing NCCL group on actor: Actor(TorchTensorWorker, 7462d30a09b1ca95f06eca7301000000)\n",
      "\u001b[36m(TorchTensorWorker pid=1092553)\u001b[0m Destructing NCCL group on actor: Actor(TorchTensorWorker, bf39958a6df51a14ef7459e401000000)\n",
      "2025-05-14 11:55:02,192\tINFO compiled_dag_node.py:2200 -- Waiting for worker tasks to exit\n",
      "2025-05-14 11:55:02,192\tINFO compiled_dag_node.py:2161 -- Killing actor: Actor(TorchTensorWorker, 7462d30a09b1ca95f06eca7301000000)\n",
      "2025-05-14 11:55:02,193\tINFO compiled_dag_node.py:2161 -- Killing actor: Actor(TorchTensorWorker, bf39958a6df51a14ef7459e401000000)\n",
      "2025-05-14 11:55:02,193\tINFO compiled_dag_node.py:2203 -- Teardown complete\n",
      "\n",
      "test_torch_tensor_dag.py::test_torch_tensor_nccl[True-True-ray_start_regular0] 2025-05-14 11:55:07,002\tINFO worker.py:1879 -- Started a local Ray instance. View the dashboard at \u001b[1m\u001b[32m127.0.0.1:8265 \u001b[39m\u001b[22m\n",
      "2025-05-14 11:55:09,394\tINFO torch_tensor_nccl_channel.py:772 -- Creating NCCL group 826410c9-b8c1-4d7c-b22c-b8cdcd7b29db on actors: [Actor(TorchTensorWorker, 60839a9caec9c835448d65df01000000), Actor(TorchTensorWorker, ce4c0000e9d12e78c50c278701000000)]\n",
      "2025-05-14 11:55:10,592\tINFO torch_tensor_nccl_channel.py:797 -- NCCL group initialized.\n",
      "2025-05-14 11:55:10,760\tINFO dag_node_operation.py:618 -- Writing compiled graph schedule visualization to compiled_graph_schedule.png\n",
      "2025-05-14 11:55:11,004\tINFO compiled_dag_node.py:2173 -- Tearing down compiled DAG\n",
      "2025-05-14 11:55:11,005\tINFO compiled_dag_node.py:2178 -- Cancelling compiled worker on actor: Actor(TorchTensorWorker, 60839a9caec9c835448d65df01000000)\n",
      "2025-05-14 11:55:11,005\tINFO compiled_dag_node.py:2178 -- Cancelling compiled worker on actor: Actor(TorchTensorWorker, ce4c0000e9d12e78c50c278701000000)\n",
      "\u001b[36m(TorchTensorWorker pid=1093840)\u001b[0m Destructing NCCL group on actor: Actor(TorchTensorWorker, ce4c0000e9d12e78c50c278701000000)\n",
      "\u001b[36m(TorchTensorWorker pid=1093842)\u001b[0m Destructing NCCL group on actor: Actor(TorchTensorWorker, 60839a9caec9c835448d65df01000000)\n",
      "2025-05-14 11:55:11,520\tINFO compiled_dag_node.py:2200 -- Waiting for worker tasks to exit\n",
      "2025-05-14 11:55:11,521\tINFO compiled_dag_node.py:2203 -- Teardown complete\n",
      "2025-05-14 11:55:11,537\tINFO torch_tensor_nccl_channel.py:772 -- Creating NCCL group 28ba69ca-555f-4946-80a9-082e0e0991b5 on actors: [Actor(TorchTensorWorker, 60839a9caec9c835448d65df01000000), Actor(TorchTensorWorker, ce4c0000e9d12e78c50c278701000000)]\n",
      "2025-05-14 11:55:11,570\tINFO torch_tensor_nccl_channel.py:797 -- NCCL group initialized.\n",
      "2025-05-14 11:55:11,692\tINFO dag_node_operation.py:618 -- Writing compiled graph schedule visualization to compiled_graph_schedule.png\n",
      "\u001b[32mPASSED\u001b[0m2025-05-14 11:55:11,887\tINFO compiled_dag_node.py:2173 -- Tearing down compiled DAG\n",
      "2025-05-14 11:55:11,888\tINFO compiled_dag_node.py:2178 -- Cancelling compiled worker on actor: Actor(TorchTensorWorker, 60839a9caec9c835448d65df01000000)\n",
      "2025-05-14 11:55:11,888\tINFO compiled_dag_node.py:2178 -- Cancelling compiled worker on actor: Actor(TorchTensorWorker, ce4c0000e9d12e78c50c278701000000)\n",
      "\u001b[36m(TorchTensorWorker pid=1093840)\u001b[0m Destructing NCCL group on actor: Actor(TorchTensorWorker, ce4c0000e9d12e78c50c278701000000)\n",
      "\u001b[36m(TorchTensorWorker pid=1093842)\u001b[0m Destructing NCCL group on actor: Actor(TorchTensorWorker, 60839a9caec9c835448d65df01000000)\n",
      "2025-05-14 11:55:12,407\tINFO compiled_dag_node.py:2200 -- Waiting for worker tasks to exit\n",
      "2025-05-14 11:55:12,407\tINFO compiled_dag_node.py:2161 -- Killing actor: Actor(TorchTensorWorker, 60839a9caec9c835448d65df01000000)\n",
      "2025-05-14 11:55:12,407\tINFO compiled_dag_node.py:2161 -- Killing actor: Actor(TorchTensorWorker, ce4c0000e9d12e78c50c278701000000)\n",
      "2025-05-14 11:55:12,408\tINFO compiled_dag_node.py:2203 -- Teardown complete\n",
      "\n",
      "test_torch_tensor_dag.py::test_torch_tensor_shm[ray_start_regular0] 2025-05-14 11:55:17,191\tINFO worker.py:1879 -- Started a local Ray instance. View the dashboard at \u001b[1m\u001b[32m127.0.0.1:8265 \u001b[39m\u001b[22m\n",
      "2025-05-14 11:55:19,247\tINFO dag_node_operation.py:618 -- Writing compiled graph schedule visualization to compiled_graph_schedule.png\n",
      "2025-05-14 11:55:20,152\tINFO compiled_dag_node.py:2173 -- Tearing down compiled DAG\n",
      "2025-05-14 11:55:20,153\tINFO compiled_dag_node.py:2178 -- Cancelling compiled worker on actor: Actor(TorchTensorWorker, cb914c308e4a4e754d0621b501000000)\n",
      "2025-05-14 11:55:20,153\tINFO compiled_dag_node.py:2178 -- Cancelling compiled worker on actor: Actor(TorchTensorWorker, 259365471e98ead78aaa22c601000000)\n",
      "2025-05-14 11:55:20,160\tINFO compiled_dag_node.py:2200 -- Waiting for worker tasks to exit\n",
      "2025-05-14 11:55:20,160\tINFO compiled_dag_node.py:2203 -- Teardown complete\n",
      "2025-05-14 11:55:20,220\tINFO dag_node_operation.py:618 -- Writing compiled graph schedule visualization to compiled_graph_schedule.png\n",
      "2025-05-14 11:55:20,423\tINFO compiled_dag_node.py:2173 -- Tearing down compiled DAG\n",
      "2025-05-14 11:55:20,423\tINFO compiled_dag_node.py:2178 -- Cancelling compiled worker on actor: Actor(TorchTensorWorker, cb914c308e4a4e754d0621b501000000)\n",
      "2025-05-14 11:55:20,423\tINFO compiled_dag_node.py:2178 -- Cancelling compiled worker on actor: Actor(TorchTensorWorker, 259365471e98ead78aaa22c601000000)\n",
      "2025-05-14 11:55:20,429\tINFO compiled_dag_node.py:2200 -- Waiting for worker tasks to exit\n",
      "2025-05-14 11:55:20,429\tINFO compiled_dag_node.py:2203 -- Teardown complete\n",
      "\u001b[32mPASSED\u001b[0m\n",
      "test_torch_tensor_dag.py::test_torch_tensor_auto[num_gpus0-ray_start_regular0] 2025-05-14 11:55:24,067\tINFO worker.py:1879 -- Started a local Ray instance. View the dashboard at \u001b[1m\u001b[32m127.0.0.1:8265 \u001b[39m\u001b[22m\n",
      "2025-05-14 11:55:26,231\tINFO dag_node_operation.py:618 -- Writing compiled graph schedule visualization to compiled_graph_schedule.png\n",
      "2025-05-14 11:55:26,402\tINFO compiled_dag_node.py:2173 -- Tearing down compiled DAG\n",
      "2025-05-14 11:55:26,403\tINFO compiled_dag_node.py:2178 -- Cancelling compiled worker on actor: Actor(TorchTensorWorker, 2ca6a3f6de38787a7f6003fc01000000)\n",
      "2025-05-14 11:55:26,403\tINFO compiled_dag_node.py:2178 -- Cancelling compiled worker on actor: Actor(TorchTensorWorker, f9cdd8d97ea14e2e424ff05d01000000)\n",
      "2025-05-14 11:55:26,411\tINFO compiled_dag_node.py:2200 -- Waiting for worker tasks to exit\n",
      "2025-05-14 11:55:26,411\tINFO compiled_dag_node.py:2203 -- Teardown complete\n",
      "2025-05-14 11:55:26,530\tINFO dag_node_operation.py:618 -- Writing compiled graph schedule visualization to compiled_graph_schedule.png\n",
      "\u001b[32mPASSED\u001b[0m2025-05-14 11:55:26,696\tINFO compiled_dag_node.py:2173 -- Tearing down compiled DAG\n",
      "2025-05-14 11:55:26,697\tINFO compiled_dag_node.py:2178 -- Cancelling compiled worker on actor: Actor(TorchTensorWorker, 2ca6a3f6de38787a7f6003fc01000000)\n",
      "2025-05-14 11:55:26,697\tINFO compiled_dag_node.py:2178 -- Cancelling compiled worker on actor: Actor(TorchTensorWorker, f9cdd8d97ea14e2e424ff05d01000000)\n",
      "2025-05-14 11:55:26,704\tINFO compiled_dag_node.py:2200 -- Waiting for worker tasks to exit\n",
      "2025-05-14 11:55:26,705\tINFO compiled_dag_node.py:2161 -- Killing actor: Actor(TorchTensorWorker, 2ca6a3f6de38787a7f6003fc01000000)\n",
      "2025-05-14 11:55:26,705\tINFO compiled_dag_node.py:2161 -- Killing actor: Actor(TorchTensorWorker, f9cdd8d97ea14e2e424ff05d01000000)\n",
      "2025-05-14 11:55:26,705\tINFO compiled_dag_node.py:2203 -- Teardown complete\n",
      "\n",
      "test_torch_tensor_dag.py::test_torch_tensor_auto[num_gpus1-ray_start_regular0] 2025-05-14 11:55:31,176\tINFO worker.py:1879 -- Started a local Ray instance. View the dashboard at \u001b[1m\u001b[32m127.0.0.1:8265 \u001b[39m\u001b[22m\n",
      "2025-05-14 11:55:33,230\tINFO dag_node_operation.py:618 -- Writing compiled graph schedule visualization to compiled_graph_schedule.png\n",
      "2025-05-14 11:55:33,786\tINFO compiled_dag_node.py:2173 -- Tearing down compiled DAG\n",
      "2025-05-14 11:55:33,787\tINFO compiled_dag_node.py:2178 -- Cancelling compiled worker on actor: Actor(TorchTensorWorker, 0b03c8106301cf7b3d8e261b01000000)\n",
      "2025-05-14 11:55:33,787\tINFO compiled_dag_node.py:2178 -- Cancelling compiled worker on actor: Actor(TorchTensorWorker, 55af9555b927e0685a42795801000000)\n",
      "2025-05-14 11:55:33,793\tINFO compiled_dag_node.py:2200 -- Waiting for worker tasks to exit\n",
      "2025-05-14 11:55:33,793\tINFO compiled_dag_node.py:2203 -- Teardown complete\n",
      "2025-05-14 11:55:33,877\tINFO dag_node_operation.py:618 -- Writing compiled graph schedule visualization to compiled_graph_schedule.png\n",
      "\u001b[32mPASSED\u001b[0m2025-05-14 11:55:34,036\tINFO compiled_dag_node.py:2173 -- Tearing down compiled DAG\n",
      "2025-05-14 11:55:34,037\tINFO compiled_dag_node.py:2178 -- Cancelling compiled worker on actor: Actor(TorchTensorWorker, 0b03c8106301cf7b3d8e261b01000000)\n",
      "2025-05-14 11:55:34,037\tINFO compiled_dag_node.py:2178 -- Cancelling compiled worker on actor: Actor(TorchTensorWorker, 55af9555b927e0685a42795801000000)\n",
      "2025-05-14 11:55:34,044\tINFO compiled_dag_node.py:2200 -- Waiting for worker tasks to exit\n",
      "2025-05-14 11:55:34,044\tINFO compiled_dag_node.py:2161 -- Killing actor: Actor(TorchTensorWorker, 0b03c8106301cf7b3d8e261b01000000)\n",
      "2025-05-14 11:55:34,044\tINFO compiled_dag_node.py:2161 -- Killing actor: Actor(TorchTensorWorker, 55af9555b927e0685a42795801000000)\n",
      "2025-05-14 11:55:34,044\tINFO compiled_dag_node.py:2203 -- Teardown complete\n",
      "\n",
      "test_torch_tensor_dag.py::test_torch_tensor_auto[num_gpus2-ray_start_regular0] 2025-05-14 11:55:37,572\tINFO worker.py:1879 -- Started a local Ray instance. View the dashboard at \u001b[1m\u001b[32m127.0.0.1:8265 \u001b[39m\u001b[22m\n",
      "2025-05-14 11:55:39,630\tINFO dag_node_operation.py:618 -- Writing compiled graph schedule visualization to compiled_graph_schedule.png\n",
      "2025-05-14 11:55:39,976\tINFO compiled_dag_node.py:2173 -- Tearing down compiled DAG\n",
      "2025-05-14 11:55:39,976\tINFO compiled_dag_node.py:2178 -- Cancelling compiled worker on actor: Actor(TorchTensorWorker, ec5a3f7bbe3996d639c2f20101000000)\n",
      "2025-05-14 11:55:39,976\tINFO compiled_dag_node.py:2178 -- Cancelling compiled worker on actor: Actor(TorchTensorWorker, 81214421876690dad2ffa44001000000)\n",
      "2025-05-14 11:55:39,982\tINFO compiled_dag_node.py:2200 -- Waiting for worker tasks to exit\n",
      "2025-05-14 11:55:39,983\tINFO compiled_dag_node.py:2203 -- Teardown complete\n",
      "2025-05-14 11:55:40,068\tINFO dag_node_operation.py:618 -- Writing compiled graph schedule visualization to compiled_graph_schedule.png\n",
      "\u001b[32mPASSED\u001b[0m2025-05-14 11:55:40,238\tINFO compiled_dag_node.py:2173 -- Tearing down compiled DAG\n",
      "2025-05-14 11:55:40,239\tINFO compiled_dag_node.py:2178 -- Cancelling compiled worker on actor: Actor(TorchTensorWorker, ec5a3f7bbe3996d639c2f20101000000)\n",
      "2025-05-14 11:55:40,239\tINFO compiled_dag_node.py:2178 -- Cancelling compiled worker on actor: Actor(TorchTensorWorker, 81214421876690dad2ffa44001000000)\n",
      "2025-05-14 11:55:40,246\tINFO compiled_dag_node.py:2200 -- Waiting for worker tasks to exit\n",
      "2025-05-14 11:55:40,246\tINFO compiled_dag_node.py:2161 -- Killing actor: Actor(TorchTensorWorker, ec5a3f7bbe3996d639c2f20101000000)\n",
      "2025-05-14 11:55:40,246\tINFO compiled_dag_node.py:2161 -- Killing actor: Actor(TorchTensorWorker, 81214421876690dad2ffa44001000000)\n",
      "2025-05-14 11:55:40,246\tINFO compiled_dag_node.py:2203 -- Teardown complete\n",
      "\n",
      "test_torch_tensor_dag.py::test_torch_tensor_auto[num_gpus3-ray_start_regular0] 2025-05-14 11:55:43,777\tINFO worker.py:1879 -- Started a local Ray instance. View the dashboard at \u001b[1m\u001b[32m127.0.0.1:8265 \u001b[39m\u001b[22m\n",
      "\u001b[33m(raylet)\u001b[0m [2025-05-14 11:55:43,746 E 1099924 1099952] (raylet) file_system_monitor.cc:116: /tmp/ray/session_2025-05-14_11-55-41_932761_1086541 is over 95% full, available space: 81.3766 GB; capacity: 1830.63 GB. Object creation will fail if spilling is required.\n",
      "2025-05-14 11:55:46,076\tINFO torch_tensor_nccl_channel.py:772 -- Creating NCCL group 4035760d-2cf2-4037-8779-8af60be1b888 on actors: [Actor(TorchTensorWorker, fa2bec1f18bfb0c40f7016ba01000000), Actor(TorchTensorWorker, 4a79a3ca75403b5f41b9b43d01000000)]\n",
      "2025-05-14 11:55:47,271\tINFO torch_tensor_nccl_channel.py:797 -- NCCL group initialized.\n",
      "2025-05-14 11:55:47,421\tINFO dag_node_operation.py:618 -- Writing compiled graph schedule visualization to compiled_graph_schedule.png\n",
      "2025-05-14 11:55:47,654\tINFO compiled_dag_node.py:2173 -- Tearing down compiled DAG\n",
      "2025-05-14 11:55:47,655\tINFO compiled_dag_node.py:2178 -- Cancelling compiled worker on actor: Actor(TorchTensorWorker, 4a79a3ca75403b5f41b9b43d01000000)\n",
      "2025-05-14 11:55:47,655\tINFO compiled_dag_node.py:2178 -- Cancelling compiled worker on actor: Actor(TorchTensorWorker, fa2bec1f18bfb0c40f7016ba01000000)\n",
      "\u001b[36m(TorchTensorWorker pid=1100013)\u001b[0m Destructing NCCL group on actor: Actor(TorchTensorWorker, fa2bec1f18bfb0c40f7016ba01000000)\n",
      "\u001b[36m(TorchTensorWorker pid=1100014)\u001b[0m Destructing NCCL group on actor: Actor(TorchTensorWorker, 4a79a3ca75403b5f41b9b43d01000000)\n",
      "2025-05-14 11:55:48,170\tINFO compiled_dag_node.py:2200 -- Waiting for worker tasks to exit\n",
      "2025-05-14 11:55:48,170\tINFO compiled_dag_node.py:2203 -- Teardown complete\n",
      "2025-05-14 11:55:48,189\tINFO torch_tensor_nccl_channel.py:772 -- Creating NCCL group e63e46f9-eb3d-4f06-8b3b-1c9dcb3ffb44 on actors: [Actor(TorchTensorWorker, fa2bec1f18bfb0c40f7016ba01000000), Actor(TorchTensorWorker, 4a79a3ca75403b5f41b9b43d01000000)]\n",
      "2025-05-14 11:55:48,221\tINFO torch_tensor_nccl_channel.py:797 -- NCCL group initialized.\n",
      "2025-05-14 11:55:48,334\tINFO dag_node_operation.py:618 -- Writing compiled graph schedule visualization to compiled_graph_schedule.png\n",
      "\u001b[32mPASSED\u001b[0m2025-05-14 11:55:48,530\tINFO compiled_dag_node.py:2173 -- Tearing down compiled DAG\n",
      "2025-05-14 11:55:48,531\tINFO compiled_dag_node.py:2178 -- Cancelling compiled worker on actor: Actor(TorchTensorWorker, 4a79a3ca75403b5f41b9b43d01000000)\n",
      "2025-05-14 11:55:48,531\tINFO compiled_dag_node.py:2178 -- Cancelling compiled worker on actor: Actor(TorchTensorWorker, fa2bec1f18bfb0c40f7016ba01000000)\n",
      "\u001b[36m(TorchTensorWorker pid=1100013)\u001b[0m Destructing NCCL group on actor: Actor(TorchTensorWorker, fa2bec1f18bfb0c40f7016ba01000000)\n",
      "\u001b[36m(TorchTensorWorker pid=1100014)\u001b[0m Destructing NCCL group on actor: Actor(TorchTensorWorker, 4a79a3ca75403b5f41b9b43d01000000)\n",
      "2025-05-14 11:55:49,032\tINFO compiled_dag_node.py:2200 -- Waiting for worker tasks to exit\n",
      "2025-05-14 11:55:49,032\tINFO compiled_dag_node.py:2161 -- Killing actor: Actor(TorchTensorWorker, 4a79a3ca75403b5f41b9b43d01000000)\n",
      "2025-05-14 11:55:49,033\tINFO compiled_dag_node.py:2161 -- Killing actor: Actor(TorchTensorWorker, fa2bec1f18bfb0c40f7016ba01000000)\n",
      "2025-05-14 11:55:49,033\tINFO compiled_dag_node.py:2203 -- Teardown complete\n",
      "\n",
      "test_torch_tensor_dag.py::test_torch_tensor_auto[num_gpus4-ray_start_regular0] 2025-05-14 11:55:53,568\tINFO worker.py:1879 -- Started a local Ray instance. View the dashboard at \u001b[1m\u001b[32m127.0.0.1:8265 \u001b[39m\u001b[22m\n",
      "2025-05-14 11:55:55,714\tINFO dag_node_operation.py:618 -- Writing compiled graph schedule visualization to compiled_graph_schedule.png\n",
      "2025-05-14 11:55:56,554\tINFO compiled_dag_node.py:2173 -- Tearing down compiled DAG\n",
      "2025-05-14 11:55:56,555\tINFO compiled_dag_node.py:2178 -- Cancelling compiled worker on actor: Actor(TorchTensorWorker, 20459de9fbda1ba07dada0f901000000)\n",
      "2025-05-14 11:55:56,555\tINFO compiled_dag_node.py:2178 -- Cancelling compiled worker on actor: Actor(TorchTensorWorker, ace4e4a465e594c4052d58b601000000)\n",
      "2025-05-14 11:55:56,562\tINFO compiled_dag_node.py:2200 -- Waiting for worker tasks to exit\n",
      "2025-05-14 11:55:56,562\tINFO compiled_dag_node.py:2203 -- Teardown complete\n",
      "2025-05-14 11:55:56,648\tINFO dag_node_operation.py:618 -- Writing compiled graph schedule visualization to compiled_graph_schedule.png\n",
      "\u001b[32mPASSED\u001b[0m2025-05-14 11:55:56,820\tINFO compiled_dag_node.py:2173 -- Tearing down compiled DAG\n",
      "2025-05-14 11:55:56,821\tINFO compiled_dag_node.py:2178 -- Cancelling compiled worker on actor: Actor(TorchTensorWorker, 20459de9fbda1ba07dada0f901000000)\n",
      "2025-05-14 11:55:56,821\tINFO compiled_dag_node.py:2178 -- Cancelling compiled worker on actor: Actor(TorchTensorWorker, ace4e4a465e594c4052d58b601000000)\n",
      "2025-05-14 11:55:56,827\tINFO compiled_dag_node.py:2200 -- Waiting for worker tasks to exit\n",
      "2025-05-14 11:55:56,827\tINFO compiled_dag_node.py:2161 -- Killing actor: Actor(TorchTensorWorker, 20459de9fbda1ba07dada0f901000000)\n",
      "2025-05-14 11:55:56,828\tINFO compiled_dag_node.py:2161 -- Killing actor: Actor(TorchTensorWorker, ace4e4a465e594c4052d58b601000000)\n",
      "2025-05-14 11:55:56,828\tINFO compiled_dag_node.py:2203 -- Teardown complete\n",
      "\n",
      "test_torch_tensor_dag.py::test_torch_tensor_nccl_overlap_timed[ray_start_regular0-False] 2025-05-14 11:56:00,574\tINFO worker.py:1879 -- Started a local Ray instance. View the dashboard at \u001b[1m\u001b[32m127.0.0.1:8265 \u001b[39m\u001b[22m\n",
      "\u001b[33m(raylet)\u001b[0m [2025-05-14 11:56:00,533 E 1102487 1102515] (raylet) file_system_monitor.cc:116: /tmp/ray/session_2025-05-14_11-55-58_714681_1086541 is over 95% full, available space: 81.375 GB; capacity: 1830.63 GB. Object creation will fail if spilling is required.\n",
      "\u001b[31mFAILED\u001b[0m\n",
      "test_torch_tensor_dag.py::test_torch_tensor_nccl_overlap_timed[ray_start_regular1-True] 2025-05-14 11:56:05,181\tINFO worker.py:1879 -- Started a local Ray instance. View the dashboard at \u001b[1m\u001b[32m127.0.0.1:8265 \u001b[39m\u001b[22m\n",
      "\u001b[31mFAILED\u001b[0m\n",
      "test_torch_tensor_dag.py::test_torch_tensor_nccl_disallows_driver 2025-05-14 11:56:09,991\tINFO worker.py:1879 -- Started a local Ray instance. View the dashboard at \u001b[1m\u001b[32m127.0.0.1:8265 \u001b[39m\u001b[22m\n",
      "\u001b[32mPASSED\u001b[0m\n",
      "test_torch_tensor_dag.py::test_torch_tensor_custom_comm[ray_start_regular0] 2025-05-14 11:56:16,491\tINFO worker.py:1879 -- Started a local Ray instance. View the dashboard at \u001b[1m\u001b[32m127.0.0.1:8265 \u001b[39m\u001b[22m\n",
      "2025-05-14 11:56:18,837\tINFO torch_tensor_nccl_channel.py:770 -- Initializing custom NCCL group 31859e1d-9606-43aa-b878-8726ecf3a284 on actors: [Actor(TorchTensorWorker, d4205bf5d28988b55230de5401000000), Actor(TorchTensorWorker, e800dc15faa780889991c90101000000)]\n",
      "2025-05-14 11:56:19,803\tINFO torch_tensor_nccl_channel.py:797 -- NCCL group initialized.\n",
      "2025-05-14 11:56:19,932\tINFO dag_node_operation.py:618 -- Writing compiled graph schedule visualization to compiled_graph_schedule.png\n",
      "\u001b[32mPASSED\u001b[0m2025-05-14 11:56:20,166\tINFO compiled_dag_node.py:2173 -- Tearing down compiled DAG\n",
      "2025-05-14 11:56:20,167\tINFO compiled_dag_node.py:2178 -- Cancelling compiled worker on actor: Actor(TorchTensorWorker, d4205bf5d28988b55230de5401000000)\n",
      "2025-05-14 11:56:20,167\tINFO compiled_dag_node.py:2178 -- Cancelling compiled worker on actor: Actor(TorchTensorWorker, e800dc15faa780889991c90101000000)\n",
      "\u001b[36m(TorchTensorWorker pid=1106057)\u001b[0m Destructing NCCL group on actor: Actor(TorchTensorWorker, d4205bf5d28988b55230de5401000000)\n",
      "\u001b[36m(TorchTensorWorker pid=1106059)\u001b[0m Destructing NCCL group on actor: Actor(TorchTensorWorker, e800dc15faa780889991c90101000000)\n",
      "2025-05-14 11:56:20,688\tINFO compiled_dag_node.py:2200 -- Waiting for worker tasks to exit\n",
      "2025-05-14 11:56:20,688\tINFO compiled_dag_node.py:2161 -- Killing actor: Actor(TorchTensorWorker, d4205bf5d28988b55230de5401000000)\n",
      "2025-05-14 11:56:20,689\tINFO compiled_dag_node.py:2161 -- Killing actor: Actor(TorchTensorWorker, e800dc15faa780889991c90101000000)\n",
      "2025-05-14 11:56:20,689\tINFO compiled_dag_node.py:2203 -- Teardown complete\n",
      "\n",
      "test_torch_tensor_dag.py::test_torch_tensor_custom_comm_inited[ray_start_regular0] 2025-05-14 11:56:25,304\tINFO worker.py:1879 -- Started a local Ray instance. View the dashboard at \u001b[1m\u001b[32m127.0.0.1:8265 \u001b[39m\u001b[22m\n",
      "\u001b[33mSKIPPED\u001b[0m\n",
      "test_torch_tensor_dag.py::test_torch_tensor_default_comm[transports0-ray_start_regular0] 2025-05-14 11:56:28,986\tINFO worker.py:1879 -- Started a local Ray instance. View the dashboard at \u001b[1m\u001b[32m127.0.0.1:8265 \u001b[39m\u001b[22m\n",
      "\u001b[33m(raylet)\u001b[0m [2025-05-14 11:56:28,951 E 1108325 1108353] (raylet) file_system_monitor.cc:116: /tmp/ray/session_2025-05-14_11-56-27_146814_1086541 is over 95% full, available space: 81.3714 GB; capacity: 1830.63 GB. Object creation will fail if spilling is required.\n",
      "\u001b[31mFAILED\u001b[0m\n",
      "test_torch_tensor_dag.py::test_torch_tensor_default_comm[transports1-ray_start_regular0] 2025-05-14 11:56:33,583\tINFO worker.py:1879 -- Started a local Ray instance. View the dashboard at \u001b[1m\u001b[32m127.0.0.1:8265 \u001b[39m\u001b[22m\n",
      "\u001b[31mFAILED\u001b[0m\n",
      "test_torch_tensor_dag.py::test_torch_tensor_default_comm[transports2-ray_start_regular0] 2025-05-14 11:56:37,461\tINFO worker.py:1879 -- Started a local Ray instance. View the dashboard at \u001b[1m\u001b[32m127.0.0.1:8265 \u001b[39m\u001b[22m\n",
      "\u001b[33m(raylet)\u001b[0m [2025-05-14 11:56:37,428 E 1110586 1110614] (raylet) file_system_monitor.cc:116: /tmp/ray/session_2025-05-14_11-56-35_528009_1086541 is over 95% full, available space: 81.3699 GB; capacity: 1830.63 GB. Object creation will fail if spilling is required.\n",
      "\u001b[31mFAILED\u001b[0m\n",
      "test_torch_tensor_dag.py::test_torch_tensor_default_comm[transports3-ray_start_regular0] 2025-05-14 11:56:41,209\tINFO worker.py:1879 -- Started a local Ray instance. View the dashboard at \u001b[1m\u001b[32m127.0.0.1:8265 \u001b[39m\u001b[22m\n",
      "\u001b[33m(raylet)\u001b[0m [2025-05-14 11:56:41,182 E 1111678 1111706] (raylet) file_system_monitor.cc:116: /tmp/ray/session_2025-05-14_11-56-39_352791_1086541 is over 95% full, available space: 81.3692 GB; capacity: 1830.63 GB. Object creation will fail if spilling is required.\n",
      "\u001b[31mFAILED\u001b[0m\n",
      "test_torch_tensor_dag.py::test_torch_tensor_invalid_custom_comm[ray_start_regular0] 2025-05-14 11:56:45,800\tINFO worker.py:1879 -- Started a local Ray instance. View the dashboard at \u001b[1m\u001b[32m127.0.0.1:8265 \u001b[39m\u001b[22m\n",
      "\u001b[33mSKIPPED\u001b[0m\n",
      "test_torch_tensor_dag.py::test_torch_tensor_nccl_static_shape[ray_start_regular0] 2025-05-14 11:56:50,494\tINFO worker.py:1879 -- Started a local Ray instance. View the dashboard at \u001b[1m\u001b[32m127.0.0.1:8265 \u001b[39m\u001b[22m\n",
      "2025-05-14 11:56:52,912\tINFO torch_tensor_nccl_channel.py:772 -- Creating NCCL group 6a02d46e-ef23-41a3-aa91-5144580285fe on actors: [Actor(TorchTensorWorker, a79cc95a06dc122af1ca76d001000000), Actor(TorchTensorWorker, a47641384a87126f7dbd721e01000000)]\n",
      "2025-05-14 11:56:54,118\tINFO torch_tensor_nccl_channel.py:797 -- NCCL group initialized.\n",
      "2025-05-14 11:56:54,269\tINFO dag_node_operation.py:618 -- Writing compiled graph schedule visualization to compiled_graph_schedule.png\n",
      "2025-05-14 11:56:54,538\tINFO compiled_dag_node.py:2173 -- Tearing down compiled DAG\n",
      "\u001b[32mPASSED\u001b[0m2025-05-14 11:56:54,541\tINFO compiled_dag_node.py:2178 -- Cancelling compiled worker on actor: Actor(TorchTensorWorker, a47641384a87126f7dbd721e01000000)\n",
      "2025-05-14 11:56:54,541\tINFO compiled_dag_node.py:2178 -- Cancelling compiled worker on actor: Actor(TorchTensorWorker, a79cc95a06dc122af1ca76d001000000)\n",
      "\u001b[36m(TorchTensorWorker pid=1114065)\u001b[0m ERROR:root:Compiled DAG task exited with exception\n",
      "\u001b[36m(TorchTensorWorker pid=1114065)\u001b[0m Traceback (most recent call last):\n",
      "\u001b[36m(TorchTensorWorker pid=1114065)\u001b[0m   File \"/home/wxdeng/stbr-min/python/ray/dag/compiled_dag_node.py\", line 253, in do_exec_tasks\n",
      "\u001b[36m(TorchTensorWorker pid=1114065)\u001b[0m     done = tasks[operation.exec_task_idx].exec_operation(\n",
      "\u001b[36m(TorchTensorWorker pid=1114065)\u001b[0m   File \"/home/wxdeng/stbr-min/python/ray/dag/compiled_dag_node.py\", line 786, in exec_operation\n",
      "\u001b[36m(TorchTensorWorker pid=1114065)\u001b[0m     return self._write()\n",
      "\u001b[36m(TorchTensorWorker pid=1114065)\u001b[0m   File \"/home/wxdeng/stbr-min/python/ray/dag/compiled_dag_node.py\", line 751, in _write\n",
      "\u001b[36m(TorchTensorWorker pid=1114065)\u001b[0m     self.output_writer.write(output_val)\n",
      "\u001b[36m(TorchTensorWorker pid=1114065)\u001b[0m   File \"/home/wxdeng/stbr-min/python/ray/experimental/channel/common.py\", line 617, in write\n",
      "\u001b[36m(TorchTensorWorker pid=1114065)\u001b[0m     channel.write(val_i, timeout)\n",
      "\u001b[36m(TorchTensorWorker pid=1114065)\u001b[0m   File \"/home/wxdeng/stbr-min/python/ray/experimental/channel/torch_tensor_nccl_channel.py\", line 270, in write\n",
      "\u001b[36m(TorchTensorWorker pid=1114065)\u001b[0m     self._send_cpu_and_gpu_data(value, timeout)\n",
      "\u001b[36m(TorchTensorWorker pid=1114065)\u001b[0m   File \"/home/wxdeng/stbr-min/python/ray/experimental/channel/torch_tensor_nccl_channel.py\", line 203, in _send_cpu_and_gpu_data\n",
      "\u001b[36m(TorchTensorWorker pid=1114065)\u001b[0m     self._gpu_data_channel.write(gpu_tensors)\n",
      "\u001b[36m(TorchTensorWorker pid=1114065)\u001b[0m   File \"/home/wxdeng/stbr-min/python/ray/experimental/channel/torch_tensor_nccl_channel.py\", line 559, in write\n",
      "\u001b[36m(TorchTensorWorker pid=1114065)\u001b[0m     metadata = self._get_send_tensors_metadata(tensors)\n",
      "\u001b[36m(TorchTensorWorker pid=1114065)\u001b[0m   File \"/home/wxdeng/stbr-min/python/ray/experimental/channel/torch_tensor_nccl_channel.py\", line 514, in _get_send_tensors_metadata\n",
      "\u001b[36m(TorchTensorWorker pid=1114065)\u001b[0m     raise ValueError(\n",
      "\u001b[36m(TorchTensorWorker pid=1114065)\u001b[0m ValueError: Expected torch.Tensors with shapes and dtypes: [(shape=torch.Size([10]), dtype=torch.float16)], found: [(shape=torch.Size([20]), dtype=torch.float16)]. DAG will shut down.\n",
      "\u001b[36m(TorchTensorWorker pid=1114065)\u001b[0m Destructing NCCL group on actor: Actor(TorchTensorWorker, a47641384a87126f7dbd721e01000000)\n",
      "\u001b[36m(TorchTensorWorker pid=1114066)\u001b[0m Destructing NCCL group on actor: Actor(TorchTensorWorker, a79cc95a06dc122af1ca76d001000000)\n",
      "2025-05-14 11:56:55,059\tINFO compiled_dag_node.py:2200 -- Waiting for worker tasks to exit\n",
      "2025-05-14 11:56:55,060\tINFO compiled_dag_node.py:2203 -- Teardown complete\n",
      "\n",
      "test_torch_tensor_dag.py::test_torch_tensor_nccl_direct_return[ray_start_regular0] 2025-05-14 11:56:58,676\tINFO worker.py:1879 -- Started a local Ray instance. View the dashboard at \u001b[1m\u001b[32m127.0.0.1:8265 \u001b[39m\u001b[22m\n",
      "\u001b[33m(raylet)\u001b[0m [2025-05-14 11:56:58,636 E 1115215 1115243] (raylet) file_system_monitor.cc:116: /tmp/ray/session_2025-05-14_11-56-56_824333_1086541 is over 95% full, available space: 81.367 GB; capacity: 1830.63 GB. Object creation will fail if spilling is required.\n",
      "2025-05-14 11:57:00,953\tINFO torch_tensor_nccl_channel.py:772 -- Creating NCCL group 19076f14-20c5-47c1-a5f6-4a72d8500405 on actors: [Actor(TorchTensorWorker, b32da9994f43393335a6612e01000000), Actor(TorchTensorWorker, d4552633cd2ba255bb2d828101000000)]\n",
      "2025-05-14 11:57:02,155\tINFO torch_tensor_nccl_channel.py:797 -- NCCL group initialized.\n",
      "2025-05-14 11:57:02,324\tINFO dag_node_operation.py:618 -- Writing compiled graph schedule visualization to compiled_graph_schedule.png\n",
      "2025-05-14 11:57:02,555\tINFO compiled_dag_node.py:2173 -- Tearing down compiled DAG\n",
      "\u001b[32mPASSED\u001b[0m2025-05-14 11:57:02,558\tINFO compiled_dag_node.py:2178 -- Cancelling compiled worker on actor: Actor(TorchTensorWorker, d4552633cd2ba255bb2d828101000000)\n",
      "2025-05-14 11:57:02,559\tINFO compiled_dag_node.py:2178 -- Cancelling compiled worker on actor: Actor(TorchTensorWorker, b32da9994f43393335a6612e01000000)\n",
      "\u001b[36m(TorchTensorWorker pid=1115306)\u001b[0m ERROR:root:Compiled DAG task exited with exception\n",
      "\u001b[36m(TorchTensorWorker pid=1115306)\u001b[0m Traceback (most recent call last):\n",
      "\u001b[36m(TorchTensorWorker pid=1115306)\u001b[0m   File \"/home/wxdeng/stbr-min/python/ray/dag/compiled_dag_node.py\", line 253, in do_exec_tasks\n",
      "\u001b[36m(TorchTensorWorker pid=1115306)\u001b[0m     done = tasks[operation.exec_task_idx].exec_operation(\n",
      "\u001b[36m(TorchTensorWorker pid=1115306)\u001b[0m   File \"/home/wxdeng/stbr-min/python/ray/dag/compiled_dag_node.py\", line 786, in exec_operation\n",
      "\u001b[36m(TorchTensorWorker pid=1115306)\u001b[0m     return self._write()\n",
      "\u001b[36m(TorchTensorWorker pid=1115306)\u001b[0m   File \"/home/wxdeng/stbr-min/python/ray/dag/compiled_dag_node.py\", line 751, in _write\n",
      "\u001b[36m(TorchTensorWorker pid=1115306)\u001b[0m     self.output_writer.write(output_val)\n",
      "\u001b[36m(TorchTensorWorker pid=1115306)\u001b[0m   File \"/home/wxdeng/stbr-min/python/ray/experimental/channel/common.py\", line 617, in write\n",
      "\u001b[36m(TorchTensorWorker pid=1115306)\u001b[0m     channel.write(val_i, timeout)\n",
      "\u001b[36m(TorchTensorWorker pid=1115306)\u001b[0m   File \"/home/wxdeng/stbr-min/python/ray/experimental/channel/torch_tensor_nccl_channel.py\", line 257, in write\n",
      "\u001b[36m(TorchTensorWorker pid=1115306)\u001b[0m     raise ValueError(\n",
      "\u001b[36m(TorchTensorWorker pid=1115306)\u001b[0m ValueError: Task annotated with _direct_return=True must return a CUDA torch.Tensor, instead found value `1`. DAG will shut down.\n",
      "\u001b[36m(TorchTensorWorker pid=1115306)\u001b[0m Destructing NCCL group on actor: Actor(TorchTensorWorker, d4552633cd2ba255bb2d828101000000)\n",
      "\u001b[36m(TorchTensorWorker pid=1115304)\u001b[0m Destructing NCCL group on actor: Actor(TorchTensorWorker, b32da9994f43393335a6612e01000000)\n",
      "2025-05-14 11:57:03,077\tINFO compiled_dag_node.py:2200 -- Waiting for worker tasks to exit\n",
      "2025-05-14 11:57:03,078\tINFO compiled_dag_node.py:2203 -- Teardown complete\n",
      "\n",
      "test_torch_tensor_dag.py::test_torch_tensor_nccl_nested_dynamic[ray_start_regular0] 2025-05-14 11:57:06,765\tINFO worker.py:1879 -- Started a local Ray instance. View the dashboard at \u001b[1m\u001b[32m127.0.0.1:8265 \u001b[39m\u001b[22m\n",
      "2025-05-14 11:57:09,125\tINFO torch_tensor_nccl_channel.py:772 -- Creating NCCL group eee4893b-9da3-45bb-b916-958fd7acc7c3 on actors: [Actor(TorchTensorWorker, f356e4aa1b99fd60ac16340501000000), Actor(TorchTensorWorker, bc085c1c0ba6f36d9395579201000000)]\n",
      "2025-05-14 11:57:10,326\tINFO torch_tensor_nccl_channel.py:797 -- NCCL group initialized.\n",
      "2025-05-14 11:57:10,413\tINFO dag_node_operation.py:618 -- Writing compiled graph schedule visualization to compiled_graph_schedule.png\n",
      "\u001b[32mPASSED\u001b[0m2025-05-14 11:57:10,622\tINFO compiled_dag_node.py:2173 -- Tearing down compiled DAG\n",
      "2025-05-14 11:57:10,623\tINFO compiled_dag_node.py:2178 -- Cancelling compiled worker on actor: Actor(TorchTensorWorker, bc085c1c0ba6f36d9395579201000000)\n",
      "2025-05-14 11:57:10,623\tINFO compiled_dag_node.py:2178 -- Cancelling compiled worker on actor: Actor(TorchTensorWorker, f356e4aa1b99fd60ac16340501000000)\n",
      "\u001b[36m(TorchTensorWorker pid=1116534)\u001b[0m Destructing NCCL group on actor: Actor(TorchTensorWorker, f356e4aa1b99fd60ac16340501000000)\n",
      "\u001b[36m(TorchTensorWorker pid=1116532)\u001b[0m Destructing NCCL group on actor: Actor(TorchTensorWorker, bc085c1c0ba6f36d9395579201000000)\n",
      "2025-05-14 11:57:11,332\tINFO compiled_dag_node.py:2200 -- Waiting for worker tasks to exit\n",
      "2025-05-14 11:57:11,332\tINFO compiled_dag_node.py:2161 -- Killing actor: Actor(TorchTensorWorker, bc085c1c0ba6f36d9395579201000000)\n",
      "2025-05-14 11:57:11,332\tINFO compiled_dag_node.py:2161 -- Killing actor: Actor(TorchTensorWorker, f356e4aa1b99fd60ac16340501000000)\n",
      "2025-05-14 11:57:11,333\tINFO compiled_dag_node.py:2203 -- Teardown complete\n",
      "\n",
      "test_torch_tensor_dag.py::test_torch_tensor_exceptions[False-False-False-ray_start_regular0] 2025-05-14 11:57:15,966\tINFO worker.py:1879 -- Started a local Ray instance. View the dashboard at \u001b[1m\u001b[32m127.0.0.1:8265 \u001b[39m\u001b[22m\n",
      "2025-05-14 11:57:18,355\tINFO torch_tensor_nccl_channel.py:772 -- Creating NCCL group a8bea5b5-2142-404c-a64a-f915db49ddc9 on actors: [Actor(TorchTensorWorker, f998c7cb472b3214395f593301000000), Actor(TorchTensorWorker, 853b54e1a51c7c9ce187877601000000)]\n",
      "2025-05-14 11:57:19,551\tINFO torch_tensor_nccl_channel.py:797 -- NCCL group initialized.\n",
      "2025-05-14 11:57:19,740\tINFO dag_node_operation.py:618 -- Writing compiled graph schedule visualization to compiled_graph_schedule.png\n",
      "\u001b[32mPASSED\u001b[0m2025-05-14 11:57:19,970\tINFO compiled_dag_node.py:2173 -- Tearing down compiled DAG\n",
      "2025-05-14 11:57:19,971\tINFO compiled_dag_node.py:2178 -- Cancelling compiled worker on actor: Actor(TorchTensorWorker, 853b54e1a51c7c9ce187877601000000)\n",
      "2025-05-14 11:57:19,971\tINFO compiled_dag_node.py:2178 -- Cancelling compiled worker on actor: Actor(TorchTensorWorker, f998c7cb472b3214395f593301000000)\n",
      "\u001b[36m(TorchTensorWorker pid=1117835)\u001b[0m Destructing NCCL group on actor: Actor(TorchTensorWorker, 853b54e1a51c7c9ce187877601000000)\n",
      "\u001b[36m(TorchTensorWorker pid=1117836)\u001b[0m Destructing NCCL group on actor: Actor(TorchTensorWorker, f998c7cb472b3214395f593301000000)\n",
      "2025-05-14 11:57:20,488\tINFO compiled_dag_node.py:2200 -- Waiting for worker tasks to exit\n",
      "2025-05-14 11:57:20,489\tINFO compiled_dag_node.py:2161 -- Killing actor: Actor(TorchTensorWorker, 853b54e1a51c7c9ce187877601000000)\n",
      "2025-05-14 11:57:20,489\tINFO compiled_dag_node.py:2161 -- Killing actor: Actor(TorchTensorWorker, f998c7cb472b3214395f593301000000)\n",
      "2025-05-14 11:57:20,490\tINFO compiled_dag_node.py:2203 -- Teardown complete\n",
      "\n",
      "test_torch_tensor_dag.py::test_torch_tensor_exceptions[False-False-True-ray_start_regular0] 2025-05-14 11:57:24,174\tINFO worker.py:1879 -- Started a local Ray instance. View the dashboard at \u001b[1m\u001b[32m127.0.0.1:8265 \u001b[39m\u001b[22m\n",
      "2025-05-14 11:57:26,579\tINFO torch_tensor_nccl_channel.py:772 -- Creating NCCL group 006aca02-1d0b-4fe6-bc2c-e417d66ebbb8 on actors: [Actor(TorchTensorWorker, cd62316d65b1972a06e1977501000000), Actor(TorchTensorWorker, 0d038d533577e66c29c26ad701000000)]\n",
      "2025-05-14 11:57:27,773\tINFO torch_tensor_nccl_channel.py:797 -- NCCL group initialized.\n",
      "2025-05-14 11:57:27,959\tINFO dag_node_operation.py:618 -- Writing compiled graph schedule visualization to compiled_graph_schedule.png\n",
      "2025-05-14 11:57:28,196\tINFO compiled_dag_node.py:2173 -- Tearing down compiled DAG\n",
      "\u001b[32mPASSED\u001b[0m2025-05-14 11:57:28,200\tINFO compiled_dag_node.py:2178 -- Cancelling compiled worker on actor: Actor(TorchTensorWorker, 0d038d533577e66c29c26ad701000000)\n",
      "2025-05-14 11:57:28,200\tINFO compiled_dag_node.py:2178 -- Cancelling compiled worker on actor: Actor(TorchTensorWorker, cd62316d65b1972a06e1977501000000)\n",
      "\u001b[36m(TorchTensorWorker pid=1119120)\u001b[0m Destructing NCCL group on actor: Actor(TorchTensorWorker, cd62316d65b1972a06e1977501000000)\n",
      "\u001b[36m(TorchTensorWorker pid=1119119)\u001b[0m ERROR:root:Compiled DAG task exited with exception\n",
      "\u001b[36m(TorchTensorWorker pid=1119119)\u001b[0m Traceback (most recent call last):\n",
      "\u001b[36m(TorchTensorWorker pid=1119119)\u001b[0m   File \"/home/wxdeng/stbr-min/python/ray/dag/compiled_dag_node.py\", line 253, in do_exec_tasks\n",
      "\u001b[36m(TorchTensorWorker pid=1119119)\u001b[0m     done = tasks[operation.exec_task_idx].exec_operation(\n",
      "\u001b[36m(TorchTensorWorker pid=1119119)\u001b[0m   File \"/home/wxdeng/stbr-min/python/ray/dag/compiled_dag_node.py\", line 786, in exec_operation\n",
      "\u001b[36m(TorchTensorWorker pid=1119119)\u001b[0m     return self._write()\n",
      "\u001b[36m(TorchTensorWorker pid=1119119)\u001b[0m   File \"/home/wxdeng/stbr-min/python/ray/dag/compiled_dag_node.py\", line 751, in _write\n",
      "\u001b[36m(TorchTensorWorker pid=1119119)\u001b[0m     self.output_writer.write(output_val)\n",
      "\u001b[36m(TorchTensorWorker pid=1119119)\u001b[0m   File \"/home/wxdeng/stbr-min/python/ray/experimental/channel/common.py\", line 617, in write\n",
      "\u001b[36m(TorchTensorWorker pid=1119119)\u001b[0m     channel.write(val_i, timeout)\n",
      "\u001b[36m(TorchTensorWorker pid=1119119)\u001b[0m   File \"/home/wxdeng/stbr-min/python/ray/experimental/channel/torch_tensor_nccl_channel.py\", line 243, in write\n",
      "\u001b[36m(TorchTensorWorker pid=1119119)\u001b[0m     raise value\n",
      "\u001b[36m(TorchTensorWorker pid=1119119)\u001b[0m ray.exceptions.RayTaskError: \u001b[36mray::TorchTensorWorker.__ray_call__()\u001b[39m (pid=1119119, ip=128.208.3.124)\n",
      "\u001b[36m(TorchTensorWorker pid=1119119)\u001b[0m   File \"/home/wxdeng/stbr-min/python/ray/dag/tests/experimental/test_torch_tensor_dag.py\", line 67, in send_or_raise\n",
      "\u001b[36m(TorchTensorWorker pid=1119119)\u001b[0m     raise RuntimeError()\n",
      "\u001b[36m(TorchTensorWorker pid=1119119)\u001b[0m RuntimeError\n",
      "\u001b[36m(TorchTensorWorker pid=1119119)\u001b[0m Destructing NCCL group on actor: Actor(TorchTensorWorker, 0d038d533577e66c29c26ad701000000)\n",
      "2025-05-14 11:57:28,688\tINFO compiled_dag_node.py:2200 -- Waiting for worker tasks to exit\n",
      "2025-05-14 11:57:28,689\tINFO compiled_dag_node.py:2203 -- Teardown complete\n",
      "\n",
      "test_torch_tensor_dag.py::test_torch_tensor_exceptions[False-True-False-ray_start_regular0] 2025-05-14 11:57:33,175\tINFO worker.py:1879 -- Started a local Ray instance. View the dashboard at \u001b[1m\u001b[32m127.0.0.1:8265 \u001b[39m\u001b[22m\n",
      "2025-05-14 11:57:35,489\tINFO torch_tensor_nccl_channel.py:772 -- Creating NCCL group 08dda025-802c-4604-b47c-65d4ecbfa73a on actors: [Actor(TorchTensorWorker, 519ee3616f8bd207851b688c01000000), Actor(TorchTensorWorker, e4199a3c1f4b3f6339eb141901000000)]\n",
      "2025-05-14 11:57:36,686\tINFO torch_tensor_nccl_channel.py:797 -- NCCL group initialized.\n",
      "2025-05-14 11:57:36,852\tINFO dag_node_operation.py:618 -- Writing compiled graph schedule visualization to compiled_graph_schedule.png\n",
      "2025-05-14 11:57:37,079\tINFO compiled_dag_node.py:2173 -- Tearing down compiled DAG\n",
      "\u001b[32mPASSED\u001b[0m2025-05-14 11:57:37,082\tINFO compiled_dag_node.py:2178 -- Cancelling compiled worker on actor: Actor(TorchTensorWorker, e4199a3c1f4b3f6339eb141901000000)\n",
      "2025-05-14 11:57:37,082\tINFO compiled_dag_node.py:2178 -- Cancelling compiled worker on actor: Actor(TorchTensorWorker, 519ee3616f8bd207851b688c01000000)\n",
      "\u001b[36m(TorchTensorWorker pid=1120418)\u001b[0m ERROR:root:Compiled DAG task exited with exception\n",
      "\u001b[36m(TorchTensorWorker pid=1120418)\u001b[0m Traceback (most recent call last):\n",
      "\u001b[36m(TorchTensorWorker pid=1120418)\u001b[0m   File \"/home/wxdeng/stbr-min/python/ray/dag/compiled_dag_node.py\", line 253, in do_exec_tasks\n",
      "\u001b[36m(TorchTensorWorker pid=1120418)\u001b[0m     done = tasks[operation.exec_task_idx].exec_operation(\n",
      "\u001b[36m(TorchTensorWorker pid=1120418)\u001b[0m   File \"/home/wxdeng/stbr-min/python/ray/dag/compiled_dag_node.py\", line 786, in exec_operation\n",
      "\u001b[36m(TorchTensorWorker pid=1120418)\u001b[0m     return self._write()\n",
      "\u001b[36m(TorchTensorWorker pid=1120418)\u001b[0m   File \"/home/wxdeng/stbr-min/python/ray/dag/compiled_dag_node.py\", line 751, in _write\n",
      "\u001b[36m(TorchTensorWorker pid=1120418)\u001b[0m     self.output_writer.write(output_val)\n",
      "\u001b[36m(TorchTensorWorker pid=1120418)\u001b[0m   File \"/home/wxdeng/stbr-min/python/ray/experimental/channel/common.py\", line 617, in write\n",
      "\u001b[36m(TorchTensorWorker pid=1120418)\u001b[0m     channel.write(val_i, timeout)\n",
      "\u001b[36m(TorchTensorWorker pid=1120418)\u001b[0m   File \"/home/wxdeng/stbr-min/python/ray/experimental/channel/torch_tensor_nccl_channel.py\", line 243, in write\n",
      "\u001b[36m(TorchTensorWorker pid=1120418)\u001b[0m     raise value\n",
      "\u001b[36m(TorchTensorWorker pid=1120418)\u001b[0m ray.exceptions.RayTaskError: \u001b[36mray::TorchTensorWorker.__ray_call__()\u001b[39m (pid=1120418, ip=128.208.3.124)\n",
      "\u001b[36m(TorchTensorWorker pid=1120418)\u001b[0m   File \"/home/wxdeng/stbr-min/python/ray/dag/tests/experimental/test_torch_tensor_dag.py\", line 67, in send_or_raise\n",
      "\u001b[36m(TorchTensorWorker pid=1120418)\u001b[0m     raise RuntimeError()\n",
      "\u001b[36m(TorchTensorWorker pid=1120418)\u001b[0m RuntimeError\n",
      "\u001b[36m(TorchTensorWorker pid=1120418)\u001b[0m Destructing NCCL group on actor: Actor(TorchTensorWorker, e4199a3c1f4b3f6339eb141901000000)\n",
      "\u001b[36m(TorchTensorWorker pid=1120420)\u001b[0m Destructing NCCL group on actor: Actor(TorchTensorWorker, 519ee3616f8bd207851b688c01000000)\n",
      "2025-05-14 11:57:37,571\tINFO compiled_dag_node.py:2200 -- Waiting for worker tasks to exit\n",
      "2025-05-14 11:57:37,571\tINFO compiled_dag_node.py:2203 -- Teardown complete\n",
      "\n",
      "test_torch_tensor_dag.py::test_torch_tensor_exceptions[False-True-True-ray_start_regular0] 2025-05-14 11:57:42,240\tINFO worker.py:1879 -- Started a local Ray instance. View the dashboard at \u001b[1m\u001b[32m127.0.0.1:8265 \u001b[39m\u001b[22m\n",
      "2025-05-14 11:57:44,539\tINFO torch_tensor_nccl_channel.py:772 -- Creating NCCL group 61ab4416-5322-4345-8c26-6adb41083ba8 on actors: [Actor(TorchTensorWorker, 953f64af3792efbfee0eeaa501000000), Actor(TorchTensorWorker, ae0d2dedad79e50410b2204101000000)]\n",
      "2025-05-14 11:57:45,739\tINFO torch_tensor_nccl_channel.py:797 -- NCCL group initialized.\n",
      "2025-05-14 11:57:45,920\tINFO dag_node_operation.py:618 -- Writing compiled graph schedule visualization to compiled_graph_schedule.png\n",
      "2025-05-14 11:57:46,157\tINFO compiled_dag_node.py:2173 -- Tearing down compiled DAG\n",
      "\u001b[32mPASSED\u001b[0m2025-05-14 11:57:46,161\tINFO compiled_dag_node.py:2178 -- Cancelling compiled worker on actor: Actor(TorchTensorWorker, ae0d2dedad79e50410b2204101000000)\n",
      "2025-05-14 11:57:46,161\tINFO compiled_dag_node.py:2178 -- Cancelling compiled worker on actor: Actor(TorchTensorWorker, 953f64af3792efbfee0eeaa501000000)\n",
      "\u001b[36m(TorchTensorWorker pid=1121670)\u001b[0m ERROR:root:Compiled DAG task exited with exception\n",
      "\u001b[36m(TorchTensorWorker pid=1121670)\u001b[0m Traceback (most recent call last):\n",
      "\u001b[36m(TorchTensorWorker pid=1121670)\u001b[0m   File \"/home/wxdeng/stbr-min/python/ray/dag/compiled_dag_node.py\", line 253, in do_exec_tasks\n",
      "\u001b[36m(TorchTensorWorker pid=1121670)\u001b[0m     done = tasks[operation.exec_task_idx].exec_operation(\n",
      "\u001b[36m(TorchTensorWorker pid=1121670)\u001b[0m   File \"/home/wxdeng/stbr-min/python/ray/dag/compiled_dag_node.py\", line 786, in exec_operation\n",
      "\u001b[36m(TorchTensorWorker pid=1121670)\u001b[0m     return self._write()\n",
      "\u001b[36m(TorchTensorWorker pid=1121670)\u001b[0m   File \"/home/wxdeng/stbr-min/python/ray/dag/compiled_dag_node.py\", line 751, in _write\n",
      "\u001b[36m(TorchTensorWorker pid=1121670)\u001b[0m     self.output_writer.write(output_val)\n",
      "\u001b[36m(TorchTensorWorker pid=1121670)\u001b[0m   File \"/home/wxdeng/stbr-min/python/ray/experimental/channel/common.py\", line 617, in write\n",
      "\u001b[36m(TorchTensorWorker pid=1121670)\u001b[0m     channel.write(val_i, timeout)\n",
      "\u001b[36m(TorchTensorWorker pid=1121670)\u001b[0m   File \"/home/wxdeng/stbr-min/python/ray/experimental/channel/torch_tensor_nccl_channel.py\", line 243, in write\n",
      "\u001b[36m(TorchTensorWorker pid=1121670)\u001b[0m     raise value\n",
      "\u001b[36m(TorchTensorWorker pid=1121670)\u001b[0m ray.exceptions.RayTaskError: \u001b[36mray::TorchTensorWorker.__ray_call__()\u001b[39m (pid=1121670, ip=128.208.3.124)\n",
      "\u001b[36m(TorchTensorWorker pid=1121670)\u001b[0m   File \"/home/wxdeng/stbr-min/python/ray/dag/tests/experimental/test_torch_tensor_dag.py\", line 67, in send_or_raise\n",
      "\u001b[36m(TorchTensorWorker pid=1121670)\u001b[0m     raise RuntimeError()\n",
      "\u001b[36m(TorchTensorWorker pid=1121670)\u001b[0m RuntimeError\n",
      "\u001b[36m(TorchTensorWorker pid=1121670)\u001b[0m Destructing NCCL group on actor: Actor(TorchTensorWorker, ae0d2dedad79e50410b2204101000000)\n",
      "\u001b[36m(TorchTensorWorker pid=1121671)\u001b[0m Destructing NCCL group on actor: Actor(TorchTensorWorker, 953f64af3792efbfee0eeaa501000000)\n",
      "2025-05-14 11:57:46,681\tINFO compiled_dag_node.py:2200 -- Waiting for worker tasks to exit\n",
      "2025-05-14 11:57:46,681\tINFO compiled_dag_node.py:2203 -- Teardown complete\n",
      "\n",
      "test_torch_tensor_dag.py::test_torch_tensor_exceptions[True-False-False-ray_start_regular0] 2025-05-14 11:57:51,395\tINFO worker.py:1879 -- Started a local Ray instance. View the dashboard at \u001b[1m\u001b[32m127.0.0.1:8265 \u001b[39m\u001b[22m\n",
      "2025-05-14 11:57:53,763\tINFO torch_tensor_nccl_channel.py:772 -- Creating NCCL group 0c9983f3-08f0-4b43-b8ec-271c9224a9b1 on actors: [Actor(TorchTensorWorker, 190f7b460ea19cfe293a891301000000), Actor(TorchTensorWorker, 2971b8342f2c8d7cd0d5445101000000)]\n",
      "2025-05-14 11:57:54,958\tINFO torch_tensor_nccl_channel.py:797 -- NCCL group initialized.\n",
      "2025-05-14 11:57:55,144\tINFO dag_node_operation.py:618 -- Writing compiled graph schedule visualization to compiled_graph_schedule.png\n",
      "\u001b[32mPASSED\u001b[0m2025-05-14 11:57:55,418\tINFO compiled_dag_node.py:2173 -- Tearing down compiled DAG\n",
      "2025-05-14 11:57:55,419\tINFO compiled_dag_node.py:2178 -- Cancelling compiled worker on actor: Actor(TorchTensorWorker, 190f7b460ea19cfe293a891301000000)\n",
      "2025-05-14 11:57:55,419\tINFO compiled_dag_node.py:2178 -- Cancelling compiled worker on actor: Actor(TorchTensorWorker, 2971b8342f2c8d7cd0d5445101000000)\n",
      "\u001b[36m(TorchTensorWorker pid=1122948)\u001b[0m Destructing NCCL group on actor: Actor(TorchTensorWorker, 2971b8342f2c8d7cd0d5445101000000)\n",
      "\u001b[36m(TorchTensorWorker pid=1122949)\u001b[0m Destructing NCCL group on actor: Actor(TorchTensorWorker, 190f7b460ea19cfe293a891301000000)\n",
      "2025-05-14 11:57:55,938\tINFO compiled_dag_node.py:2200 -- Waiting for worker tasks to exit\n",
      "2025-05-14 11:57:55,938\tINFO compiled_dag_node.py:2161 -- Killing actor: Actor(TorchTensorWorker, 190f7b460ea19cfe293a891301000000)\n",
      "2025-05-14 11:57:55,939\tINFO compiled_dag_node.py:2161 -- Killing actor: Actor(TorchTensorWorker, 2971b8342f2c8d7cd0d5445101000000)\n",
      "2025-05-14 11:57:55,939\tINFO compiled_dag_node.py:2203 -- Teardown complete\n",
      "\n",
      "test_torch_tensor_dag.py::test_torch_tensor_exceptions[True-False-True-ray_start_regular0] 2025-05-14 11:57:59,686\tINFO worker.py:1879 -- Started a local Ray instance. View the dashboard at \u001b[1m\u001b[32m127.0.0.1:8265 \u001b[39m\u001b[22m\n",
      "\u001b[33m(raylet)\u001b[0m [2025-05-14 11:57:59,654 E 1124100 1124128] (raylet) file_system_monitor.cc:116: /tmp/ray/session_2025-05-14_11-57-57_842239_1086541 is over 95% full, available space: 81.3616 GB; capacity: 1830.63 GB. Object creation will fail if spilling is required.\n",
      "2025-05-14 11:58:01,979\tINFO torch_tensor_nccl_channel.py:772 -- Creating NCCL group 4a62d72e-14e9-4137-9314-4d5a9615dcb6 on actors: [Actor(TorchTensorWorker, 36cefedf32198375dfc1311501000000), Actor(TorchTensorWorker, f4e515545b099179cb655eab01000000)]\n",
      "2025-05-14 11:58:03,186\tINFO torch_tensor_nccl_channel.py:797 -- NCCL group initialized.\n",
      "2025-05-14 11:58:03,368\tINFO dag_node_operation.py:618 -- Writing compiled graph schedule visualization to compiled_graph_schedule.png\n",
      "2025-05-14 11:58:03,617\tINFO compiled_dag_node.py:2173 -- Tearing down compiled DAG\n",
      "\u001b[32mPASSED\u001b[0m2025-05-14 11:58:03,621\tINFO compiled_dag_node.py:2178 -- Cancelling compiled worker on actor: Actor(TorchTensorWorker, f4e515545b099179cb655eab01000000)\n",
      "2025-05-14 11:58:03,621\tINFO compiled_dag_node.py:2178 -- Cancelling compiled worker on actor: Actor(TorchTensorWorker, 36cefedf32198375dfc1311501000000)\n",
      "\u001b[36m(TorchTensorWorker pid=1124190)\u001b[0m ERROR:root:Compiled DAG task exited with exception\n",
      "\u001b[36m(TorchTensorWorker pid=1124190)\u001b[0m Traceback (most recent call last):\n",
      "\u001b[36m(TorchTensorWorker pid=1124190)\u001b[0m   File \"/home/wxdeng/stbr-min/python/ray/dag/compiled_dag_node.py\", line 253, in do_exec_tasks\n",
      "\u001b[36m(TorchTensorWorker pid=1124190)\u001b[0m     done = tasks[operation.exec_task_idx].exec_operation(\n",
      "\u001b[36m(TorchTensorWorker pid=1124190)\u001b[0m   File \"/home/wxdeng/stbr-min/python/ray/dag/compiled_dag_node.py\", line 786, in exec_operation\n",
      "\u001b[36m(TorchTensorWorker pid=1124190)\u001b[0m     return self._write()\n",
      "\u001b[36m(TorchTensorWorker pid=1124190)\u001b[0m   File \"/home/wxdeng/stbr-min/python/ray/dag/compiled_dag_node.py\", line 751, in _write\n",
      "\u001b[36m(TorchTensorWorker pid=1124190)\u001b[0m     self.output_writer.write(output_val)\n",
      "\u001b[36m(TorchTensorWorker pid=1124190)\u001b[0m   File \"/home/wxdeng/stbr-min/python/ray/experimental/channel/common.py\", line 617, in write\n",
      "\u001b[36m(TorchTensorWorker pid=1124190)\u001b[0m     channel.write(val_i, timeout)\n",
      "\u001b[36m(TorchTensorWorker pid=1124190)\u001b[0m   File \"/home/wxdeng/stbr-min/python/ray/experimental/channel/torch_tensor_nccl_channel.py\", line 243, in write\n",
      "\u001b[36m(TorchTensorWorker pid=1124190)\u001b[0m     raise value\n",
      "\u001b[36m(TorchTensorWorker pid=1124190)\u001b[0m ray.exceptions.RayTaskError: \u001b[36mray::TorchTensorWorker.__ray_call__()\u001b[39m (pid=1124190, ip=128.208.3.124)\n",
      "\u001b[36m(TorchTensorWorker pid=1124190)\u001b[0m   File \"/home/wxdeng/stbr-min/python/ray/dag/tests/experimental/test_torch_tensor_dag.py\", line 67, in send_or_raise\n",
      "\u001b[36m(TorchTensorWorker pid=1124190)\u001b[0m     raise RuntimeError()\n",
      "\u001b[36m(TorchTensorWorker pid=1124190)\u001b[0m RuntimeError\n",
      "\u001b[36m(TorchTensorWorker pid=1124190)\u001b[0m Destructing NCCL group on actor: Actor(TorchTensorWorker, f4e515545b099179cb655eab01000000)\n",
      "\u001b[36m(TorchTensorWorker pid=1124189)\u001b[0m Destructing NCCL group on actor: Actor(TorchTensorWorker, 36cefedf32198375dfc1311501000000)\n",
      "2025-05-14 11:58:04,140\tINFO compiled_dag_node.py:2200 -- Waiting for worker tasks to exit\n",
      "2025-05-14 11:58:04,140\tINFO compiled_dag_node.py:2203 -- Teardown complete\n",
      "\n",
      "test_torch_tensor_dag.py::test_torch_tensor_exceptions[True-True-False-ray_start_regular0] 2025-05-14 11:58:07,797\tINFO worker.py:1879 -- Started a local Ray instance. View the dashboard at \u001b[1m\u001b[32m127.0.0.1:8265 \u001b[39m\u001b[22m\n",
      "\u001b[33m(raylet)\u001b[0m [2025-05-14 11:58:07,768 E 1125319 1125347] (raylet) file_system_monitor.cc:116: /tmp/ray/session_2025-05-14_11-58-05_933000_1086541 is over 95% full, available space: 81.3609 GB; capacity: 1830.63 GB. Object creation will fail if spilling is required.\n",
      "2025-05-14 11:58:10,098\tINFO torch_tensor_nccl_channel.py:772 -- Creating NCCL group 4a6fcb3c-fe76-4572-acda-b4df8515e5cb on actors: [Actor(TorchTensorWorker, 07ade4ec1ad0000aa8291b9a01000000), Actor(TorchTensorWorker, aec2373ae1e21e549417d69201000000)]\n",
      "2025-05-14 11:58:11,284\tINFO torch_tensor_nccl_channel.py:797 -- NCCL group initialized.\n",
      "2025-05-14 11:58:11,429\tINFO dag_node_operation.py:618 -- Writing compiled graph schedule visualization to compiled_graph_schedule.png\n",
      "2025-05-14 11:58:11,695\tINFO compiled_dag_node.py:2173 -- Tearing down compiled DAG\n",
      "\u001b[32mPASSED\u001b[0m2025-05-14 11:58:11,698\tINFO compiled_dag_node.py:2178 -- Cancelling compiled worker on actor: Actor(TorchTensorWorker, aec2373ae1e21e549417d69201000000)\n",
      "2025-05-14 11:58:11,698\tINFO compiled_dag_node.py:2178 -- Cancelling compiled worker on actor: Actor(TorchTensorWorker, 07ade4ec1ad0000aa8291b9a01000000)\n",
      "\u001b[36m(TorchTensorWorker pid=1125409)\u001b[0m Destructing NCCL group on actor: Actor(TorchTensorWorker, 07ade4ec1ad0000aa8291b9a01000000)\n",
      "\u001b[36m(TorchTensorWorker pid=1125408)\u001b[0m ERROR:root:Compiled DAG task exited with exception\n",
      "\u001b[36m(TorchTensorWorker pid=1125408)\u001b[0m Traceback (most recent call last):\n",
      "\u001b[36m(TorchTensorWorker pid=1125408)\u001b[0m   File \"/home/wxdeng/stbr-min/python/ray/dag/compiled_dag_node.py\", line 253, in do_exec_tasks\n",
      "\u001b[36m(TorchTensorWorker pid=1125408)\u001b[0m     done = tasks[operation.exec_task_idx].exec_operation(\n",
      "\u001b[36m(TorchTensorWorker pid=1125408)\u001b[0m   File \"/home/wxdeng/stbr-min/python/ray/dag/compiled_dag_node.py\", line 786, in exec_operation\n",
      "\u001b[36m(TorchTensorWorker pid=1125408)\u001b[0m     return self._write()\n",
      "\u001b[36m(TorchTensorWorker pid=1125408)\u001b[0m   File \"/home/wxdeng/stbr-min/python/ray/dag/compiled_dag_node.py\", line 751, in _write\n",
      "\u001b[36m(TorchTensorWorker pid=1125408)\u001b[0m     self.output_writer.write(output_val)\n",
      "\u001b[36m(TorchTensorWorker pid=1125408)\u001b[0m   File \"/home/wxdeng/stbr-min/python/ray/experimental/channel/common.py\", line 617, in write\n",
      "\u001b[36m(TorchTensorWorker pid=1125408)\u001b[0m     channel.write(val_i, timeout)\n",
      "\u001b[36m(TorchTensorWorker pid=1125408)\u001b[0m   File \"/home/wxdeng/stbr-min/python/ray/experimental/channel/torch_tensor_nccl_channel.py\", line 243, in write\n",
      "\u001b[36m(TorchTensorWorker pid=1125408)\u001b[0m     raise value\n",
      "\u001b[36m(TorchTensorWorker pid=1125408)\u001b[0m ray.exceptions.RayTaskError: \u001b[36mray::TorchTensorWorker.__ray_call__()\u001b[39m (pid=1125408, ip=128.208.3.124)\n",
      "\u001b[36m(TorchTensorWorker pid=1125408)\u001b[0m   File \"/home/wxdeng/stbr-min/python/ray/dag/tests/experimental/test_torch_tensor_dag.py\", line 67, in send_or_raise\n",
      "\u001b[36m(TorchTensorWorker pid=1125408)\u001b[0m     raise RuntimeError()\n",
      "\u001b[36m(TorchTensorWorker pid=1125408)\u001b[0m RuntimeError\n",
      "\u001b[36m(TorchTensorWorker pid=1125408)\u001b[0m Destructing NCCL group on actor: Actor(TorchTensorWorker, aec2373ae1e21e549417d69201000000)\n",
      "2025-05-14 11:58:12,219\tINFO compiled_dag_node.py:2200 -- Waiting for worker tasks to exit\n",
      "2025-05-14 11:58:12,219\tINFO compiled_dag_node.py:2203 -- Teardown complete\n",
      "\n",
      "test_torch_tensor_dag.py::test_torch_tensor_exceptions[True-True-True-ray_start_regular0] 2025-05-14 11:58:16,716\tINFO worker.py:1879 -- Started a local Ray instance. View the dashboard at \u001b[1m\u001b[32m127.0.0.1:8265 \u001b[39m\u001b[22m\n",
      "2025-05-14 11:58:19,113\tINFO torch_tensor_nccl_channel.py:772 -- Creating NCCL group 49ff4474-0da6-44c9-81ec-7507f0ddcb00 on actors: [Actor(TorchTensorWorker, 6f22ed9f403cef6db8a1329001000000), Actor(TorchTensorWorker, 34adc447265da6d7211b4fbe01000000)]\n",
      "2025-05-14 11:58:20,318\tINFO torch_tensor_nccl_channel.py:797 -- NCCL group initialized.\n",
      "2025-05-14 11:58:20,494\tINFO dag_node_operation.py:618 -- Writing compiled graph schedule visualization to compiled_graph_schedule.png\n",
      "2025-05-14 11:58:20,727\tINFO compiled_dag_node.py:2173 -- Tearing down compiled DAG\n",
      "\u001b[32mPASSED\u001b[0m2025-05-14 11:58:20,731\tINFO compiled_dag_node.py:2178 -- Cancelling compiled worker on actor: Actor(TorchTensorWorker, 34adc447265da6d7211b4fbe01000000)\n",
      "2025-05-14 11:58:20,731\tINFO compiled_dag_node.py:2178 -- Cancelling compiled worker on actor: Actor(TorchTensorWorker, 6f22ed9f403cef6db8a1329001000000)\n",
      "\u001b[36m(TorchTensorWorker pid=1126706)\u001b[0m Destructing NCCL group on actor: Actor(TorchTensorWorker, 6f22ed9f403cef6db8a1329001000000)\n",
      "\u001b[36m(TorchTensorWorker pid=1126705)\u001b[0m ERROR:root:Compiled DAG task exited with exception\n",
      "\u001b[36m(TorchTensorWorker pid=1126705)\u001b[0m Traceback (most recent call last):\n",
      "\u001b[36m(TorchTensorWorker pid=1126705)\u001b[0m   File \"/home/wxdeng/stbr-min/python/ray/dag/compiled_dag_node.py\", line 253, in do_exec_tasks\n",
      "\u001b[36m(TorchTensorWorker pid=1126705)\u001b[0m     done = tasks[operation.exec_task_idx].exec_operation(\n",
      "\u001b[36m(TorchTensorWorker pid=1126705)\u001b[0m   File \"/home/wxdeng/stbr-min/python/ray/dag/compiled_dag_node.py\", line 786, in exec_operation\n",
      "\u001b[36m(TorchTensorWorker pid=1126705)\u001b[0m     return self._write()\n",
      "\u001b[36m(TorchTensorWorker pid=1126705)\u001b[0m   File \"/home/wxdeng/stbr-min/python/ray/dag/compiled_dag_node.py\", line 751, in _write\n",
      "\u001b[36m(TorchTensorWorker pid=1126705)\u001b[0m     self.output_writer.write(output_val)\n",
      "\u001b[36m(TorchTensorWorker pid=1126705)\u001b[0m   File \"/home/wxdeng/stbr-min/python/ray/experimental/channel/common.py\", line 617, in write\n",
      "\u001b[36m(TorchTensorWorker pid=1126705)\u001b[0m     channel.write(val_i, timeout)\n",
      "\u001b[36m(TorchTensorWorker pid=1126705)\u001b[0m   File \"/home/wxdeng/stbr-min/python/ray/experimental/channel/torch_tensor_nccl_channel.py\", line 243, in write\n",
      "\u001b[36m(TorchTensorWorker pid=1126705)\u001b[0m     raise value\n",
      "\u001b[36m(TorchTensorWorker pid=1126705)\u001b[0m ray.exceptions.RayTaskError: \u001b[36mray::TorchTensorWorker.__ray_call__()\u001b[39m (pid=1126705, ip=128.208.3.124)\n",
      "\u001b[36m(TorchTensorWorker pid=1126705)\u001b[0m   File \"/home/wxdeng/stbr-min/python/ray/dag/tests/experimental/test_torch_tensor_dag.py\", line 67, in send_or_raise\n",
      "\u001b[36m(TorchTensorWorker pid=1126705)\u001b[0m     raise RuntimeError()\n",
      "\u001b[36m(TorchTensorWorker pid=1126705)\u001b[0m RuntimeError\n",
      "\u001b[36m(TorchTensorWorker pid=1126705)\u001b[0m Destructing NCCL group on actor: Actor(TorchTensorWorker, 34adc447265da6d7211b4fbe01000000)\n",
      "2025-05-14 11:58:21,225\tINFO compiled_dag_node.py:2200 -- Waiting for worker tasks to exit\n",
      "2025-05-14 11:58:21,226\tINFO compiled_dag_node.py:2203 -- Teardown complete\n",
      "\n",
      "test_torch_tensor_dag.py::test_torch_tensor_exceptions2[ray_start_regular0] 2025-05-14 11:58:25,730\tINFO worker.py:1879 -- Started a local Ray instance. View the dashboard at \u001b[1m\u001b[32m127.0.0.1:8265 \u001b[39m\u001b[22m\n",
      "2025-05-14 11:58:28,061\tINFO torch_tensor_nccl_channel.py:772 -- Creating NCCL group 614a777a-2158-43d3-913b-482c78b02592 on actors: [Actor(TorchTensorWorker, cc4f38022b06194c86ed0b0301000000), Actor(TorchTensorWorker, d7b39c5a099294ef0a86ac3001000000)]\n",
      "2025-05-14 11:58:29,254\tINFO torch_tensor_nccl_channel.py:797 -- NCCL group initialized.\n",
      "2025-05-14 11:58:29,340\tINFO dag_node_operation.py:618 -- Writing compiled graph schedule visualization to compiled_graph_schedule.png\n",
      "2025-05-14 11:58:29,541\tINFO compiled_dag_node.py:2173 -- Tearing down compiled DAG\n",
      "\u001b[32mPASSED\u001b[0m2025-05-14 11:58:29,545\tINFO compiled_dag_node.py:2178 -- Cancelling compiled worker on actor: Actor(TorchTensorWorker, cc4f38022b06194c86ed0b0301000000)\n",
      "2025-05-14 11:58:29,545\tINFO compiled_dag_node.py:2178 -- Cancelling compiled worker on actor: Actor(TorchTensorWorker, d7b39c5a099294ef0a86ac3001000000)\n",
      "\u001b[36m(TorchTensorWorker pid=1127974)\u001b[0m ERROR:root:Compiled DAG task exited with exception\n",
      "\u001b[36m(TorchTensorWorker pid=1127974)\u001b[0m Traceback (most recent call last):\n",
      "\u001b[36m(TorchTensorWorker pid=1127974)\u001b[0m   File \"/home/wxdeng/stbr-min/python/ray/dag/compiled_dag_node.py\", line 253, in do_exec_tasks\n",
      "\u001b[36m(TorchTensorWorker pid=1127974)\u001b[0m     done = tasks[operation.exec_task_idx].exec_operation(\n",
      "\u001b[36m(TorchTensorWorker pid=1127974)\u001b[0m   File \"/home/wxdeng/stbr-min/python/ray/dag/compiled_dag_node.py\", line 786, in exec_operation\n",
      "\u001b[36m(TorchTensorWorker pid=1127974)\u001b[0m     return self._write()\n",
      "\u001b[36m(TorchTensorWorker pid=1127974)\u001b[0m   File \"/home/wxdeng/stbr-min/python/ray/dag/compiled_dag_node.py\", line 751, in _write\n",
      "\u001b[36m(TorchTensorWorker pid=1127974)\u001b[0m     self.output_writer.write(output_val)\n",
      "\u001b[36m(TorchTensorWorker pid=1127974)\u001b[0m   File \"/home/wxdeng/stbr-min/python/ray/experimental/channel/common.py\", line 617, in write\n",
      "\u001b[36m(TorchTensorWorker pid=1127974)\u001b[0m     channel.write(val_i, timeout)\n",
      "\u001b[36m(TorchTensorWorker pid=1127974)\u001b[0m   File \"/home/wxdeng/stbr-min/python/ray/experimental/channel/torch_tensor_nccl_channel.py\", line 257, in write\n",
      "\u001b[36m(TorchTensorWorker pid=1127974)\u001b[0m     raise ValueError(\n",
      "\u001b[36m(TorchTensorWorker pid=1127974)\u001b[0m ValueError: Task annotated with _direct_return=True must return a CUDA torch.Tensor, instead found value `1`. DAG will shut down.\n",
      "\u001b[36m(TorchTensorWorker pid=1127974)\u001b[0m Destructing NCCL group on actor: Actor(TorchTensorWorker, cc4f38022b06194c86ed0b0301000000)\n",
      "\u001b[36m(TorchTensorWorker pid=1127976)\u001b[0m Destructing NCCL group on actor: Actor(TorchTensorWorker, d7b39c5a099294ef0a86ac3001000000)\n",
      "2025-05-14 11:58:30,260\tINFO compiled_dag_node.py:2200 -- Waiting for worker tasks to exit\n",
      "2025-05-14 11:58:30,260\tINFO compiled_dag_node.py:2203 -- Teardown complete\n",
      "\n",
      "test_torch_tensor_dag.py::test_torch_tensor_explicit_communicator[ray_start_regular0] 2025-05-14 11:58:33,966\tINFO worker.py:1879 -- Started a local Ray instance. View the dashboard at \u001b[1m\u001b[32m127.0.0.1:8265 \u001b[39m\u001b[22m\n",
      "\u001b[32mPASSED\u001b[0m\n",
      "test_torch_tensor_dag.py::test_torch_tensor_nccl_collective_ops[operation0-None-ray_start_regular0] 2025-05-14 11:58:39,171\tINFO worker.py:1879 -- Started a local Ray instance. View the dashboard at \u001b[1m\u001b[32m127.0.0.1:8265 \u001b[39m\u001b[22m\n",
      "2025-05-14 11:58:41,554\tINFO torch_tensor_nccl_channel.py:772 -- Creating NCCL group ab9ea875-5261-4a30-8abf-7a908cd3dcf0 on actors: [Actor(TorchTensorWorker, 1c74c801ba6170b92e4f855001000000), Actor(TorchTensorWorker, ff2b6ff741799d2a3ec1185601000000)]\n",
      "2025-05-14 11:58:42,749\tINFO torch_tensor_nccl_channel.py:797 -- NCCL group initialized.\n",
      "2025-05-14 11:58:42,920\tINFO dag_node_operation.py:618 -- Writing compiled graph schedule visualization to compiled_graph_schedule.png\n",
      "\u001b[32mPASSED\u001b[0m2025-05-14 11:58:43,514\tINFO compiled_dag_node.py:2173 -- Tearing down compiled DAG\n",
      "2025-05-14 11:58:43,514\tINFO compiled_dag_node.py:2178 -- Cancelling compiled worker on actor: Actor(TorchTensorWorker, 1c74c801ba6170b92e4f855001000000)\n",
      "2025-05-14 11:58:43,514\tINFO compiled_dag_node.py:2178 -- Cancelling compiled worker on actor: Actor(TorchTensorWorker, ff2b6ff741799d2a3ec1185601000000)\n",
      "\u001b[36m(TorchTensorWorker pid=1130375)\u001b[0m Destructing NCCL group on actor: Actor(TorchTensorWorker, ff2b6ff741799d2a3ec1185601000000)\n",
      "\u001b[36m(TorchTensorWorker pid=1130376)\u001b[0m Destructing NCCL group on actor: Actor(TorchTensorWorker, 1c74c801ba6170b92e4f855001000000)\n",
      "2025-05-14 11:58:43,737\tINFO compiled_dag_node.py:2200 -- Waiting for worker tasks to exit\n",
      "2025-05-14 11:58:43,738\tINFO compiled_dag_node.py:2161 -- Killing actor: Actor(TorchTensorWorker, 1c74c801ba6170b92e4f855001000000)\n",
      "2025-05-14 11:58:43,738\tINFO compiled_dag_node.py:2161 -- Killing actor: Actor(TorchTensorWorker, ff2b6ff741799d2a3ec1185601000000)\n",
      "2025-05-14 11:58:43,738\tINFO compiled_dag_node.py:2203 -- Teardown complete\n",
      "\n",
      "test_torch_tensor_dag.py::test_torch_tensor_nccl_collective_ops[operation1-ReduceOp.SUM-ray_start_regular0] 2025-05-14 11:58:47,080\tINFO worker.py:1879 -- Started a local Ray instance. View the dashboard at \u001b[1m\u001b[32m127.0.0.1:8265 \u001b[39m\u001b[22m\n",
      "2025-05-14 11:58:49,372\tINFO torch_tensor_nccl_channel.py:772 -- Creating NCCL group 149f0f1a-90bb-4441-9341-e8f59db1513c on actors: [Actor(TorchTensorWorker, b79ec3fc8e352feb1d69c2c501000000), Actor(TorchTensorWorker, ffed7645e04ac4f8ffae055c01000000)]\n",
      "2025-05-14 11:58:50,328\tINFO torch_tensor_nccl_channel.py:797 -- NCCL group initialized.\n",
      "2025-05-14 11:58:50,489\tINFO dag_node_operation.py:618 -- Writing compiled graph schedule visualization to compiled_graph_schedule.png\n",
      "\u001b[32mPASSED\u001b[0m2025-05-14 11:58:50,821\tINFO compiled_dag_node.py:2173 -- Tearing down compiled DAG\n",
      "2025-05-14 11:58:50,822\tINFO compiled_dag_node.py:2178 -- Cancelling compiled worker on actor: Actor(TorchTensorWorker, b79ec3fc8e352feb1d69c2c501000000)\n",
      "2025-05-14 11:58:50,822\tINFO compiled_dag_node.py:2178 -- Cancelling compiled worker on actor: Actor(TorchTensorWorker, ffed7645e04ac4f8ffae055c01000000)\n",
      "\u001b[36m(TorchTensorWorker pid=1131667)\u001b[0m Destructing NCCL group on actor: Actor(TorchTensorWorker, ffed7645e04ac4f8ffae055c01000000)\n",
      "\u001b[36m(TorchTensorWorker pid=1131668)\u001b[0m Destructing NCCL group on actor: Actor(TorchTensorWorker, b79ec3fc8e352feb1d69c2c501000000)\n",
      "2025-05-14 11:58:51,308\tINFO compiled_dag_node.py:2200 -- Waiting for worker tasks to exit\n",
      "2025-05-14 11:58:51,308\tINFO compiled_dag_node.py:2161 -- Killing actor: Actor(TorchTensorWorker, b79ec3fc8e352feb1d69c2c501000000)\n",
      "2025-05-14 11:58:51,308\tINFO compiled_dag_node.py:2161 -- Killing actor: Actor(TorchTensorWorker, ffed7645e04ac4f8ffae055c01000000)\n",
      "2025-05-14 11:58:51,309\tINFO compiled_dag_node.py:2203 -- Teardown complete\n",
      "\n",
      "test_torch_tensor_dag.py::test_torch_tensor_nccl_collective_ops[operation2-ReduceOp.PRODUCT-ray_start_regular0] 2025-05-14 11:58:55,106\tINFO worker.py:1879 -- Started a local Ray instance. View the dashboard at \u001b[1m\u001b[32m127.0.0.1:8265 \u001b[39m\u001b[22m\n",
      "2025-05-14 11:58:57,423\tINFO torch_tensor_nccl_channel.py:772 -- Creating NCCL group e07d780b-3319-4181-a15b-508882db74a6 on actors: [Actor(TorchTensorWorker, d9c08e3e36215212f28acdbe01000000), Actor(TorchTensorWorker, 5036757cb403a02c911a7ad701000000)]\n",
      "2025-05-14 11:58:58,392\tINFO torch_tensor_nccl_channel.py:797 -- NCCL group initialized.\n",
      "2025-05-14 11:58:58,569\tINFO dag_node_operation.py:618 -- Writing compiled graph schedule visualization to compiled_graph_schedule.png\n",
      "\u001b[32mPASSED\u001b[0m2025-05-14 11:58:58,918\tINFO compiled_dag_node.py:2173 -- Tearing down compiled DAG\n",
      "2025-05-14 11:58:58,919\tINFO compiled_dag_node.py:2178 -- Cancelling compiled worker on actor: Actor(TorchTensorWorker, d9c08e3e36215212f28acdbe01000000)\n",
      "2025-05-14 11:58:58,919\tINFO compiled_dag_node.py:2178 -- Cancelling compiled worker on actor: Actor(TorchTensorWorker, 5036757cb403a02c911a7ad701000000)\n",
      "\u001b[36m(TorchTensorWorker pid=1132936)\u001b[0m Destructing NCCL group on actor: Actor(TorchTensorWorker, 5036757cb403a02c911a7ad701000000)\n",
      "\u001b[36m(TorchTensorWorker pid=1132933)\u001b[0m Destructing NCCL group on actor: Actor(TorchTensorWorker, d9c08e3e36215212f28acdbe01000000)\n",
      "2025-05-14 11:58:59,408\tINFO compiled_dag_node.py:2200 -- Waiting for worker tasks to exit\n",
      "2025-05-14 11:58:59,408\tINFO compiled_dag_node.py:2161 -- Killing actor: Actor(TorchTensorWorker, d9c08e3e36215212f28acdbe01000000)\n",
      "2025-05-14 11:58:59,409\tINFO compiled_dag_node.py:2161 -- Killing actor: Actor(TorchTensorWorker, 5036757cb403a02c911a7ad701000000)\n",
      "2025-05-14 11:58:59,409\tINFO compiled_dag_node.py:2203 -- Teardown complete\n",
      "\n",
      "test_torch_tensor_dag.py::test_torch_tensor_nccl_collective_ops[operation3-ReduceOp.MIN-ray_start_regular0] 2025-05-14 11:59:02,996\tINFO worker.py:1879 -- Started a local Ray instance. View the dashboard at \u001b[1m\u001b[32m127.0.0.1:8265 \u001b[39m\u001b[22m\n",
      "2025-05-14 11:59:05,373\tINFO torch_tensor_nccl_channel.py:772 -- Creating NCCL group 91626d24-9f86-4605-badf-d34a0b1c413f on actors: [Actor(TorchTensorWorker, ea4145532d143277c9eaabd401000000), Actor(TorchTensorWorker, 2cd639529a9f23ffd6594d8001000000)]\n",
      "2025-05-14 11:59:06,352\tINFO torch_tensor_nccl_channel.py:797 -- NCCL group initialized.\n",
      "2025-05-14 11:59:06,522\tINFO dag_node_operation.py:618 -- Writing compiled graph schedule visualization to compiled_graph_schedule.png\n",
      "\u001b[32mPASSED\u001b[0m2025-05-14 11:59:06,865\tINFO compiled_dag_node.py:2173 -- Tearing down compiled DAG\n",
      "2025-05-14 11:59:06,866\tINFO compiled_dag_node.py:2178 -- Cancelling compiled worker on actor: Actor(TorchTensorWorker, ea4145532d143277c9eaabd401000000)\n",
      "2025-05-14 11:59:06,866\tINFO compiled_dag_node.py:2178 -- Cancelling compiled worker on actor: Actor(TorchTensorWorker, 2cd639529a9f23ffd6594d8001000000)\n",
      "\u001b[36m(TorchTensorWorker pid=1134187)\u001b[0m Destructing NCCL group on actor: Actor(TorchTensorWorker, ea4145532d143277c9eaabd401000000)\n",
      "\u001b[36m(TorchTensorWorker pid=1134189)\u001b[0m Destructing NCCL group on actor: Actor(TorchTensorWorker, 2cd639529a9f23ffd6594d8001000000)\n",
      "2025-05-14 11:59:07,352\tINFO compiled_dag_node.py:2200 -- Waiting for worker tasks to exit\n",
      "2025-05-14 11:59:07,352\tINFO compiled_dag_node.py:2161 -- Killing actor: Actor(TorchTensorWorker, ea4145532d143277c9eaabd401000000)\n",
      "2025-05-14 11:59:07,353\tINFO compiled_dag_node.py:2161 -- Killing actor: Actor(TorchTensorWorker, 2cd639529a9f23ffd6594d8001000000)\n",
      "2025-05-14 11:59:07,353\tINFO compiled_dag_node.py:2203 -- Teardown complete\n",
      "\n",
      "test_torch_tensor_dag.py::test_torch_tensor_nccl_collective_ops[operation4-ReduceOp.MAX-ray_start_regular0] 2025-05-14 11:59:12,180\tINFO worker.py:1879 -- Started a local Ray instance. View the dashboard at \u001b[1m\u001b[32m127.0.0.1:8265 \u001b[39m\u001b[22m\n",
      "2025-05-14 11:59:14,553\tINFO torch_tensor_nccl_channel.py:772 -- Creating NCCL group 2c9b60d9-dda5-45a6-b59e-7f1aba86dbe3 on actors: [Actor(TorchTensorWorker, 84ec72c66e8b8287b9c32d8701000000), Actor(TorchTensorWorker, 62f5d56dc1ef1d90162b97c501000000)]\n",
      "2025-05-14 11:59:15,534\tINFO torch_tensor_nccl_channel.py:797 -- NCCL group initialized.\n",
      "2025-05-14 11:59:15,672\tINFO dag_node_operation.py:618 -- Writing compiled graph schedule visualization to compiled_graph_schedule.png\n",
      "\u001b[32mPASSED\u001b[0m2025-05-14 11:59:16,019\tINFO compiled_dag_node.py:2173 -- Tearing down compiled DAG\n",
      "2025-05-14 11:59:16,020\tINFO compiled_dag_node.py:2178 -- Cancelling compiled worker on actor: Actor(TorchTensorWorker, 84ec72c66e8b8287b9c32d8701000000)\n",
      "2025-05-14 11:59:16,020\tINFO compiled_dag_node.py:2178 -- Cancelling compiled worker on actor: Actor(TorchTensorWorker, 62f5d56dc1ef1d90162b97c501000000)\n",
      "\u001b[36m(TorchTensorWorker pid=1135534)\u001b[0m Destructing NCCL group on actor: Actor(TorchTensorWorker, 84ec72c66e8b8287b9c32d8701000000)\n",
      "\u001b[36m(TorchTensorWorker pid=1135532)\u001b[0m Destructing NCCL group on actor: Actor(TorchTensorWorker, 62f5d56dc1ef1d90162b97c501000000)\n",
      "2025-05-14 11:59:16,510\tINFO compiled_dag_node.py:2200 -- Waiting for worker tasks to exit\n",
      "2025-05-14 11:59:16,510\tINFO compiled_dag_node.py:2161 -- Killing actor: Actor(TorchTensorWorker, 84ec72c66e8b8287b9c32d8701000000)\n",
      "2025-05-14 11:59:16,511\tINFO compiled_dag_node.py:2161 -- Killing actor: Actor(TorchTensorWorker, 62f5d56dc1ef1d90162b97c501000000)\n",
      "2025-05-14 11:59:16,511\tINFO compiled_dag_node.py:2203 -- Teardown complete\n",
      "\n",
      "test_torch_tensor_dag.py::test_torch_tensor_nccl_collective_ops[operation5-ReduceOp.SUM-ray_start_regular0] 2025-05-14 11:59:20,175\tINFO worker.py:1879 -- Started a local Ray instance. View the dashboard at \u001b[1m\u001b[32m127.0.0.1:8265 \u001b[39m\u001b[22m\n",
      "2025-05-14 11:59:22,524\tINFO torch_tensor_nccl_channel.py:772 -- Creating NCCL group 0c0b6fa6-a23e-45ee-a008-a357afe1514f on actors: [Actor(TorchTensorWorker, 6fcd3cfe56db41d3621993bc01000000), Actor(TorchTensorWorker, 0bfc8bcca62467f130fe042401000000)]\n",
      "2025-05-14 11:59:23,491\tINFO torch_tensor_nccl_channel.py:797 -- NCCL group initialized.\n",
      "2025-05-14 11:59:23,658\tINFO dag_node_operation.py:618 -- Writing compiled graph schedule visualization to compiled_graph_schedule.png\n",
      "\u001b[32mPASSED\u001b[0m2025-05-14 11:59:24,013\tINFO compiled_dag_node.py:2173 -- Tearing down compiled DAG\n",
      "2025-05-14 11:59:24,014\tINFO compiled_dag_node.py:2178 -- Cancelling compiled worker on actor: Actor(TorchTensorWorker, 6fcd3cfe56db41d3621993bc01000000)\n",
      "2025-05-14 11:59:24,014\tINFO compiled_dag_node.py:2178 -- Cancelling compiled worker on actor: Actor(TorchTensorWorker, 0bfc8bcca62467f130fe042401000000)\n",
      "\u001b[36m(TorchTensorWorker pid=1136763)\u001b[0m Destructing NCCL group on actor: Actor(TorchTensorWorker, 6fcd3cfe56db41d3621993bc01000000)\n",
      "\u001b[36m(TorchTensorWorker pid=1136762)\u001b[0m Destructing NCCL group on actor: Actor(TorchTensorWorker, 0bfc8bcca62467f130fe042401000000)\n",
      "2025-05-14 11:59:24,500\tINFO compiled_dag_node.py:2200 -- Waiting for worker tasks to exit\n",
      "2025-05-14 11:59:24,500\tINFO compiled_dag_node.py:2161 -- Killing actor: Actor(TorchTensorWorker, 6fcd3cfe56db41d3621993bc01000000)\n",
      "2025-05-14 11:59:24,501\tINFO compiled_dag_node.py:2161 -- Killing actor: Actor(TorchTensorWorker, 0bfc8bcca62467f130fe042401000000)\n",
      "2025-05-14 11:59:24,501\tINFO compiled_dag_node.py:2203 -- Teardown complete\n",
      "\n",
      "test_torch_tensor_dag.py::test_torch_tensor_nccl_collective_ops[operation6-ReduceOp.PRODUCT-ray_start_regular0] 2025-05-14 11:59:29,114\tINFO worker.py:1879 -- Started a local Ray instance. View the dashboard at \u001b[1m\u001b[32m127.0.0.1:8265 \u001b[39m\u001b[22m\n",
      "2025-05-14 11:59:31,456\tINFO torch_tensor_nccl_channel.py:772 -- Creating NCCL group 09542a57-3dda-44b1-a548-518a258c6859 on actors: [Actor(TorchTensorWorker, 864241b3d89807d2c63c3e8c01000000), Actor(TorchTensorWorker, 0f9497f3944393975621845d01000000)]\n",
      "2025-05-14 11:59:32,435\tINFO torch_tensor_nccl_channel.py:797 -- NCCL group initialized.\n",
      "2025-05-14 11:59:32,599\tINFO dag_node_operation.py:618 -- Writing compiled graph schedule visualization to compiled_graph_schedule.png\n",
      "\u001b[32mPASSED\u001b[0m2025-05-14 11:59:32,952\tINFO compiled_dag_node.py:2173 -- Tearing down compiled DAG\n",
      "2025-05-14 11:59:32,952\tINFO compiled_dag_node.py:2178 -- Cancelling compiled worker on actor: Actor(TorchTensorWorker, 864241b3d89807d2c63c3e8c01000000)\n",
      "2025-05-14 11:59:32,952\tINFO compiled_dag_node.py:2178 -- Cancelling compiled worker on actor: Actor(TorchTensorWorker, 0f9497f3944393975621845d01000000)\n",
      "\u001b[36m(TorchTensorWorker pid=1138004)\u001b[0m Destructing NCCL group on actor: Actor(TorchTensorWorker, 864241b3d89807d2c63c3e8c01000000)\n",
      "\u001b[36m(TorchTensorWorker pid=1138002)\u001b[0m Destructing NCCL group on actor: Actor(TorchTensorWorker, 0f9497f3944393975621845d01000000)\n",
      "2025-05-14 11:59:33,442\tINFO compiled_dag_node.py:2200 -- Waiting for worker tasks to exit\n",
      "2025-05-14 11:59:33,442\tINFO compiled_dag_node.py:2161 -- Killing actor: Actor(TorchTensorWorker, 864241b3d89807d2c63c3e8c01000000)\n",
      "2025-05-14 11:59:33,443\tINFO compiled_dag_node.py:2161 -- Killing actor: Actor(TorchTensorWorker, 0f9497f3944393975621845d01000000)\n",
      "2025-05-14 11:59:33,443\tINFO compiled_dag_node.py:2203 -- Teardown complete\n",
      "\n",
      "test_torch_tensor_dag.py::test_torch_tensor_nccl_collective_ops[operation7-ReduceOp.MIN-ray_start_regular0] 2025-05-14 11:59:37,090\tINFO worker.py:1879 -- Started a local Ray instance. View the dashboard at \u001b[1m\u001b[32m127.0.0.1:8265 \u001b[39m\u001b[22m\n",
      "2025-05-14 11:59:39,410\tINFO torch_tensor_nccl_channel.py:772 -- Creating NCCL group 6d5bc081-4332-44a5-8a8e-16399c31ffa0 on actors: [Actor(TorchTensorWorker, 12ad4e6044498879d6cccd7401000000), Actor(TorchTensorWorker, 3cef651502f19c548cd6bef901000000)]\n",
      "2025-05-14 11:59:40,391\tINFO torch_tensor_nccl_channel.py:797 -- NCCL group initialized.\n",
      "2025-05-14 11:59:40,558\tINFO dag_node_operation.py:618 -- Writing compiled graph schedule visualization to compiled_graph_schedule.png\n",
      "\u001b[32mPASSED\u001b[0m2025-05-14 11:59:40,905\tINFO compiled_dag_node.py:2173 -- Tearing down compiled DAG\n",
      "2025-05-14 11:59:40,906\tINFO compiled_dag_node.py:2178 -- Cancelling compiled worker on actor: Actor(TorchTensorWorker, 12ad4e6044498879d6cccd7401000000)\n",
      "2025-05-14 11:59:40,906\tINFO compiled_dag_node.py:2178 -- Cancelling compiled worker on actor: Actor(TorchTensorWorker, 3cef651502f19c548cd6bef901000000)\n",
      "\u001b[36m(TorchTensorWorker pid=1139253)\u001b[0m Destructing NCCL group on actor: Actor(TorchTensorWorker, 3cef651502f19c548cd6bef901000000)\n",
      "\u001b[36m(TorchTensorWorker pid=1139252)\u001b[0m Destructing NCCL group on actor: Actor(TorchTensorWorker, 12ad4e6044498879d6cccd7401000000)\n",
      "2025-05-14 11:59:41,394\tINFO compiled_dag_node.py:2200 -- Waiting for worker tasks to exit\n",
      "2025-05-14 11:59:41,394\tINFO compiled_dag_node.py:2161 -- Killing actor: Actor(TorchTensorWorker, 12ad4e6044498879d6cccd7401000000)\n",
      "2025-05-14 11:59:41,394\tINFO compiled_dag_node.py:2161 -- Killing actor: Actor(TorchTensorWorker, 3cef651502f19c548cd6bef901000000)\n",
      "2025-05-14 11:59:41,395\tINFO compiled_dag_node.py:2203 -- Teardown complete\n",
      "\n",
      "test_torch_tensor_dag.py::test_torch_tensor_nccl_collective_ops[operation8-ReduceOp.MAX-ray_start_regular0] 2025-05-14 11:59:45,119\tINFO worker.py:1879 -- Started a local Ray instance. View the dashboard at \u001b[1m\u001b[32m127.0.0.1:8265 \u001b[39m\u001b[22m\n",
      "2025-05-14 11:59:47,436\tINFO torch_tensor_nccl_channel.py:772 -- Creating NCCL group 13489809-52bb-4490-8e3c-182f5f2b43e9 on actors: [Actor(TorchTensorWorker, 65c2e903d6433408d565532201000000), Actor(TorchTensorWorker, bcd900b4662b2f44a739e08801000000)]\n",
      "2025-05-14 11:59:48,438\tINFO torch_tensor_nccl_channel.py:797 -- NCCL group initialized.\n",
      "2025-05-14 11:59:48,616\tINFO dag_node_operation.py:618 -- Writing compiled graph schedule visualization to compiled_graph_schedule.png\n",
      "\u001b[32mPASSED\u001b[0m2025-05-14 11:59:48,965\tINFO compiled_dag_node.py:2173 -- Tearing down compiled DAG\n",
      "2025-05-14 11:59:48,966\tINFO compiled_dag_node.py:2178 -- Cancelling compiled worker on actor: Actor(TorchTensorWorker, 65c2e903d6433408d565532201000000)\n",
      "2025-05-14 11:59:48,966\tINFO compiled_dag_node.py:2178 -- Cancelling compiled worker on actor: Actor(TorchTensorWorker, bcd900b4662b2f44a739e08801000000)\n",
      "\u001b[36m(TorchTensorWorker pid=1140500)\u001b[0m Destructing NCCL group on actor: Actor(TorchTensorWorker, bcd900b4662b2f44a739e08801000000)\n",
      "\u001b[36m(TorchTensorWorker pid=1140499)\u001b[0m Destructing NCCL group on actor: Actor(TorchTensorWorker, 65c2e903d6433408d565532201000000)\n",
      "2025-05-14 11:59:49,489\tINFO compiled_dag_node.py:2200 -- Waiting for worker tasks to exit\n",
      "2025-05-14 11:59:49,490\tINFO compiled_dag_node.py:2161 -- Killing actor: Actor(TorchTensorWorker, 65c2e903d6433408d565532201000000)\n",
      "2025-05-14 11:59:49,490\tINFO compiled_dag_node.py:2161 -- Killing actor: Actor(TorchTensorWorker, bcd900b4662b2f44a739e08801000000)\n",
      "2025-05-14 11:59:49,490\tINFO compiled_dag_node.py:2203 -- Teardown complete\n",
      "\n",
      "test_torch_tensor_dag.py::test_torch_tensor_nccl_all_reduce_get_partial[ray_start_regular0] 2025-05-14 11:59:53,957\tINFO worker.py:1879 -- Started a local Ray instance. View the dashboard at \u001b[1m\u001b[32m127.0.0.1:8265 \u001b[39m\u001b[22m\n",
      "2025-05-14 11:59:56,294\tINFO torch_tensor_nccl_channel.py:772 -- Creating NCCL group 3d8996b0-fa7b-44fb-b63b-e87055d021cc on actors: [Actor(TorchTensorWorker, 0423931701eff6976831714d01000000), Actor(TorchTensorWorker, d5422e859088eba99197869601000000)]\n",
      "2025-05-14 11:59:57,274\tINFO torch_tensor_nccl_channel.py:797 -- NCCL group initialized.\n",
      "2025-05-14 11:59:57,427\tINFO dag_node_operation.py:618 -- Writing compiled graph schedule visualization to compiled_graph_schedule.png\n",
      "\u001b[32mPASSED\u001b[0m2025-05-14 11:59:57,714\tINFO compiled_dag_node.py:2173 -- Tearing down compiled DAG\n",
      "2025-05-14 11:59:57,714\tINFO compiled_dag_node.py:2178 -- Cancelling compiled worker on actor: Actor(TorchTensorWorker, 0423931701eff6976831714d01000000)\n",
      "2025-05-14 11:59:57,714\tINFO compiled_dag_node.py:2178 -- Cancelling compiled worker on actor: Actor(TorchTensorWorker, d5422e859088eba99197869601000000)\n",
      "\u001b[36m(TorchTensorWorker pid=1141793)\u001b[0m Destructing NCCL group on actor: Actor(TorchTensorWorker, d5422e859088eba99197869601000000)\n",
      "\u001b[36m(TorchTensorWorker pid=1141792)\u001b[0m Destructing NCCL group on actor: Actor(TorchTensorWorker, 0423931701eff6976831714d01000000)\n",
      "2025-05-14 11:59:58,238\tINFO compiled_dag_node.py:2200 -- Waiting for worker tasks to exit\n",
      "2025-05-14 11:59:58,239\tINFO compiled_dag_node.py:2161 -- Killing actor: Actor(TorchTensorWorker, 0423931701eff6976831714d01000000)\n",
      "2025-05-14 11:59:58,239\tINFO compiled_dag_node.py:2161 -- Killing actor: Actor(TorchTensorWorker, d5422e859088eba99197869601000000)\n",
      "2025-05-14 11:59:58,240\tINFO compiled_dag_node.py:2203 -- Teardown complete\n",
      "\n",
      "test_torch_tensor_dag.py::test_torch_tensor_nccl_all_reduce_wrong_shape[ray_start_regular0] 2025-05-14 12:00:01,892\tINFO worker.py:1879 -- Started a local Ray instance. View the dashboard at \u001b[1m\u001b[32m127.0.0.1:8265 \u001b[39m\u001b[22m\n",
      "2025-05-14 12:00:04,256\tINFO torch_tensor_nccl_channel.py:772 -- Creating NCCL group 6faf739c-787d-4bf4-b473-06dfb91e346d on actors: [Actor(TorchTensorWorker, bc6ab04f2bd30f7717f24d9f01000000), Actor(TorchTensorWorker, 68811c20b1292f058d5653f301000000)]\n",
      "2025-05-14 12:00:05,240\tINFO torch_tensor_nccl_channel.py:797 -- NCCL group initialized.\n",
      "2025-05-14 12:00:05,376\tINFO dag_node_operation.py:618 -- Writing compiled graph schedule visualization to compiled_graph_schedule.png\n",
      "\u001b[33m(raylet)\u001b[0m [2025-05-14 12:00:11,852 E 1142936 1142965] (raylet) file_system_monitor.cc:116: /tmp/ray/session_2025-05-14_12-00-00_006162_1086541 is over 95% full, available space: 81.3498 GB; capacity: 1830.63 GB. Object creation will fail if spilling is required.\n",
      "\u001b[33m(raylet)\u001b[0m [2025-05-14 12:00:21,860 E 1142936 1142965] (raylet) file_system_monitor.cc:116: /tmp/ray/session_2025-05-14_12-00-00_006162_1086541 is over 95% full, available space: 81.3498 GB; capacity: 1830.63 GB. Object creation will fail if spilling is required.\n",
      "\u001b[32mPASSED\u001b[0m2025-05-14 12:00:25,732\tINFO compiled_dag_node.py:2173 -- Tearing down compiled DAG\n",
      "2025-05-14 12:00:25,733\tINFO compiled_dag_node.py:2178 -- Cancelling compiled worker on actor: Actor(TorchTensorWorker, bc6ab04f2bd30f7717f24d9f01000000)\n",
      "2025-05-14 12:00:25,733\tINFO compiled_dag_node.py:2178 -- Cancelling compiled worker on actor: Actor(TorchTensorWorker, 68811c20b1292f058d5653f301000000)\n",
      "\u001b[36m(TorchTensorWorker pid=1143026)\u001b[0m Destructing NCCL group on actor: Actor(TorchTensorWorker, bc6ab04f2bd30f7717f24d9f01000000)\n",
      "\u001b[36m(TorchTensorWorker pid=1143025)\u001b[0m Destructing NCCL group on actor: Actor(TorchTensorWorker, 68811c20b1292f058d5653f301000000)\n",
      "2025-05-14 12:00:25,821\tINFO compiled_dag_node.py:2200 -- Waiting for worker tasks to exit\n",
      "2025-05-14 12:00:25,821\tINFO compiled_dag_node.py:2161 -- Killing actor: Actor(TorchTensorWorker, bc6ab04f2bd30f7717f24d9f01000000)\n",
      "2025-05-14 12:00:25,822\tINFO compiled_dag_node.py:2161 -- Killing actor: Actor(TorchTensorWorker, 68811c20b1292f058d5653f301000000)\n",
      "2025-05-14 12:00:25,822\tINFO compiled_dag_node.py:2203 -- Teardown complete\n",
      "\n",
      "test_torch_tensor_dag.py::test_torch_tensor_nccl_all_reduce_custom_comm[ray_start_regular0] 2025-05-14 12:00:30,302\tINFO worker.py:1879 -- Started a local Ray instance. View the dashboard at \u001b[1m\u001b[32m127.0.0.1:8265 \u001b[39m\u001b[22m\n",
      "2025-05-14 12:00:32,623\tINFO torch_tensor_nccl_channel.py:770 -- Initializing custom NCCL group 2f198e47-d72c-4da2-abf0-3459f6f689b9 on actors: [Actor(TorchTensorWorker, 875fe56a4cc97485bcf5f73901000000), Actor(TorchTensorWorker, b4dea5a1e8d6e23c29fec21501000000)]\n",
      "2025-05-14 12:00:33,494\tINFO torch_tensor_nccl_channel.py:797 -- NCCL group initialized.\n",
      "2025-05-14 12:00:33,627\tINFO dag_node_operation.py:618 -- Writing compiled graph schedule visualization to compiled_graph_schedule.png\n",
      "\u001b[32mPASSED\u001b[0m2025-05-14 12:00:34,011\tINFO compiled_dag_node.py:2173 -- Tearing down compiled DAG\n",
      "2025-05-14 12:00:34,011\tINFO compiled_dag_node.py:2178 -- Cancelling compiled worker on actor: Actor(TorchTensorWorker, 875fe56a4cc97485bcf5f73901000000)\n",
      "2025-05-14 12:00:34,011\tINFO compiled_dag_node.py:2178 -- Cancelling compiled worker on actor: Actor(TorchTensorWorker, b4dea5a1e8d6e23c29fec21501000000)\n",
      "2025-05-14 12:00:34,022\tINFO compiled_dag_node.py:2200 -- Waiting for worker tasks to exit\n",
      "2025-05-14 12:00:34,022\tINFO compiled_dag_node.py:2161 -- Killing actor: Actor(TorchTensorWorker, 875fe56a4cc97485bcf5f73901000000)\n",
      "2025-05-14 12:00:34,022\tINFO compiled_dag_node.py:2161 -- Killing actor: Actor(TorchTensorWorker, b4dea5a1e8d6e23c29fec21501000000)\n",
      "2025-05-14 12:00:34,022\tINFO compiled_dag_node.py:2203 -- Teardown complete\n",
      "\n",
      "test_torch_tensor_dag.py::test_torch_tensor_nccl_all_reduce_scheduling[ray_start_regular0] 2025-05-14 12:00:37,589\tINFO worker.py:1879 -- Started a local Ray instance. View the dashboard at \u001b[1m\u001b[32m127.0.0.1:8265 \u001b[39m\u001b[22m\n",
      "2025-05-14 12:00:39,931\tINFO torch_tensor_nccl_channel.py:772 -- Creating NCCL group 903bf11a-7f61-444c-ba52-2e432ecbbc75 on actors: [Actor(TorchTensorWorker, ed1cdc9a0daa8c9a6ee5890e01000000), Actor(TorchTensorWorker, 720da40fd881b84de56c157201000000)]\n",
      "2025-05-14 12:00:40,945\tINFO torch_tensor_nccl_channel.py:797 -- NCCL group initialized.\n",
      "2025-05-14 12:00:41,109\tINFO dag_node_operation.py:618 -- Writing compiled graph schedule visualization to compiled_graph_schedule.png\n",
      "\u001b[32mPASSED\u001b[0m2025-05-14 12:00:41,437\tINFO compiled_dag_node.py:2173 -- Tearing down compiled DAG\n",
      "2025-05-14 12:00:41,437\tINFO compiled_dag_node.py:2178 -- Cancelling compiled worker on actor: Actor(TorchTensorWorker, ed1cdc9a0daa8c9a6ee5890e01000000)\n",
      "2025-05-14 12:00:41,437\tINFO compiled_dag_node.py:2178 -- Cancelling compiled worker on actor: Actor(TorchTensorWorker, 720da40fd881b84de56c157201000000)\n",
      "\u001b[36m(TorchTensorWorker pid=1145666)\u001b[0m Destructing NCCL group on actor: Actor(TorchTensorWorker, ed1cdc9a0daa8c9a6ee5890e01000000)\n",
      "\u001b[36m(TorchTensorWorker pid=1145667)\u001b[0m Destructing NCCL group on actor: Actor(TorchTensorWorker, 720da40fd881b84de56c157201000000)\n",
      "2025-05-14 12:00:41,959\tINFO compiled_dag_node.py:2200 -- Waiting for worker tasks to exit\n",
      "2025-05-14 12:00:41,960\tINFO compiled_dag_node.py:2161 -- Killing actor: Actor(TorchTensorWorker, ed1cdc9a0daa8c9a6ee5890e01000000)\n",
      "2025-05-14 12:00:41,960\tINFO compiled_dag_node.py:2161 -- Killing actor: Actor(TorchTensorWorker, 720da40fd881b84de56c157201000000)\n",
      "2025-05-14 12:00:41,961\tINFO compiled_dag_node.py:2203 -- Teardown complete\n",
      "\n",
      "test_torch_tensor_dag.py::test_nccl_all_reduce_with_class_method_output_node[ray_start_regular0] 2025-05-14 12:00:45,541\tINFO worker.py:1879 -- Started a local Ray instance. View the dashboard at \u001b[1m\u001b[32m127.0.0.1:8265 \u001b[39m\u001b[22m\n",
      "2025-05-14 12:00:47,966\tINFO torch_tensor_nccl_channel.py:772 -- Creating NCCL group 1a0bc0b7-b226-4042-9325-24edbd15ef1f on actors: [Actor(TorchTensorWorker, 37be9e1f744cd0bc78532b0d01000000), Actor(TorchTensorWorker, c209bedc16754c430906fc8201000000)]\n",
      "2025-05-14 12:00:48,946\tINFO torch_tensor_nccl_channel.py:797 -- NCCL group initialized.\n",
      "2025-05-14 12:00:49,225\tINFO dag_node_operation.py:618 -- Writing compiled graph schedule visualization to compiled_graph_schedule.png\n",
      "\u001b[32mPASSED\u001b[0m2025-05-14 12:00:49,589\tINFO compiled_dag_node.py:2173 -- Tearing down compiled DAG\n",
      "2025-05-14 12:00:49,590\tINFO compiled_dag_node.py:2178 -- Cancelling compiled worker on actor: Actor(TorchTensorWorker, 37be9e1f744cd0bc78532b0d01000000)\n",
      "2025-05-14 12:00:49,590\tINFO compiled_dag_node.py:2178 -- Cancelling compiled worker on actor: Actor(TorchTensorWorker, c209bedc16754c430906fc8201000000)\n",
      "\u001b[36m(TorchTensorWorker pid=1146932)\u001b[0m Destructing NCCL group on actor: Actor(TorchTensorWorker, c209bedc16754c430906fc8201000000)\n",
      "\u001b[36m(TorchTensorWorker pid=1146931)\u001b[0m Destructing NCCL group on actor: Actor(TorchTensorWorker, 37be9e1f744cd0bc78532b0d01000000)\n",
      "2025-05-14 12:00:50,018\tINFO compiled_dag_node.py:2200 -- Waiting for worker tasks to exit\n",
      "2025-05-14 12:00:50,019\tINFO compiled_dag_node.py:2161 -- Killing actor: Actor(TorchTensorWorker, 37be9e1f744cd0bc78532b0d01000000)\n",
      "2025-05-14 12:00:50,019\tINFO compiled_dag_node.py:2161 -- Killing actor: Actor(TorchTensorWorker, c209bedc16754c430906fc8201000000)\n",
      "2025-05-14 12:00:50,019\tINFO compiled_dag_node.py:2203 -- Teardown complete\n",
      "\n",
      "test_torch_tensor_dag.py::test_tensor_writable_warning_suppressed[ray_start_regular0] 2025-05-14 12:00:53,623\tINFO worker.py:1879 -- Started a local Ray instance. View the dashboard at \u001b[1m\u001b[32m127.0.0.1:8265 \u001b[39m\u001b[22m\n",
      "\u001b[33m(raylet)\u001b[0m [2025-05-14 12:00:53,571 E 1148085 1148113] (raylet) file_system_monitor.cc:116: /tmp/ray/session_2025-05-14_12-00-51_639933_1086541 is over 95% full, available space: 81.3469 GB; capacity: 1830.63 GB. Object creation will fail if spilling is required.\n",
      "2025-05-14 12:00:54,129\tINFO dag_node_operation.py:618 -- Writing compiled graph schedule visualization to compiled_graph_schedule.png\n",
      "2025-05-14 12:00:55,687\tINFO compiled_dag_node.py:2173 -- Tearing down compiled DAG\n",
      "2025-05-14 12:00:55,688\tINFO compiled_dag_node.py:2178 -- Cancelling compiled worker on actor: Actor(test_tensor_writable_warning_suppressed.<locals>.A, 7a24a91dad6b3a4300e54d0001000000)\n",
      "2025-05-14 12:00:55,692\tINFO compiled_dag_node.py:2200 -- Waiting for worker tasks to exit\n",
      "2025-05-14 12:00:55,692\tINFO compiled_dag_node.py:2203 -- Teardown complete\n",
      "\u001b[32mPASSED\u001b[0m\n",
      "test_torch_tensor_dag.py::test_torch_nccl_channel_with_local_reader[ray_start_regular0] 2025-05-14 12:00:59,008\tINFO worker.py:1879 -- Started a local Ray instance. View the dashboard at \u001b[1m\u001b[32m127.0.0.1:8265 \u001b[39m\u001b[22m\n",
      "2025-05-14 12:01:01,319\tINFO torch_tensor_nccl_channel.py:772 -- Creating NCCL group 74177d83-c839-4e9b-a004-c33d97d03f13 on actors: [Actor(TorchTensorWorker, 850a50ce478a32417277048d01000000), Actor(TorchTensorWorker, 28383bfbb4fb1b174a62153301000000)]\n",
      "2025-05-14 12:01:02,296\tINFO torch_tensor_nccl_channel.py:797 -- NCCL group initialized.\n",
      "2025-05-14 12:01:02,441\tINFO dag_node_operation.py:618 -- Writing compiled graph schedule visualization to compiled_graph_schedule.png\n",
      "\u001b[32mPASSED\u001b[0m2025-05-14 12:01:02,699\tINFO compiled_dag_node.py:2173 -- Tearing down compiled DAG\n",
      "2025-05-14 12:01:02,700\tINFO compiled_dag_node.py:2178 -- Cancelling compiled worker on actor: Actor(TorchTensorWorker, 850a50ce478a32417277048d01000000)\n",
      "2025-05-14 12:01:02,700\tINFO compiled_dag_node.py:2178 -- Cancelling compiled worker on actor: Actor(TorchTensorWorker, 28383bfbb4fb1b174a62153301000000)\n",
      "\u001b[36m(TorchTensorWorker pid=1149227)\u001b[0m Destructing NCCL group on actor: Actor(TorchTensorWorker, 28383bfbb4fb1b174a62153301000000)\n",
      "\u001b[36m(TorchTensorWorker pid=1149226)\u001b[0m Destructing NCCL group on actor: Actor(TorchTensorWorker, 850a50ce478a32417277048d01000000)\n",
      "2025-05-14 12:01:03,217\tINFO compiled_dag_node.py:2200 -- Waiting for worker tasks to exit\n",
      "2025-05-14 12:01:03,217\tINFO compiled_dag_node.py:2161 -- Killing actor: Actor(TorchTensorWorker, 850a50ce478a32417277048d01000000)\n",
      "2025-05-14 12:01:03,218\tINFO compiled_dag_node.py:2161 -- Killing actor: Actor(TorchTensorWorker, 28383bfbb4fb1b174a62153301000000)\n",
      "2025-05-14 12:01:03,218\tINFO compiled_dag_node.py:2203 -- Teardown complete\n",
      "\n",
      "test_torch_tensor_dag.py::test_torch_nccl_channel_with_two_local_readers[ray_start_regular0] 2025-05-14 12:01:06,917\tINFO worker.py:1879 -- Started a local Ray instance. View the dashboard at \u001b[1m\u001b[32m127.0.0.1:8265 \u001b[39m\u001b[22m\n",
      "2025-05-14 12:01:09,291\tINFO torch_tensor_nccl_channel.py:772 -- Creating NCCL group f98d45fc-f347-490d-a9c5-074aa35e17a9 on actors: [Actor(TorchTensorWorker, 887b56f9e4ae641d6b649b3e01000000), Actor(TorchTensorWorker, ca1ac1a72c9feb6cfbd410bd01000000)]\n",
      "2025-05-14 12:01:10,316\tINFO torch_tensor_nccl_channel.py:797 -- NCCL group initialized.\n",
      "2025-05-14 12:01:10,538\tINFO dag_node_operation.py:618 -- Writing compiled graph schedule visualization to compiled_graph_schedule.png\n",
      "\u001b[32mPASSED\u001b[0m2025-05-14 12:01:10,803\tINFO compiled_dag_node.py:2173 -- Tearing down compiled DAG\n",
      "2025-05-14 12:01:10,805\tINFO compiled_dag_node.py:2178 -- Cancelling compiled worker on actor: Actor(TorchTensorWorker, 887b56f9e4ae641d6b649b3e01000000)\n",
      "2025-05-14 12:01:10,805\tINFO compiled_dag_node.py:2178 -- Cancelling compiled worker on actor: Actor(TorchTensorWorker, ca1ac1a72c9feb6cfbd410bd01000000)\n",
      "\u001b[36m(TorchTensorWorker pid=1150499)\u001b[0m Destructing NCCL group on actor: Actor(TorchTensorWorker, 887b56f9e4ae641d6b649b3e01000000)\n",
      "\u001b[36m(TorchTensorWorker pid=1150500)\u001b[0m Destructing NCCL group on actor: Actor(TorchTensorWorker, ca1ac1a72c9feb6cfbd410bd01000000)\n",
      "2025-05-14 12:01:11,324\tINFO compiled_dag_node.py:2200 -- Waiting for worker tasks to exit\n",
      "2025-05-14 12:01:11,325\tINFO compiled_dag_node.py:2161 -- Killing actor: Actor(TorchTensorWorker, 887b56f9e4ae641d6b649b3e01000000)\n",
      "2025-05-14 12:01:11,325\tINFO compiled_dag_node.py:2161 -- Killing actor: Actor(TorchTensorWorker, ca1ac1a72c9feb6cfbd410bd01000000)\n",
      "2025-05-14 12:01:11,325\tINFO compiled_dag_node.py:2203 -- Teardown complete\n",
      "\n",
      "test_torch_tensor_dag.py::test_torch_nccl_channel_with_all_local_readers[ray_start_regular0] 2025-05-14 12:01:15,956\tINFO worker.py:1879 -- Started a local Ray instance. View the dashboard at \u001b[1m\u001b[32m127.0.0.1:8265 \u001b[39m\u001b[22m\n",
      "2025-05-14 12:01:18,260\tINFO torch_tensor_nccl_channel.py:772 -- Creating NCCL group 2cccab49-bcdb-44ea-9bfe-e299e53f469e on actors: [Actor(TorchTensorWorker, 7bea469dfefac6b27531687901000000)]\n",
      "2025-05-14 12:01:18,758\tINFO torch_tensor_nccl_channel.py:797 -- NCCL group initialized.\n",
      "\u001b[32mPASSED\u001b[0m\n",
      "\n",
      "=================================== FAILURES ===================================\n",
      "\u001b[31m\u001b[1m________ test_torch_tensor_nccl_overlap_timed[ray_start_regular0-False] ________\u001b[0m\n",
      "\n",
      "ray_start_regular = RayContext(dashboard_url='127.0.0.1:8265', python_version='3.9.22', ray_version='3.0.0.dev0', ray_commit='{{RAY_COMMIT_SHA}}')\n",
      "overlap_gpu_communication = False\n",
      "\n",
      "    \u001b[0m\u001b[37m@pytest\u001b[39;49;00m.mark.skipif(\u001b[95mnot\u001b[39;49;00m USE_GPU, reason=\u001b[33m\"\u001b[39;49;00m\u001b[33mSkipping GPU Test\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[33m\"\u001b[39;49;00m\u001b[33mray_start_regular, overlap_gpu_communication\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m,\u001b[90m\u001b[39;49;00m\n",
      "        [({\u001b[33m\"\u001b[39;49;00m\u001b[33mnum_cpus\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m: \u001b[94m4\u001b[39;49;00m}, \u001b[94mFalse\u001b[39;49;00m), ({\u001b[33m\"\u001b[39;49;00m\u001b[33mnum_cpus\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m: \u001b[94m4\u001b[39;49;00m}, \u001b[94mTrue\u001b[39;49;00m)],\u001b[90m\u001b[39;49;00m\n",
      "        indirect=[\u001b[33m\"\u001b[39;49;00m\u001b[33mray_start_regular\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m],\u001b[90m\u001b[39;49;00m\n",
      "    )\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mtest_torch_tensor_nccl_overlap_timed\u001b[39;49;00m(ray_start_regular, overlap_gpu_communication):\u001b[90m\u001b[39;49;00m\n",
      ">       \u001b[94massert\u001b[39;49;00m (\u001b[90m\u001b[39;49;00m\n",
      "            \u001b[96msum\u001b[39;49;00m(node[\u001b[33m\"\u001b[39;49;00m\u001b[33mResources\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m].get(\u001b[33m\"\u001b[39;49;00m\u001b[33mGPU\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[94m0\u001b[39;49;00m) \u001b[94mfor\u001b[39;49;00m node \u001b[95min\u001b[39;49;00m ray.nodes()) >= \u001b[94m4\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      "        ), \u001b[33m\"\u001b[39;49;00m\u001b[33mThis test requires at least 4 GPUs\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      "\u001b[1m\u001b[31mE       AssertionError: This test requires at least 4 GPUs\u001b[0m\n",
      "\u001b[1m\u001b[31mE       assert 2.0 >= 4\u001b[0m\n",
      "\u001b[1m\u001b[31mE        +  where 2.0 = sum(<generator object test_torch_tensor_nccl_overlap_timed.<locals>.<genexpr> at 0x7fea5626f350>)\u001b[0m\n",
      "\n",
      "\u001b[1m\u001b[31mtest_torch_tensor_dag.py\u001b[0m:379: AssertionError\n",
      "\u001b[31m\u001b[1m________ test_torch_tensor_nccl_overlap_timed[ray_start_regular1-True] _________\u001b[0m\n",
      "\n",
      "ray_start_regular = RayContext(dashboard_url='127.0.0.1:8265', python_version='3.9.22', ray_version='3.0.0.dev0', ray_commit='{{RAY_COMMIT_SHA}}')\n",
      "overlap_gpu_communication = True\n",
      "\n",
      "    \u001b[0m\u001b[37m@pytest\u001b[39;49;00m.mark.skipif(\u001b[95mnot\u001b[39;49;00m USE_GPU, reason=\u001b[33m\"\u001b[39;49;00m\u001b[33mSkipping GPU Test\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[33m\"\u001b[39;49;00m\u001b[33mray_start_regular, overlap_gpu_communication\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m,\u001b[90m\u001b[39;49;00m\n",
      "        [({\u001b[33m\"\u001b[39;49;00m\u001b[33mnum_cpus\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m: \u001b[94m4\u001b[39;49;00m}, \u001b[94mFalse\u001b[39;49;00m), ({\u001b[33m\"\u001b[39;49;00m\u001b[33mnum_cpus\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m: \u001b[94m4\u001b[39;49;00m}, \u001b[94mTrue\u001b[39;49;00m)],\u001b[90m\u001b[39;49;00m\n",
      "        indirect=[\u001b[33m\"\u001b[39;49;00m\u001b[33mray_start_regular\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m],\u001b[90m\u001b[39;49;00m\n",
      "    )\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mtest_torch_tensor_nccl_overlap_timed\u001b[39;49;00m(ray_start_regular, overlap_gpu_communication):\u001b[90m\u001b[39;49;00m\n",
      ">       \u001b[94massert\u001b[39;49;00m (\u001b[90m\u001b[39;49;00m\n",
      "            \u001b[96msum\u001b[39;49;00m(node[\u001b[33m\"\u001b[39;49;00m\u001b[33mResources\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m].get(\u001b[33m\"\u001b[39;49;00m\u001b[33mGPU\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[94m0\u001b[39;49;00m) \u001b[94mfor\u001b[39;49;00m node \u001b[95min\u001b[39;49;00m ray.nodes()) >= \u001b[94m4\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      "        ), \u001b[33m\"\u001b[39;49;00m\u001b[33mThis test requires at least 4 GPUs\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      "\u001b[1m\u001b[31mE       AssertionError: This test requires at least 4 GPUs\u001b[0m\n",
      "\u001b[1m\u001b[31mE       assert 2.0 >= 4\u001b[0m\n",
      "\u001b[1m\u001b[31mE        +  where 2.0 = sum(<generator object test_torch_tensor_nccl_overlap_timed.<locals>.<genexpr> at 0x7fea4e562c10>)\u001b[0m\n",
      "\n",
      "\u001b[1m\u001b[31mtest_torch_tensor_dag.py\u001b[0m:379: AssertionError\n",
      "\u001b[31m\u001b[1m________ test_torch_tensor_default_comm[transports0-ray_start_regular0] ________\u001b[0m\n",
      "\n",
      "ray_start_regular = RayContext(dashboard_url='127.0.0.1:8265', python_version='3.9.22', ray_version='3.0.0.dev0', ray_commit='{{RAY_COMMIT_SHA}}')\n",
      "transports = ['auto', 'nccl']\n",
      "\n",
      "    \u001b[0m\u001b[37m@pytest\u001b[39;49;00m.mark.skipif(\u001b[95mnot\u001b[39;49;00m USE_GPU, reason=\u001b[33m\"\u001b[39;49;00m\u001b[33mSkipping GPU Test\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mray_start_regular\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [{\u001b[33m\"\u001b[39;49;00m\u001b[33mnum_cpus\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m: \u001b[94m4\u001b[39;49;00m}], indirect=\u001b[94mTrue\u001b[39;49;00m)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[33m\"\u001b[39;49;00m\u001b[33mtransports\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m,\u001b[90m\u001b[39;49;00m\n",
      "        [[\u001b[33m\"\u001b[39;49;00m\u001b[33mauto\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33mnccl\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m], [\u001b[33m\"\u001b[39;49;00m\u001b[33mcustom\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33mnccl\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m], [\u001b[33m\"\u001b[39;49;00m\u001b[33mauto\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33mnccl\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m], [\u001b[33m\"\u001b[39;49;00m\u001b[33mcustom\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33mcustom\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m]],\u001b[90m\u001b[39;49;00m\n",
      "    )\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mtest_torch_tensor_default_comm\u001b[39;49;00m(ray_start_regular, transports):\u001b[90m\u001b[39;49;00m\n",
      ">       \u001b[94massert\u001b[39;49;00m (\u001b[90m\u001b[39;49;00m\n",
      "            \u001b[96msum\u001b[39;49;00m(node[\u001b[33m\"\u001b[39;49;00m\u001b[33mResources\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m].get(\u001b[33m\"\u001b[39;49;00m\u001b[33mGPU\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[94m0\u001b[39;49;00m) \u001b[94mfor\u001b[39;49;00m node \u001b[95min\u001b[39;49;00m ray.nodes()) > \u001b[94m2\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      "        ), \u001b[33m\"\u001b[39;49;00m\u001b[33mThis test requires at least 3 GPUs\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      "\u001b[1m\u001b[31mE       AssertionError: This test requires at least 3 GPUs\u001b[0m\n",
      "\u001b[1m\u001b[31mE       assert 2.0 > 2\u001b[0m\n",
      "\u001b[1m\u001b[31mE        +  where 2.0 = sum(<generator object test_torch_tensor_default_comm.<locals>.<genexpr> at 0x7fea5e9112e0>)\u001b[0m\n",
      "\n",
      "\u001b[1m\u001b[31mtest_torch_tensor_dag.py\u001b[0m:744: AssertionError\n",
      "\u001b[31m\u001b[1m________ test_torch_tensor_default_comm[transports1-ray_start_regular0] ________\u001b[0m\n",
      "\n",
      "ray_start_regular = RayContext(dashboard_url='127.0.0.1:8265', python_version='3.9.22', ray_version='3.0.0.dev0', ray_commit='{{RAY_COMMIT_SHA}}')\n",
      "transports = ['custom', 'nccl']\n",
      "\n",
      "    \u001b[0m\u001b[37m@pytest\u001b[39;49;00m.mark.skipif(\u001b[95mnot\u001b[39;49;00m USE_GPU, reason=\u001b[33m\"\u001b[39;49;00m\u001b[33mSkipping GPU Test\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mray_start_regular\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [{\u001b[33m\"\u001b[39;49;00m\u001b[33mnum_cpus\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m: \u001b[94m4\u001b[39;49;00m}], indirect=\u001b[94mTrue\u001b[39;49;00m)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[33m\"\u001b[39;49;00m\u001b[33mtransports\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m,\u001b[90m\u001b[39;49;00m\n",
      "        [[\u001b[33m\"\u001b[39;49;00m\u001b[33mauto\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33mnccl\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m], [\u001b[33m\"\u001b[39;49;00m\u001b[33mcustom\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33mnccl\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m], [\u001b[33m\"\u001b[39;49;00m\u001b[33mauto\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33mnccl\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m], [\u001b[33m\"\u001b[39;49;00m\u001b[33mcustom\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33mcustom\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m]],\u001b[90m\u001b[39;49;00m\n",
      "    )\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mtest_torch_tensor_default_comm\u001b[39;49;00m(ray_start_regular, transports):\u001b[90m\u001b[39;49;00m\n",
      ">       \u001b[94massert\u001b[39;49;00m (\u001b[90m\u001b[39;49;00m\n",
      "            \u001b[96msum\u001b[39;49;00m(node[\u001b[33m\"\u001b[39;49;00m\u001b[33mResources\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m].get(\u001b[33m\"\u001b[39;49;00m\u001b[33mGPU\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[94m0\u001b[39;49;00m) \u001b[94mfor\u001b[39;49;00m node \u001b[95min\u001b[39;49;00m ray.nodes()) > \u001b[94m2\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      "        ), \u001b[33m\"\u001b[39;49;00m\u001b[33mThis test requires at least 3 GPUs\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      "\u001b[1m\u001b[31mE       AssertionError: This test requires at least 3 GPUs\u001b[0m\n",
      "\u001b[1m\u001b[31mE       assert 2.0 > 2\u001b[0m\n",
      "\u001b[1m\u001b[31mE        +  where 2.0 = sum(<generator object test_torch_tensor_default_comm.<locals>.<genexpr> at 0x7fea4e585200>)\u001b[0m\n",
      "\n",
      "\u001b[1m\u001b[31mtest_torch_tensor_dag.py\u001b[0m:744: AssertionError\n",
      "\u001b[31m\u001b[1m________ test_torch_tensor_default_comm[transports2-ray_start_regular0] ________\u001b[0m\n",
      "\n",
      "ray_start_regular = RayContext(dashboard_url='127.0.0.1:8265', python_version='3.9.22', ray_version='3.0.0.dev0', ray_commit='{{RAY_COMMIT_SHA}}')\n",
      "transports = ['auto', 'nccl']\n",
      "\n",
      "    \u001b[0m\u001b[37m@pytest\u001b[39;49;00m.mark.skipif(\u001b[95mnot\u001b[39;49;00m USE_GPU, reason=\u001b[33m\"\u001b[39;49;00m\u001b[33mSkipping GPU Test\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mray_start_regular\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [{\u001b[33m\"\u001b[39;49;00m\u001b[33mnum_cpus\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m: \u001b[94m4\u001b[39;49;00m}], indirect=\u001b[94mTrue\u001b[39;49;00m)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[33m\"\u001b[39;49;00m\u001b[33mtransports\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m,\u001b[90m\u001b[39;49;00m\n",
      "        [[\u001b[33m\"\u001b[39;49;00m\u001b[33mauto\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33mnccl\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m], [\u001b[33m\"\u001b[39;49;00m\u001b[33mcustom\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33mnccl\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m], [\u001b[33m\"\u001b[39;49;00m\u001b[33mauto\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33mnccl\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m], [\u001b[33m\"\u001b[39;49;00m\u001b[33mcustom\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33mcustom\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m]],\u001b[90m\u001b[39;49;00m\n",
      "    )\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mtest_torch_tensor_default_comm\u001b[39;49;00m(ray_start_regular, transports):\u001b[90m\u001b[39;49;00m\n",
      ">       \u001b[94massert\u001b[39;49;00m (\u001b[90m\u001b[39;49;00m\n",
      "            \u001b[96msum\u001b[39;49;00m(node[\u001b[33m\"\u001b[39;49;00m\u001b[33mResources\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m].get(\u001b[33m\"\u001b[39;49;00m\u001b[33mGPU\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[94m0\u001b[39;49;00m) \u001b[94mfor\u001b[39;49;00m node \u001b[95min\u001b[39;49;00m ray.nodes()) > \u001b[94m2\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      "        ), \u001b[33m\"\u001b[39;49;00m\u001b[33mThis test requires at least 3 GPUs\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      "\u001b[1m\u001b[31mE       AssertionError: This test requires at least 3 GPUs\u001b[0m\n",
      "\u001b[1m\u001b[31mE       assert 2.0 > 2\u001b[0m\n",
      "\u001b[1m\u001b[31mE        +  where 2.0 = sum(<generator object test_torch_tensor_default_comm.<locals>.<genexpr> at 0x7fea5626f350>)\u001b[0m\n",
      "\n",
      "\u001b[1m\u001b[31mtest_torch_tensor_dag.py\u001b[0m:744: AssertionError\n",
      "\u001b[31m\u001b[1m________ test_torch_tensor_default_comm[transports3-ray_start_regular0] ________\u001b[0m\n",
      "\n",
      "ray_start_regular = RayContext(dashboard_url='127.0.0.1:8265', python_version='3.9.22', ray_version='3.0.0.dev0', ray_commit='{{RAY_COMMIT_SHA}}')\n",
      "transports = ['custom', 'custom']\n",
      "\n",
      "    \u001b[0m\u001b[37m@pytest\u001b[39;49;00m.mark.skipif(\u001b[95mnot\u001b[39;49;00m USE_GPU, reason=\u001b[33m\"\u001b[39;49;00m\u001b[33mSkipping GPU Test\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mray_start_regular\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [{\u001b[33m\"\u001b[39;49;00m\u001b[33mnum_cpus\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m: \u001b[94m4\u001b[39;49;00m}], indirect=\u001b[94mTrue\u001b[39;49;00m)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[33m\"\u001b[39;49;00m\u001b[33mtransports\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m,\u001b[90m\u001b[39;49;00m\n",
      "        [[\u001b[33m\"\u001b[39;49;00m\u001b[33mauto\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33mnccl\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m], [\u001b[33m\"\u001b[39;49;00m\u001b[33mcustom\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33mnccl\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m], [\u001b[33m\"\u001b[39;49;00m\u001b[33mauto\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33mnccl\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m], [\u001b[33m\"\u001b[39;49;00m\u001b[33mcustom\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33mcustom\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m]],\u001b[90m\u001b[39;49;00m\n",
      "    )\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mtest_torch_tensor_default_comm\u001b[39;49;00m(ray_start_regular, transports):\u001b[90m\u001b[39;49;00m\n",
      ">       \u001b[94massert\u001b[39;49;00m (\u001b[90m\u001b[39;49;00m\n",
      "            \u001b[96msum\u001b[39;49;00m(node[\u001b[33m\"\u001b[39;49;00m\u001b[33mResources\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m].get(\u001b[33m\"\u001b[39;49;00m\u001b[33mGPU\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[94m0\u001b[39;49;00m) \u001b[94mfor\u001b[39;49;00m node \u001b[95min\u001b[39;49;00m ray.nodes()) > \u001b[94m2\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      "        ), \u001b[33m\"\u001b[39;49;00m\u001b[33mThis test requires at least 3 GPUs\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      "\u001b[1m\u001b[31mE       AssertionError: This test requires at least 3 GPUs\u001b[0m\n",
      "\u001b[1m\u001b[31mE       assert 2.0 > 2\u001b[0m\n",
      "\u001b[1m\u001b[31mE        +  where 2.0 = sum(<generator object test_torch_tensor_default_comm.<locals>.<genexpr> at 0x7fea4e670dd0>)\u001b[0m\n",
      "\n",
      "\u001b[1m\u001b[31mtest_torch_tensor_dag.py\u001b[0m:744: AssertionError\n",
      "\u001b[36m\u001b[1m=========================== short test summary info ============================\u001b[0m\n",
      "\u001b[31mFAILED\u001b[0m test_torch_tensor_dag.py::\u001b[1mtest_torch_tensor_nccl_overlap_timed[ray_start_regular0-False]\u001b[0m - AssertionError: This test requires at least 4 GPUs\n",
      "\u001b[31mFAILED\u001b[0m test_torch_tensor_dag.py::\u001b[1mtest_torch_tensor_nccl_overlap_timed[ray_start_regular1-True]\u001b[0m - AssertionError: This test requires at least 4 GPUs\n",
      "\u001b[31mFAILED\u001b[0m test_torch_tensor_dag.py::\u001b[1mtest_torch_tensor_default_comm[transports0-ray_start_regular0]\u001b[0m - AssertionError: This test requires at least 3 GPUs\n",
      "\u001b[31mFAILED\u001b[0m test_torch_tensor_dag.py::\u001b[1mtest_torch_tensor_default_comm[transports1-ray_start_regular0]\u001b[0m - AssertionError: This test requires at least 3 GPUs\n",
      "\u001b[31mFAILED\u001b[0m test_torch_tensor_dag.py::\u001b[1mtest_torch_tensor_default_comm[transports2-ray_start_regular0]\u001b[0m - AssertionError: This test requires at least 3 GPUs\n",
      "\u001b[31mFAILED\u001b[0m test_torch_tensor_dag.py::\u001b[1mtest_torch_tensor_default_comm[transports3-ray_start_regular0]\u001b[0m - AssertionError: This test requires at least 3 GPUs\n",
      "\u001b[31m============= \u001b[31m\u001b[1m6 failed\u001b[0m, \u001b[32m45 passed\u001b[0m, \u001b[33m2 skipped\u001b[0m\u001b[31m in 422.69s (0:07:02)\u001b[0m\u001b[31m ==============\u001b[0m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "# !python -m pytest -v -s test_torch_tensor_dag.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m============================= test session starts ==============================\u001b[0m\n",
      "platform linux -- Python 3.9.22, pytest-8.3.5, pluggy-1.5.0 -- /home/wxdeng/miniconda3/envs/coll-sched-0512-py39/bin/python\n",
      "cachedir: .pytest_cache\n",
      "rootdir: /home/wxdeng/stbr-min\n",
      "configfile: pytest.ini\n",
      "plugins: anyio-4.9.0\n",
      "collected 1 item                                                               \u001b[0m\u001b[1m\n",
      "\n",
      "test_accelerated_dag.py::test_event_profiling 2025-05-14 12:19:08,992\tINFO worker.py:1879 -- Started a local Ray instance. View the dashboard at \u001b[1m\u001b[32m127.0.0.1:8265 \u001b[39m\u001b[22m\n",
      "\u001b[33m(raylet)\u001b[0m [2025-05-14 12:19:08,973 E 1342745 1342774] (raylet) file_system_monitor.cc:116: /tmp/ray/session_2025-05-14_12-19-06_983534_1341950 is over 95% full, available space: 81.2457 GB; capacity: 1830.63 GB. Object creation will fail if spilling is required.\n",
      "2025-05-14 12:19:10,342\tINFO dag_node_operation.py:618 -- Writing compiled graph schedule visualization to compiled_graph_schedule.png\n",
      "\u001b[32mPASSED\u001b[0m2025-05-14 12:19:11,893\tINFO compiled_dag_node.py:2173 -- Tearing down compiled DAG\n",
      "2025-05-14 12:19:11,893\tINFO compiled_dag_node.py:2178 -- Cancelling compiled worker on actor: Actor(Actor, e0d74c8839f6826ba2508a2e01000000)\n",
      "2025-05-14 12:19:11,893\tINFO compiled_dag_node.py:2178 -- Cancelling compiled worker on actor: Actor(Actor, 6602945de71d6111e082988b01000000)\n",
      "2025-05-14 12:19:11,898\tINFO compiled_dag_node.py:2200 -- Waiting for worker tasks to exit\n",
      "2025-05-14 12:19:11,898\tINFO compiled_dag_node.py:2161 -- Killing actor: Actor(Actor, e0d74c8839f6826ba2508a2e01000000)\n",
      "2025-05-14 12:19:11,899\tINFO compiled_dag_node.py:2161 -- Killing actor: Actor(Actor, 6602945de71d6111e082988b01000000)\n",
      "2025-05-14 12:19:11,899\tINFO compiled_dag_node.py:2203 -- Teardown complete\n",
      "\n",
      "\n",
      "\u001b[32m============================== \u001b[32m\u001b[1m1 passed\u001b[0m\u001b[32m in 7.88s\u001b[0m\u001b[32m ===============================\u001b[0m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "# !python -m pytest -v -s test_accelerated_dag.py::test_event_profiling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m============================= test session starts ==============================\u001b[0m\n",
      "platform linux -- Python 3.9.22, pytest-8.3.5, pluggy-1.5.0 -- /home/wxdeng/miniconda3/envs/coll-sched-0512-py39/bin/python\n",
      "cachedir: .pytest_cache\n",
      "rootdir: /home/wxdeng/stbr-min\n",
      "configfile: pytest.ini\n",
      "plugins: anyio-4.9.0\n",
      "collected 7 items                                                              \u001b[0m\u001b[1m\n",
      "\n",
      "test_cpu_communicator_dag.py::test_p2p_basic[ray_start_cluster0] 2025-05-14 12:01:25,738\tINFO worker.py:1694 -- Connecting to existing Ray cluster at address: 128.208.3.124:57649...\n",
      "2025-05-14 12:01:25,748\tINFO worker.py:1879 -- Connected to Ray cluster. View the dashboard at \u001b[1m\u001b[32m127.0.0.1:8265 \u001b[39m\u001b[22m\n",
      "2025-05-14 12:01:27,857\tINFO dag_node_operation.py:618 -- Writing compiled graph schedule visualization to compiled_graph_schedule.png\n",
      "\u001b[32mPASSED\u001b[0m2025-05-14 12:01:28,025\tINFO compiled_dag_node.py:2173 -- Tearing down compiled DAG\n",
      "2025-05-14 12:01:28,026\tINFO compiled_dag_node.py:2178 -- Cancelling compiled worker on actor: Actor(CPUTorchTensorWorker, dbacf34d15b40a2124032e7b01000000)\n",
      "2025-05-14 12:01:28,026\tINFO compiled_dag_node.py:2178 -- Cancelling compiled worker on actor: Actor(CPUTorchTensorWorker, d290cb89c83c49890d866db701000000)\n",
      "2025-05-14 12:01:28,034\tINFO compiled_dag_node.py:2200 -- Waiting for worker tasks to exit\n",
      "2025-05-14 12:01:28,035\tINFO compiled_dag_node.py:2161 -- Killing actor: Actor(CPUTorchTensorWorker, dbacf34d15b40a2124032e7b01000000)\n",
      "2025-05-14 12:01:28,035\tINFO compiled_dag_node.py:2161 -- Killing actor: Actor(CPUTorchTensorWorker, d290cb89c83c49890d866db701000000)\n",
      "2025-05-14 12:01:28,035\tINFO compiled_dag_node.py:2203 -- Teardown complete\n",
      "\n",
      "test_cpu_communicator_dag.py::test_allreduce_basic[ray_start_cluster0] 2025-05-14 12:01:31,398\tINFO worker.py:1694 -- Connecting to existing Ray cluster at address: 128.208.3.124:63236...\n",
      "2025-05-14 12:01:31,409\tINFO worker.py:1879 -- Connected to Ray cluster. View the dashboard at \u001b[1m\u001b[32m127.0.0.1:8265 \u001b[39m\u001b[22m\n",
      "2025-05-14 12:01:33,309\tINFO torch_tensor_nccl_channel.py:770 -- Initializing custom NCCL group 49b43831-f2f3-4158-b70b-28ad32d27720 on actors: [Actor(CPUTorchTensorWorker, 220b11c90a89b5e72a2e415401000000), Actor(CPUTorchTensorWorker, 68f44897b83710d9f161328b01000000)]\n",
      "2025-05-14 12:01:33,314\tINFO torch_tensor_nccl_channel.py:797 -- NCCL group initialized.\n",
      "2025-05-14 12:01:33,474\tINFO dag_node_operation.py:618 -- Writing compiled graph schedule visualization to compiled_graph_schedule.png\n",
      "\u001b[32mPASSED\u001b[0m2025-05-14 12:01:35,588\tINFO compiled_dag_node.py:2173 -- Tearing down compiled DAG\n",
      "2025-05-14 12:01:35,589\tINFO compiled_dag_node.py:2178 -- Cancelling compiled worker on actor: Actor(CPUTorchTensorWorker, 220b11c90a89b5e72a2e415401000000)\n",
      "2025-05-14 12:01:35,589\tINFO compiled_dag_node.py:2178 -- Cancelling compiled worker on actor: Actor(CPUTorchTensorWorker, 68f44897b83710d9f161328b01000000)\n",
      "2025-05-14 12:01:35,596\tINFO compiled_dag_node.py:2200 -- Waiting for worker tasks to exit\n",
      "2025-05-14 12:01:35,596\tINFO compiled_dag_node.py:2161 -- Killing actor: Actor(CPUTorchTensorWorker, 220b11c90a89b5e72a2e415401000000)\n",
      "2025-05-14 12:01:35,596\tINFO compiled_dag_node.py:2161 -- Killing actor: Actor(CPUTorchTensorWorker, 68f44897b83710d9f161328b01000000)\n",
      "2025-05-14 12:01:35,596\tINFO compiled_dag_node.py:2203 -- Teardown complete\n",
      "\n",
      "test_cpu_communicator_dag.py::test_allreduce_get_partial[ray_start_cluster0] 2025-05-14 12:01:38,898\tINFO worker.py:1694 -- Connecting to existing Ray cluster at address: 128.208.3.124:44092...\n",
      "2025-05-14 12:01:38,908\tINFO worker.py:1879 -- Connected to Ray cluster. View the dashboard at \u001b[1m\u001b[32m127.0.0.1:8265 \u001b[39m\u001b[22m\n",
      "2025-05-14 12:01:40,806\tINFO torch_tensor_nccl_channel.py:770 -- Initializing custom NCCL group 65cbde50-a51d-404d-a181-54251e51ed9b on actors: [Actor(CPUTorchTensorWorker, f96b90246dd7c69af7ce2f7901000000), Actor(CPUTorchTensorWorker, 677bd611bbd3b735c0be55be01000000)]\n",
      "2025-05-14 12:01:40,810\tINFO torch_tensor_nccl_channel.py:797 -- NCCL group initialized.\n",
      "2025-05-14 12:01:41,028\tINFO dag_node_operation.py:618 -- Writing compiled graph schedule visualization to compiled_graph_schedule.png\n",
      "\u001b[32mPASSED\u001b[0m2025-05-14 12:01:43,057\tINFO compiled_dag_node.py:2173 -- Tearing down compiled DAG\n",
      "2025-05-14 12:01:43,058\tINFO compiled_dag_node.py:2178 -- Cancelling compiled worker on actor: Actor(CPUTorchTensorWorker, f96b90246dd7c69af7ce2f7901000000)\n",
      "2025-05-14 12:01:43,058\tINFO compiled_dag_node.py:2178 -- Cancelling compiled worker on actor: Actor(CPUTorchTensorWorker, 677bd611bbd3b735c0be55be01000000)\n",
      "2025-05-14 12:01:43,067\tINFO compiled_dag_node.py:2200 -- Waiting for worker tasks to exit\n",
      "2025-05-14 12:01:43,067\tINFO compiled_dag_node.py:2161 -- Killing actor: Actor(CPUTorchTensorWorker, f96b90246dd7c69af7ce2f7901000000)\n",
      "2025-05-14 12:01:43,068\tINFO compiled_dag_node.py:2161 -- Killing actor: Actor(CPUTorchTensorWorker, 677bd611bbd3b735c0be55be01000000)\n",
      "2025-05-14 12:01:43,068\tINFO compiled_dag_node.py:2203 -- Teardown complete\n",
      "\n",
      "test_cpu_communicator_dag.py::test_allreduce_wrong_shape[ray_start_cluster0] 2025-05-14 12:01:46,526\tINFO worker.py:1694 -- Connecting to existing Ray cluster at address: 128.208.3.124:64144...\n",
      "2025-05-14 12:01:46,536\tINFO worker.py:1879 -- Connected to Ray cluster. View the dashboard at \u001b[1m\u001b[32m127.0.0.1:8265 \u001b[39m\u001b[22m\n",
      "2025-05-14 12:01:48,461\tINFO torch_tensor_nccl_channel.py:770 -- Initializing custom NCCL group f19f9177-f339-4658-bc0c-8a17e9cbd1fc on actors: [Actor(CPUTorchTensorWorker, ca6f04a829e8b419069bd44001000000), Actor(CPUTorchTensorWorker, fd52337a17d36ebd0cf0ad9601000000)]\n",
      "2025-05-14 12:01:48,465\tINFO torch_tensor_nccl_channel.py:797 -- NCCL group initialized.\n",
      "2025-05-14 12:01:48,613\tINFO dag_node_operation.py:618 -- Writing compiled graph schedule visualization to compiled_graph_schedule.png\n",
      "2025-05-14 12:01:50,714\tWARNING exceptions.py:233 -- User exception type <class 'ray.exceptions.RayTaskError(RuntimeError)'> in RayTaskError can't be subclassed! This exception is raised as RayTaskError only. You can use `ray_task_error.cause` to access the user exception. Failure in subclassing: Cannot create a consistent method resolution\n",
      "order (MRO) for bases RayTaskError, RayTaskError(RuntimeError)\n",
      "\u001b[36m(CPUTorchTensorWorker pid=1156893)\u001b[0m User exception type <class 'ray.exceptions.RayTaskError(RuntimeError)'> in RayTaskError can't be subclassed! This exception is raised as RayTaskError only. You can use `ray_task_error.cause` to access the user exception. Failure in subclassing: Cannot create a consistent method resolution\n",
      "\u001b[36m(CPUTorchTensorWorker pid=1156893)\u001b[0m order (MRO) for bases RayTaskError, RayTaskError(RuntimeError)\n",
      "\u001b[33m(raylet)\u001b[0m [2025-05-14 12:01:56,500 E 1156799 1156828] (raylet) file_system_monitor.cc:116: /tmp/ray/session_2025-05-14_12-01-44_717698_1152219 is over 95% full, available space: 81.3412 GB; capacity: 1830.63 GB. Object creation will fail if spilling is required.\n",
      "\u001b[32mPASSED\u001b[0m2025-05-14 12:02:00,716\tINFO compiled_dag_node.py:2173 -- Tearing down compiled DAG\n",
      "2025-05-14 12:02:00,716\tINFO compiled_dag_node.py:2178 -- Cancelling compiled worker on actor: Actor(CPUTorchTensorWorker, ca6f04a829e8b419069bd44001000000)\n",
      "2025-05-14 12:02:00,716\tINFO compiled_dag_node.py:2178 -- Cancelling compiled worker on actor: Actor(CPUTorchTensorWorker, fd52337a17d36ebd0cf0ad9601000000)\n",
      "2025-05-14 12:02:00,723\tINFO compiled_dag_node.py:2200 -- Waiting for worker tasks to exit\n",
      "\u001b[33m(raylet)\u001b[0m [2025-05-14 12:02:06,511 E 1156799 1156828] (raylet) file_system_monitor.cc:116: /tmp/ray/session_2025-05-14_12-01-44_717698_1152219 is over 95% full, available space: 81.3412 GB; capacity: 1830.63 GB. Object creation will fail if spilling is required.\n",
      "\u001b[33m(raylet)\u001b[0m [2025-05-14 12:02:16,525 E 1156799 1156828] (raylet) file_system_monitor.cc:116: /tmp/ray/session_2025-05-14_12-01-44_717698_1152219 is over 95% full, available space: 81.3412 GB; capacity: 1830.63 GB. Object creation will fail if spilling is required.\n",
      "\u001b[33m(raylet)\u001b[0m [2025-05-14 12:02:26,538 E 1156799 1156828] (raylet) file_system_monitor.cc:116: /tmp/ray/session_2025-05-14_12-01-44_717698_1152219 is over 95% full, available space: 81.3411 GB; capacity: 1830.63 GB. Object creation will fail if spilling is required.\n",
      "2025-05-14 12:02:30,728\tWARNING compiled_dag_node.py:2141 -- Compiled DAG actor Actor(CPUTorchTensorWorker, ca6f04a829e8b419069bd44001000000) is still running 30s after teardown(). Force-killing actor. Increase RAY_CGRAPH_teardown_timeout if you want teardown to wait longer.\n",
      "2025-05-14 12:02:30,775\tINFO compiled_dag_node.py:2161 -- Killing actor: Actor(CPUTorchTensorWorker, ca6f04a829e8b419069bd44001000000)\n",
      "2025-05-14 12:02:30,775\tINFO compiled_dag_node.py:2161 -- Killing actor: Actor(CPUTorchTensorWorker, fd52337a17d36ebd0cf0ad9601000000)\n",
      "2025-05-14 12:02:30,775\tINFO compiled_dag_node.py:2203 -- Teardown complete\n",
      "\n",
      "test_cpu_communicator_dag.py::test_allreduce_scheduling[ray_start_cluster0] 2025-05-14 12:02:35,095\tINFO worker.py:1694 -- Connecting to existing Ray cluster at address: 128.208.3.124:52388...\n",
      "2025-05-14 12:02:35,105\tINFO worker.py:1879 -- Connected to Ray cluster. View the dashboard at \u001b[1m\u001b[32m127.0.0.1:8265 \u001b[39m\u001b[22m\n",
      "2025-05-14 12:02:37,024\tINFO torch_tensor_nccl_channel.py:770 -- Initializing custom NCCL group ea16df51-00be-41a9-9369-8d6ed6933aae on actors: [Actor(CPUTorchTensorWorker, 78b7bb527f006d4ed1bf79e101000000), Actor(CPUTorchTensorWorker, 08ee4a7ad61fbee2836e4b0d01000000)]\n",
      "2025-05-14 12:02:37,029\tINFO torch_tensor_nccl_channel.py:797 -- NCCL group initialized.\n",
      "2025-05-14 12:02:37,216\tINFO dag_node_operation.py:618 -- Writing compiled graph schedule visualization to compiled_graph_schedule.png\n",
      "\u001b[32mPASSED\u001b[0m2025-05-14 12:02:39,290\tINFO compiled_dag_node.py:2173 -- Tearing down compiled DAG\n",
      "2025-05-14 12:02:39,291\tINFO compiled_dag_node.py:2178 -- Cancelling compiled worker on actor: Actor(CPUTorchTensorWorker, 78b7bb527f006d4ed1bf79e101000000)\n",
      "2025-05-14 12:02:39,291\tINFO compiled_dag_node.py:2178 -- Cancelling compiled worker on actor: Actor(CPUTorchTensorWorker, 08ee4a7ad61fbee2836e4b0d01000000)\n",
      "2025-05-14 12:02:39,301\tINFO compiled_dag_node.py:2200 -- Waiting for worker tasks to exit\n",
      "2025-05-14 12:02:39,302\tINFO compiled_dag_node.py:2161 -- Killing actor: Actor(CPUTorchTensorWorker, 78b7bb527f006d4ed1bf79e101000000)\n",
      "2025-05-14 12:02:39,302\tINFO compiled_dag_node.py:2161 -- Killing actor: Actor(CPUTorchTensorWorker, 08ee4a7ad61fbee2836e4b0d01000000)\n",
      "2025-05-14 12:02:39,302\tINFO compiled_dag_node.py:2203 -- Teardown complete\n",
      "\n",
      "test_cpu_communicator_dag.py::test_allreduce_duplicate_actors[ray_start_cluster0] 2025-05-14 12:02:42,710\tINFO worker.py:1694 -- Connecting to existing Ray cluster at address: 128.208.3.124:56918...\n",
      "2025-05-14 12:02:42,720\tINFO worker.py:1879 -- Connected to Ray cluster. View the dashboard at \u001b[1m\u001b[32m127.0.0.1:8265 \u001b[39m\u001b[22m\n",
      "\u001b[32mPASSED\u001b[0m\n",
      "test_cpu_communicator_dag.py::test_allreduce_wrong_actors[ray_start_cluster0] 2025-05-14 12:02:47,422\tINFO worker.py:1694 -- Connecting to existing Ray cluster at address: 128.208.3.124:57877...\n",
      "2025-05-14 12:02:47,433\tINFO worker.py:1879 -- Connected to Ray cluster. View the dashboard at \u001b[1m\u001b[32m127.0.0.1:8265 \u001b[39m\u001b[22m\n",
      "\u001b[32mPASSED\u001b[0m\n",
      "\n",
      "\u001b[32m========================= \u001b[32m\u001b[1m7 passed\u001b[0m\u001b[32m in 87.24s (0:01:27)\u001b[0m\u001b[32m =========================\u001b[0m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "# !python -m pytest -v -s test_cpu_communicator_dag.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "# !python -m pytest -v -s test_execution_schedule_gpu.py"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "stbr-coll-sched-0512",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
