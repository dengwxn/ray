{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.chdir(\"/home/wxdeng/stbr-min/python/ray/dag/tests/experimental\")\n",
    "os.environ[\"RAY_DEDUP_LOGS\"] = \"0\"\n",
    "os.environ[\"RAY_PYTEST_USE_GPU\"] = \"1\"\n",
    "os.environ[\"RAY_CGRAPH_VISUALIZE_SCHEDULE\"] = \"1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-05-14 00:21:10,975\tINFO worker.py:1879 -- Started a local Ray instance. View the dashboard at \u001b[1m\u001b[32mhttp://127.0.0.1:8265 \u001b[39m\u001b[22m\n",
      "2025-05-14 00:21:15,213\tINFO torch_tensor_nccl_channel.py:772 -- Creating NCCL group 092eb89c-5f22-4620-ba5b-c81a0bb35838 on actors: [Actor(Actor, c34a99bf64e99ff897a8188a01000000), Actor(Actor, b9ada918cc0ffcf0acaea8bb01000000)]\n",
      "2025-05-14 00:21:19,526\tINFO torch_tensor_nccl_channel.py:797 -- NCCL group initialized.\n",
      "2025-05-14 00:21:19,692\tINFO dag_node_operation.py:628 -- Writing compiled graph schedule visualization to compiled_graph_schedule.png\n",
      "2025-05-14 00:21:20,015\tINFO compiled_dag_node.py:2173 -- Tearing down compiled DAG\n",
      "2025-05-14 00:21:20,020\tINFO compiled_dag_node.py:2178 -- Cancelling compiled worker on actor: Actor(Actor, c34a99bf64e99ff897a8188a01000000)\n",
      "2025-05-14 00:21:20,020\tINFO compiled_dag_node.py:2178 -- Cancelling compiled worker on actor: Actor(Actor, b9ada918cc0ffcf0acaea8bb01000000)\n",
      "\u001b[36m(Actor pid=3706666)\u001b[0m Destructing NCCL group on actor: Actor(Actor, c34a99bf64e99ff897a8188a01000000)\n",
      "\u001b[36m(Actor pid=3706688)\u001b[0m Destructing NCCL group on actor: Actor(Actor, b9ada918cc0ffcf0acaea8bb01000000)\n",
      "2025-05-14 00:21:20,887\tINFO compiled_dag_node.py:2200 -- Waiting for worker tasks to exit\n",
      "2025-05-14 00:21:20,888\tINFO compiled_dag_node.py:2161 -- Killing actor: Actor(Actor, c34a99bf64e99ff897a8188a01000000)\n",
      "2025-05-14 00:21:20,889\tINFO compiled_dag_node.py:2161 -- Killing actor: Actor(Actor, b9ada918cc0ffcf0acaea8bb01000000)\n",
      "2025-05-14 00:21:20,889\tINFO compiled_dag_node.py:2203 -- Teardown complete\n",
      "2025-05-14 00:21:28,065\tINFO worker.py:1879 -- Started a local Ray instance. View the dashboard at \u001b[1m\u001b[32mhttp://127.0.0.1:8265 \u001b[39m\u001b[22m\n",
      "2025-05-14 00:21:32,382\tINFO torch_tensor_nccl_channel.py:772 -- Creating NCCL group 28e9e512-2632-4708-aa30-94e98d014ead on actors: [Actor(Actor, 8df7c6f1966539202be4c0f101000000), Actor(Actor, 444d696be60bc7400d1da6b501000000), Actor(Actor, 678c955e9a46159572dd6cb901000000), Actor(Actor, 09fe160beb7b03a67842d5d601000000)]\n",
      "2025-05-14 00:21:38,047\tINFO torch_tensor_nccl_channel.py:797 -- NCCL group initialized.\n",
      "2025-05-14 00:21:38,337\tINFO dag_node_operation.py:628 -- Writing compiled graph schedule visualization to compiled_graph_schedule.png\n",
      "2025-05-14 00:21:38,844\tINFO compiled_dag_node.py:2173 -- Tearing down compiled DAG\n",
      "2025-05-14 00:21:38,851\tINFO compiled_dag_node.py:2178 -- Cancelling compiled worker on actor: Actor(Actor, 8df7c6f1966539202be4c0f101000000)\n",
      "2025-05-14 00:21:38,852\tINFO compiled_dag_node.py:2178 -- Cancelling compiled worker on actor: Actor(Actor, 444d696be60bc7400d1da6b501000000)\n",
      "2025-05-14 00:21:38,852\tINFO compiled_dag_node.py:2178 -- Cancelling compiled worker on actor: Actor(Actor, 678c955e9a46159572dd6cb901000000)\n",
      "2025-05-14 00:21:38,852\tINFO compiled_dag_node.py:2178 -- Cancelling compiled worker on actor: Actor(Actor, 09fe160beb7b03a67842d5d601000000)\n",
      "\u001b[36m(Actor pid=3724952)\u001b[0m Destructing NCCL group on actor: Actor(Actor, 444d696be60bc7400d1da6b501000000)\n",
      "\u001b[36m(Actor pid=3724948)\u001b[0m Destructing NCCL group on actor: Actor(Actor, 09fe160beb7b03a67842d5d601000000)\n",
      "\u001b[36m(Actor pid=3724945)\u001b[0m Destructing NCCL group on actor: Actor(Actor, 678c955e9a46159572dd6cb901000000)\n",
      "\u001b[36m(Actor pid=3724963)\u001b[0m Destructing NCCL group on actor: Actor(Actor, 8df7c6f1966539202be4c0f101000000)\n",
      "2025-05-14 00:21:39,898\tINFO compiled_dag_node.py:2200 -- Waiting for worker tasks to exit\n",
      "2025-05-14 00:21:39,899\tINFO compiled_dag_node.py:2161 -- Killing actor: Actor(Actor, 8df7c6f1966539202be4c0f101000000)\n",
      "2025-05-14 00:21:39,899\tINFO compiled_dag_node.py:2161 -- Killing actor: Actor(Actor, 444d696be60bc7400d1da6b501000000)\n",
      "2025-05-14 00:21:39,899\tINFO compiled_dag_node.py:2161 -- Killing actor: Actor(Actor, 678c955e9a46159572dd6cb901000000)\n",
      "2025-05-14 00:21:39,899\tINFO compiled_dag_node.py:2161 -- Killing actor: Actor(Actor, 09fe160beb7b03a67842d5d601000000)\n",
      "2025-05-14 00:21:39,899\tINFO compiled_dag_node.py:2203 -- Teardown complete\n",
      "2025-05-14 00:21:46,219\tINFO worker.py:1879 -- Started a local Ray instance. View the dashboard at \u001b[1m\u001b[32mhttp://127.0.0.1:8265 \u001b[39m\u001b[22m\n",
      "2025-05-14 00:21:51,883\tINFO torch_tensor_nccl_channel.py:772 -- Creating NCCL group 8543da94-e76e-4db7-8987-316ab590563e on actors: [Actor(Actor, 15226e81c8a57a163551893501000000), Actor(Actor, f306fc11374a6bc061c2e14c01000000), Actor(Actor, 90658065c760f1033108ebe001000000), Actor(Actor, 286c070782b744004b3b8b4901000000), Actor(Actor, 61b46bc4eb2ac0f9de809a7b01000000), Actor(Actor, e560e2042494874e334ef3d501000000), Actor(Actor, 5bd32ced761b2518e7ce86f101000000), Actor(Actor, d4c2f43913ad77e6e7cef68001000000)]\n",
      "2025-05-14 00:22:01,799\tINFO torch_tensor_nccl_channel.py:797 -- NCCL group initialized.\n",
      "2025-05-14 00:22:02,342\tINFO dag_node_operation.py:628 -- Writing compiled graph schedule visualization to compiled_graph_schedule.png\n",
      "2025-05-14 00:22:03,182\tINFO compiled_dag_node.py:2173 -- Tearing down compiled DAG\n",
      "2025-05-14 00:22:03,198\tINFO compiled_dag_node.py:2178 -- Cancelling compiled worker on actor: Actor(Actor, 15226e81c8a57a163551893501000000)\n",
      "2025-05-14 00:22:03,198\tINFO compiled_dag_node.py:2178 -- Cancelling compiled worker on actor: Actor(Actor, f306fc11374a6bc061c2e14c01000000)\n",
      "2025-05-14 00:22:03,198\tINFO compiled_dag_node.py:2178 -- Cancelling compiled worker on actor: Actor(Actor, 90658065c760f1033108ebe001000000)\n",
      "2025-05-14 00:22:03,198\tINFO compiled_dag_node.py:2178 -- Cancelling compiled worker on actor: Actor(Actor, 286c070782b744004b3b8b4901000000)\n",
      "2025-05-14 00:22:03,198\tINFO compiled_dag_node.py:2178 -- Cancelling compiled worker on actor: Actor(Actor, 61b46bc4eb2ac0f9de809a7b01000000)\n",
      "2025-05-14 00:22:03,198\tINFO compiled_dag_node.py:2178 -- Cancelling compiled worker on actor: Actor(Actor, e560e2042494874e334ef3d501000000)\n",
      "2025-05-14 00:22:03,198\tINFO compiled_dag_node.py:2178 -- Cancelling compiled worker on actor: Actor(Actor, 5bd32ced761b2518e7ce86f101000000)\n",
      "2025-05-14 00:22:03,198\tINFO compiled_dag_node.py:2178 -- Cancelling compiled worker on actor: Actor(Actor, d4c2f43913ad77e6e7cef68001000000)\n",
      "\u001b[36m(Actor pid=3743753)\u001b[0m Destructing NCCL group on actor: Actor(Actor, 5bd32ced761b2518e7ce86f101000000)\n",
      "\u001b[36m(Actor pid=3743757)\u001b[0m Destructing NCCL group on actor: Actor(Actor, 90658065c760f1033108ebe001000000)\n",
      "\u001b[36m(Actor pid=3743762)\u001b[0m Destructing NCCL group on actor: Actor(Actor, d4c2f43913ad77e6e7cef68001000000)\n",
      "\u001b[36m(Actor pid=3743755)\u001b[0m Destructing NCCL group on actor: Actor(Actor, 61b46bc4eb2ac0f9de809a7b01000000)\n",
      "\u001b[36m(Actor pid=3743756)\u001b[0m Destructing NCCL group on actor: Actor(Actor, 15226e81c8a57a163551893501000000)\n",
      "\u001b[36m(Actor pid=3743752)\u001b[0m Destructing NCCL group on actor: Actor(Actor, f306fc11374a6bc061c2e14c01000000)\n",
      "\u001b[36m(Actor pid=3743760)\u001b[0m Destructing NCCL group on actor: Actor(Actor, 286c070782b744004b3b8b4901000000)\n",
      "\u001b[36m(Actor pid=3743765)\u001b[0m Destructing NCCL group on actor: Actor(Actor, e560e2042494874e334ef3d501000000)\n",
      "2025-05-14 00:22:04,238\tINFO compiled_dag_node.py:2200 -- Waiting for worker tasks to exit\n",
      "2025-05-14 00:22:04,239\tINFO compiled_dag_node.py:2161 -- Killing actor: Actor(Actor, 15226e81c8a57a163551893501000000)\n",
      "2025-05-14 00:22:04,239\tINFO compiled_dag_node.py:2161 -- Killing actor: Actor(Actor, f306fc11374a6bc061c2e14c01000000)\n",
      "2025-05-14 00:22:04,239\tINFO compiled_dag_node.py:2161 -- Killing actor: Actor(Actor, 90658065c760f1033108ebe001000000)\n",
      "2025-05-14 00:22:04,239\tINFO compiled_dag_node.py:2161 -- Killing actor: Actor(Actor, 286c070782b744004b3b8b4901000000)\n",
      "2025-05-14 00:22:04,239\tINFO compiled_dag_node.py:2161 -- Killing actor: Actor(Actor, 61b46bc4eb2ac0f9de809a7b01000000)\n",
      "2025-05-14 00:22:04,240\tINFO compiled_dag_node.py:2161 -- Killing actor: Actor(Actor, e560e2042494874e334ef3d501000000)\n",
      "2025-05-14 00:22:04,240\tINFO compiled_dag_node.py:2161 -- Killing actor: Actor(Actor, 5bd32ced761b2518e7ce86f101000000)\n",
      "2025-05-14 00:22:04,240\tINFO compiled_dag_node.py:2161 -- Killing actor: Actor(Actor, d4c2f43913ad77e6e7cef68001000000)\n",
      "2025-05-14 00:22:04,240\tINFO compiled_dag_node.py:2203 -- Teardown complete\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!python -m coll-sched-0512.ddp --name ddp_bench"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-05-14 00:20:27,500\tINFO worker.py:1879 -- Started a local Ray instance. View the dashboard at \u001b[1m\u001b[32mhttp://127.0.0.1:8265 \u001b[39m\u001b[22m\n",
      "2025-05-14 00:20:32,060\tINFO torch_tensor_nccl_channel.py:772 -- Creating NCCL group 48aeed99-14f3-450e-8020-5035338e3042 on actors: [Actor(Actor, 4da5a70b8086c7d41ba5546d01000000), Actor(Actor, 63cd6e1285c0533b95a23e9601000000)]\n",
      "2025-05-14 00:20:35,435\tINFO torch_tensor_nccl_channel.py:797 -- NCCL group initialized.\n",
      "2025-05-14 00:20:35,587\tINFO dag_node_operation.py:628 -- Writing compiled graph schedule visualization to compiled_graph_schedule.png\n",
      "2025-05-14 00:20:35,909\tINFO compiled_dag_node.py:2173 -- Tearing down compiled DAG\n",
      "2025-05-14 00:20:35,913\tINFO compiled_dag_node.py:2178 -- Cancelling compiled worker on actor: Actor(Actor, 4da5a70b8086c7d41ba5546d01000000)\n",
      "2025-05-14 00:20:35,913\tINFO compiled_dag_node.py:2178 -- Cancelling compiled worker on actor: Actor(Actor, 63cd6e1285c0533b95a23e9601000000)\n",
      "\u001b[36m(Actor pid=3682974)\u001b[0m Destructing NCCL group on actor: Actor(Actor, 63cd6e1285c0533b95a23e9601000000)\n",
      "\u001b[36m(Actor pid=3683012)\u001b[0m Destructing NCCL group on actor: Actor(Actor, 4da5a70b8086c7d41ba5546d01000000)\n",
      "2025-05-14 00:20:36,916\tINFO compiled_dag_node.py:2200 -- Waiting for worker tasks to exit\n",
      "2025-05-14 00:20:36,917\tINFO compiled_dag_node.py:2161 -- Killing actor: Actor(Actor, 4da5a70b8086c7d41ba5546d01000000)\n",
      "2025-05-14 00:20:36,917\tINFO compiled_dag_node.py:2161 -- Killing actor: Actor(Actor, 63cd6e1285c0533b95a23e9601000000)\n",
      "2025-05-14 00:20:36,917\tINFO compiled_dag_node.py:2203 -- Teardown complete\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!python -m coll-sched-0512.ddp --name ddp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "!python -m pytest -v -s test_coll_sched.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m============================= test session starts ==============================\u001b[0m\n",
      "platform linux -- Python 3.9.22, pytest-8.3.5, pluggy-1.5.0 -- /home/wxdeng/miniconda3/envs/coll-sched-0512-py39/bin/python\n",
      "cachedir: .pytest_cache\n",
      "rootdir: /home/wxdeng/stbr-min\n",
      "configfile: pytest.ini\n",
      "plugins: anyio-4.9.0\n",
      "collected 2 items                                                              \u001b[0m\u001b[1m\n",
      "\n",
      "test_execution_schedule_gpu.py::test_simulate_pp_2workers_2batches_1f1b[True-ray_start_regular0] 2025-05-13 23:11:47,620\tINFO worker.py:1879 -- Started a local Ray instance. View the dashboard at \u001b[1m\u001b[32m127.0.0.1:8265 \u001b[39m\u001b[22m\n",
      "\u001b[33m(raylet)\u001b[0m [2025-05-13 23:11:47,594 E 577426 577455] (raylet) file_system_monitor.cc:116: /tmp/ray/session_2025-05-13_23-11-45_749921_576668 is over 95% full, available space: 81.9487 GB; capacity: 1830.63 GB. Object creation will fail if spilling is required.\n",
      "2025-05-13 23:11:50,232\tINFO torch_tensor_nccl_channel.py:772 -- Creating NCCL group a4448c99-07b8-4d40-9a93-df94ce5e23fa on actors: [Actor(Worker, 2c1f958b2bf306b2549f6dd201000000), Actor(Worker, 13b3a6147173f81441271c1501000000)]\n",
      "2025-05-13 23:11:51,419\tINFO torch_tensor_nccl_channel.py:797 -- NCCL group initialized.\n",
      "2025-05-13 23:11:51,603\tINFO dag_node_operation.py:628 -- Writing compiled graph schedule visualization to compiled_graph_schedule.png\n",
      "\u001b[32mPASSED\u001b[0m2025-05-13 23:11:52,174\tINFO compiled_dag_node.py:2173 -- Tearing down compiled DAG\n",
      "2025-05-13 23:11:52,174\tINFO compiled_dag_node.py:2178 -- Cancelling compiled worker on actor: Actor(Worker, 2c1f958b2bf306b2549f6dd201000000)\n",
      "2025-05-13 23:11:52,174\tINFO compiled_dag_node.py:2178 -- Cancelling compiled worker on actor: Actor(Worker, 13b3a6147173f81441271c1501000000)\n",
      "\u001b[36m(Worker pid=577516)\u001b[0m Destructing NCCL group on actor: Actor(Worker, 2c1f958b2bf306b2549f6dd201000000)\n",
      "\u001b[36m(Worker pid=577704)\u001b[0m Destructing NCCL group on actor: Actor(Worker, 13b3a6147173f81441271c1501000000)\n",
      "2025-05-13 23:11:52,694\tINFO compiled_dag_node.py:2200 -- Waiting for worker tasks to exit\n",
      "2025-05-13 23:11:52,694\tINFO compiled_dag_node.py:2161 -- Killing actor: Actor(Worker, 2c1f958b2bf306b2549f6dd201000000)\n",
      "2025-05-13 23:11:52,695\tINFO compiled_dag_node.py:2161 -- Killing actor: Actor(Worker, 13b3a6147173f81441271c1501000000)\n",
      "2025-05-13 23:11:52,695\tINFO compiled_dag_node.py:2203 -- Teardown complete\n",
      "\n",
      "test_execution_schedule_gpu.py::test_simulate_pp_2workers_2batches_1f1b[False-ray_start_regular0] 2025-05-13 23:11:56,514\tINFO worker.py:1879 -- Started a local Ray instance. View the dashboard at \u001b[1m\u001b[32m127.0.0.1:8265 \u001b[39m\u001b[22m\n",
      "\u001b[33m(raylet)\u001b[0m [2025-05-13 23:11:56,483 E 578671 578699] (raylet) file_system_monitor.cc:116: /tmp/ray/session_2025-05-13_23-11-54_616268_576668 is over 95% full, available space: 81.9479 GB; capacity: 1830.63 GB. Object creation will fail if spilling is required.\n",
      "2025-05-13 23:11:59,122\tINFO torch_tensor_nccl_channel.py:772 -- Creating NCCL group 5066b28b-bc32-482f-b3a6-65ee114ea51e on actors: [Actor(Worker, 56eec98d5f38291a862c159501000000), Actor(Worker, 47a18e2e93434008f07b175901000000)]\n",
      "2025-05-13 23:12:00,095\tINFO torch_tensor_nccl_channel.py:797 -- NCCL group initialized.\n",
      "2025-05-13 23:12:00,255\tINFO dag_node_operation.py:628 -- Writing compiled graph schedule visualization to compiled_graph_schedule.png\n",
      "\u001b[32mPASSED\u001b[0m2025-05-13 23:12:00,571\tINFO compiled_dag_node.py:2173 -- Tearing down compiled DAG\n",
      "2025-05-13 23:12:00,571\tINFO compiled_dag_node.py:2178 -- Cancelling compiled worker on actor: Actor(Worker, 47a18e2e93434008f07b175901000000)\n",
      "2025-05-13 23:12:00,571\tINFO compiled_dag_node.py:2178 -- Cancelling compiled worker on actor: Actor(Worker, 56eec98d5f38291a862c159501000000)\n",
      "\u001b[36m(Worker pid=578760)\u001b[0m Destructing NCCL group on actor: Actor(Worker, 56eec98d5f38291a862c159501000000)\n",
      "\u001b[36m(Worker pid=578926)\u001b[0m Destructing NCCL group on actor: Actor(Worker, 47a18e2e93434008f07b175901000000)\n",
      "2025-05-13 23:12:01,093\tINFO compiled_dag_node.py:2200 -- Waiting for worker tasks to exit\n",
      "2025-05-13 23:12:01,094\tINFO compiled_dag_node.py:2161 -- Killing actor: Actor(Worker, 47a18e2e93434008f07b175901000000)\n",
      "2025-05-13 23:12:01,094\tINFO compiled_dag_node.py:2161 -- Killing actor: Actor(Worker, 56eec98d5f38291a862c159501000000)\n",
      "2025-05-13 23:12:01,094\tINFO compiled_dag_node.py:2203 -- Teardown complete\n",
      "\n",
      "\n",
      "\u001b[32m============================== \u001b[32m\u001b[1m2 passed\u001b[0m\u001b[32m in 18.53s\u001b[0m\u001b[32m ==============================\u001b[0m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!python -m pytest -v -s test_execution_schedule_gpu.py::test_simulate_pp_2workers_2batches_1f1b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m============================= test session starts ==============================\u001b[0m\n",
      "platform linux -- Python 3.9.22, pytest-8.3.5, pluggy-1.5.0 -- /home/wxdeng/miniconda3/envs/coll-sched-0512-py39/bin/python\n",
      "cachedir: .pytest_cache\n",
      "rootdir: /home/wxdeng/stbr-min\n",
      "configfile: pytest.ini\n",
      "plugins: anyio-4.9.0\n",
      "collected 8 items                                                              \u001b[0m\u001b[1m\n",
      "\n",
      "test_execution_schedule_gpu.py::test_simulate_pp_2workers_2batches_1f1b[True-ray_start_regular0] 2025-05-13 18:33:40,709\tINFO worker.py:1879 -- Started a local Ray instance. View the dashboard at \u001b[1m\u001b[32m127.0.0.1:8265 \u001b[39m\u001b[22m\n",
      "2025-05-13 18:33:43,304\tINFO torch_tensor_nccl_channel.py:772 -- Creating NCCL group df95a304-2789-419b-80ef-1d702fc21866 on actors: [Actor(Worker, 1fb757fb44ca889bac4dc60701000000), Actor(Worker, 645fac4515ae7e3bb91b745501000000)]\n",
      "2025-05-13 18:33:44,488\tINFO torch_tensor_nccl_channel.py:797 -- NCCL group initialized.\n",
      "2025-05-13 18:33:44,667\tINFO dag_node_operation.py:628 -- Writing compiled graph schedule visualization to compiled_graph_schedule.png\n",
      "\u001b[31mFAILED\u001b[0m2025-05-13 18:33:44,925\tINFO compiled_dag_node.py:2173 -- Tearing down compiled DAG\n",
      "2025-05-13 18:33:44,929\tINFO compiled_dag_node.py:2178 -- Cancelling compiled worker on actor: Actor(Worker, 1fb757fb44ca889bac4dc60701000000)\n",
      "2025-05-13 18:33:44,929\tINFO compiled_dag_node.py:2178 -- Cancelling compiled worker on actor: Actor(Worker, 645fac4515ae7e3bb91b745501000000)\n",
      "\u001b[36m(Worker pid=414123)\u001b[0m Destructing NCCL group on actor: Actor(Worker, 1fb757fb44ca889bac4dc60701000000)\n",
      "\u001b[36m(Worker pid=414282)\u001b[0m Destructing NCCL group on actor: Actor(Worker, 645fac4515ae7e3bb91b745501000000)\n",
      "2025-05-13 18:33:45,496\tINFO compiled_dag_node.py:2200 -- Waiting for worker tasks to exit\n",
      "2025-05-13 18:33:45,497\tINFO compiled_dag_node.py:2161 -- Killing actor: Actor(Worker, 1fb757fb44ca889bac4dc60701000000)\n",
      "2025-05-13 18:33:45,497\tINFO compiled_dag_node.py:2161 -- Killing actor: Actor(Worker, 645fac4515ae7e3bb91b745501000000)\n",
      "2025-05-13 18:33:45,497\tINFO compiled_dag_node.py:2203 -- Teardown complete\n",
      "\n",
      "test_execution_schedule_gpu.py::test_simulate_pp_2workers_2batches_1f1b[False-ray_start_regular0] 2025-05-13 18:33:50,052\tINFO worker.py:1879 -- Started a local Ray instance. View the dashboard at \u001b[1m\u001b[32m127.0.0.1:8265 \u001b[39m\u001b[22m\n",
      "2025-05-13 18:33:52,675\tINFO torch_tensor_nccl_channel.py:772 -- Creating NCCL group 42d45307-0203-4526-b791-b838b7d5f383 on actors: [Actor(Worker, 16b8511b7cc43e068e69e0e101000000), Actor(Worker, 8ec4370df7450b1936e4c44001000000)]\n",
      "2025-05-13 18:33:53,862\tINFO torch_tensor_nccl_channel.py:797 -- NCCL group initialized.\n",
      "2025-05-13 18:33:54,033\tINFO dag_node_operation.py:628 -- Writing compiled graph schedule visualization to compiled_graph_schedule.png\n",
      "\u001b[31mFAILED\u001b[0m2025-05-13 18:33:54,289\tINFO compiled_dag_node.py:2173 -- Tearing down compiled DAG\n",
      "2025-05-13 18:33:54,293\tINFO compiled_dag_node.py:2178 -- Cancelling compiled worker on actor: Actor(Worker, 16b8511b7cc43e068e69e0e101000000)\n",
      "2025-05-13 18:33:54,293\tINFO compiled_dag_node.py:2178 -- Cancelling compiled worker on actor: Actor(Worker, 8ec4370df7450b1936e4c44001000000)\n",
      "\u001b[36m(Worker pid=415359)\u001b[0m Destructing NCCL group on actor: Actor(Worker, 8ec4370df7450b1936e4c44001000000)\n",
      "\u001b[36m(Worker pid=415496)\u001b[0m Destructing NCCL group on actor: Actor(Worker, 16b8511b7cc43e068e69e0e101000000)\n",
      "2025-05-13 18:33:54,869\tINFO compiled_dag_node.py:2200 -- Waiting for worker tasks to exit\n",
      "2025-05-13 18:33:54,870\tINFO compiled_dag_node.py:2161 -- Killing actor: Actor(Worker, 16b8511b7cc43e068e69e0e101000000)\n",
      "2025-05-13 18:33:54,870\tINFO compiled_dag_node.py:2161 -- Killing actor: Actor(Worker, 8ec4370df7450b1936e4c44001000000)\n",
      "2025-05-13 18:33:54,870\tINFO compiled_dag_node.py:2203 -- Teardown complete\n",
      "\n",
      "test_execution_schedule_gpu.py::test_simulate_pp_4workers_8batches_1f1b[ray_start_regular0] 2025-05-13 18:33:58,454\tINFO worker.py:1879 -- Started a local Ray instance. View the dashboard at \u001b[1m\u001b[32m127.0.0.1:8265 \u001b[39m\u001b[22m\n",
      "\u001b[33m(raylet)\u001b[0m [2025-05-13 18:33:58,427 E 416434 416462] (raylet) file_system_monitor.cc:116: /tmp/ray/session_2025-05-13_18-33-56_649331_413246 is over 95% full, available space: 82.0799 GB; capacity: 1830.63 GB. Object creation will fail if spilling is required.\n",
      "2025-05-13 18:34:01,606\tINFO torch_tensor_nccl_channel.py:772 -- Creating NCCL group 8340c810-8b57-4c7c-ba02-ab5cc1a7b0d9 on actors: [Actor(Worker, 82094e576d3030a64c3379dc01000000), Actor(Worker, aa43d5b6e6916ac2dda40c8101000000), Actor(Worker, bd5233fe1d56954737e41bf601000000), Actor(Worker, 357474c514730ae97f1e9a4701000000)]\n",
      "\u001b[31mFAILED\u001b[0m\n",
      "test_execution_schedule_gpu.py::test_three_actors_with_nccl_1[ray_start_regular0] 2025-05-13 18:34:05,546\tINFO worker.py:1879 -- Started a local Ray instance. View the dashboard at \u001b[1m\u001b[32m127.0.0.1:8265 \u001b[39m\u001b[22m\n",
      "\u001b[33m(raylet)\u001b[0m [2025-05-13 18:34:05,524 E 417773 417801] (raylet) file_system_monitor.cc:116: /tmp/ray/session_2025-05-13_18-34-03_746250_413246 is over 95% full, available space: 82.0791 GB; capacity: 1830.63 GB. Object creation will fail if spilling is required.\n",
      "2025-05-13 18:34:08,522\tINFO torch_tensor_nccl_channel.py:772 -- Creating NCCL group 3362299a-99ab-4853-95ed-55d158069b13 on actors: [Actor(Worker, c297d2cc111cd39c0543cf4b01000000), Actor(Worker, 8bb1f07097880644884e440601000000), Actor(Worker, d8cd084a8b9a1cab27e98ef001000000)]\n",
      "\u001b[31mFAILED\u001b[0m\n",
      "test_execution_schedule_gpu.py::test_three_actors_with_nccl_2[True-ray_start_regular0] 2025-05-13 18:34:12,433\tINFO worker.py:1879 -- Started a local Ray instance. View the dashboard at \u001b[1m\u001b[32m127.0.0.1:8265 \u001b[39m\u001b[22m\n",
      "\u001b[33m(raylet)\u001b[0m [2025-05-13 18:34:12,402 E 418984 419012] (raylet) file_system_monitor.cc:116: /tmp/ray/session_2025-05-13_18-34-10_639927_413246 is over 95% full, available space: 82.0783 GB; capacity: 1830.63 GB. Object creation will fail if spilling is required.\n",
      "2025-05-13 18:34:15,262\tINFO torch_tensor_nccl_channel.py:772 -- Creating NCCL group 18ecfb94-baf2-4c2e-b471-9c8e3b87251e on actors: [Actor(Worker, 2ae7edf2979813840f8f2cf201000000), Actor(Worker, e76fd021efb1266bfc4680d301000000), Actor(Worker, 9c97d33f0607880cba102a8e01000000)]\n",
      "\u001b[31mFAILED\u001b[0m\n",
      "test_execution_schedule_gpu.py::test_three_actors_with_nccl_2[False-ray_start_regular0] 2025-05-13 18:34:19,328\tINFO worker.py:1879 -- Started a local Ray instance. View the dashboard at \u001b[1m\u001b[32m127.0.0.1:8265 \u001b[39m\u001b[22m\n",
      "\u001b[33m(raylet)\u001b[0m [2025-05-13 18:34:19,300 E 420231 420259] (raylet) file_system_monitor.cc:116: /tmp/ray/session_2025-05-13_18-34-17_525084_413246 is over 95% full, available space: 82.0775 GB; capacity: 1830.63 GB. Object creation will fail if spilling is required.\n",
      "2025-05-13 18:34:22,270\tINFO torch_tensor_nccl_channel.py:772 -- Creating NCCL group 3f527cd1-6aba-4e9a-8332-8b6b96bde83c on actors: [Actor(Worker, 7b76b108b090ec43c422bbbf01000000), Actor(Worker, a12c19ec766a86cc0c44140901000000), Actor(Worker, abdc44fa5b00ebe71bc33f7f01000000)]\n",
      "\u001b[31mFAILED\u001b[0m\n",
      "test_execution_schedule_gpu.py::test_overlap_gpu_communication[True-ray_start_regular0] 2025-05-13 18:34:26,015\tINFO worker.py:1879 -- Started a local Ray instance. View the dashboard at \u001b[1m\u001b[32m127.0.0.1:8265 \u001b[39m\u001b[22m\n",
      "\u001b[33m(raylet)\u001b[0m [2025-05-13 18:34:25,987 E 421492 421520] (raylet) file_system_monitor.cc:116: /tmp/ray/session_2025-05-13_18-34-24_222352_413246 is over 95% full, available space: 82.0768 GB; capacity: 1830.63 GB. Object creation will fail if spilling is required.\n",
      "2025-05-13 18:34:28,992\tINFO torch_tensor_nccl_channel.py:772 -- Creating NCCL group b2991ccd-625d-4dcb-93f8-ff0a05fe76dc on actors: [Actor(Worker, f6f411bac839ba6621379d9901000000), Actor(Worker, 2cb5d03b6e4e6d777b196aa801000000), Actor(Worker, 3e3d752bed46aacd9ae35ed101000000)]\n",
      "\u001b[31mFAILED\u001b[0m\n",
      "test_execution_schedule_gpu.py::test_overlap_gpu_communication[False-ray_start_regular0] 2025-05-13 18:34:32,910\tINFO worker.py:1879 -- Started a local Ray instance. View the dashboard at \u001b[1m\u001b[32m127.0.0.1:8265 \u001b[39m\u001b[22m\n",
      "\u001b[33m(raylet)\u001b[0m [2025-05-13 18:34:32,877 E 422704 422732] (raylet) file_system_monitor.cc:116: /tmp/ray/session_2025-05-13_18-34-31_106913_413246 is over 95% full, available space: 82.076 GB; capacity: 1830.63 GB. Object creation will fail if spilling is required.\n",
      "2025-05-13 18:34:35,963\tINFO torch_tensor_nccl_channel.py:772 -- Creating NCCL group 66ece479-2240-4d36-8ce9-ba850be03648 on actors: [Actor(Worker, 6b435e5e859d90f5c9882a9b01000000), Actor(Worker, a7ed62182b0670a406fb65fa01000000), Actor(Worker, 7dd980fd8c18871c6ad9e4ac01000000)]\n",
      "\u001b[31mFAILED\u001b[0m\n",
      "\n",
      "=================================== FAILURES ===================================\n",
      "\u001b[31m\u001b[1m_______ test_simulate_pp_2workers_2batches_1f1b[True-ray_start_regular0] _______\u001b[0m\n",
      "\n",
      "ray_start_regular = RayContext(dashboard_url='127.0.0.1:8265', python_version='3.9.22', ray_version='3.0.0.dev0', ray_commit='{{RAY_COMMIT_SHA}}')\n",
      "single_fetch = True\n",
      "monkeypatch = <_pytest.monkeypatch.MonkeyPatch object at 0x74df99fd48b0>\n",
      "\n",
      "    \u001b[0m\u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mray_start_regular\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [{\u001b[33m\"\u001b[39;49;00m\u001b[33mnum_gpus\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m: \u001b[94m2\u001b[39;49;00m}], indirect=\u001b[94mTrue\u001b[39;49;00m)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33msingle_fetch\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94mTrue\u001b[39;49;00m, \u001b[94mFalse\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mtest_simulate_pp_2workers_2batches_1f1b\u001b[39;49;00m(\u001b[90m\u001b[39;49;00m\n",
      "        ray_start_regular, single_fetch, monkeypatch\u001b[90m\u001b[39;49;00m\n",
      "    ):\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m    \u001b[39;49;00m\u001b[33m\"\"\"\u001b[39;49;00m\n",
      "    \u001b[33m    This test simulates a simple 1F1B pipeline parallelism for training with\u001b[39;49;00m\n",
      "    \u001b[33m    2 workers and 2 batches.\u001b[39;49;00m\n",
      "    \u001b[33m\u001b[39;49;00m\n",
      "    \u001b[33m    w1: fwd_b1  fwd_b2          bwd_b1          bwd_b2\u001b[39;49;00m\n",
      "    \u001b[33m    w2:         fwd_b1  bwd_b1  fwd_b2  bwd_b2\u001b[39;49;00m\n",
      "    \u001b[33m\u001b[39;49;00m\n",
      "    \u001b[33m    The communication between workers is done using NCCL. The communication\u001b[39;49;00m\n",
      "    \u001b[33m    within the worker actor is done using IntraProcessChannel.\u001b[39;49;00m\n",
      "    \u001b[33m    \"\"\"\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[94mif\u001b[39;49;00m \u001b[95mnot\u001b[39;49;00m USE_GPU:\u001b[90m\u001b[39;49;00m\n",
      "            pytest.skip(\u001b[33m\"\u001b[39;49;00m\u001b[33mNCCL tests require GPUs\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "        w1 = Worker.remote()\u001b[90m\u001b[39;49;00m\n",
      "        w2 = Worker.remote()\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "        \u001b[94mwith\u001b[39;49;00m InputNode() \u001b[94mas\u001b[39;49;00m inp:\u001b[90m\u001b[39;49;00m\n",
      "            w1_input = w1.read_input.bind(inp)\u001b[90m\u001b[39;49;00m\n",
      "            batch_1 = w1.fwd.bind(w1_input)\u001b[90m\u001b[39;49;00m\n",
      "            batch_1.with_tensor_transport(transport=\u001b[33m\"\u001b[39;49;00m\u001b[33mnccl\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\u001b[90m\u001b[39;49;00m\n",
      "            batch_2 = w1.fwd.bind(w1_input)\u001b[90m\u001b[39;49;00m\n",
      "            batch_2.with_tensor_transport(transport=\u001b[33m\"\u001b[39;49;00m\u001b[33mnccl\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\u001b[90m\u001b[39;49;00m\n",
      "            batch_1 = w2.fwd.bind(batch_1)\u001b[90m\u001b[39;49;00m\n",
      "            batch_1 = w2.bwd.bind(batch_1)\u001b[90m\u001b[39;49;00m\n",
      "            batch_1.with_tensor_transport(transport=\u001b[33m\"\u001b[39;49;00m\u001b[33mnccl\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\u001b[90m\u001b[39;49;00m\n",
      "            batch_2 = w2.fwd.bind(batch_2)\u001b[90m\u001b[39;49;00m\n",
      "            batch_1 = w1.bwd.bind(batch_1)\u001b[90m\u001b[39;49;00m\n",
      "            batch_2 = w2.bwd.bind(batch_2)\u001b[90m\u001b[39;49;00m\n",
      "            batch_2.with_tensor_transport(transport=\u001b[33m\"\u001b[39;49;00m\u001b[33mnccl\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\u001b[90m\u001b[39;49;00m\n",
      "            batch_2 = w1.bwd.bind(batch_2)\u001b[90m\u001b[39;49;00m\n",
      "            dag = MultiOutputNode([batch_1, batch_2])\u001b[90m\u001b[39;49;00m\n",
      "        compiled_dag = dag.experimental_compile()\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "        w1_expected_schedule = [\u001b[90m\u001b[39;49;00m\n",
      "            (\u001b[94m0\u001b[39;49;00m, _DAGNodeOperationType.READ),\u001b[90m\u001b[39;49;00m\n",
      "            (\u001b[94m0\u001b[39;49;00m, _DAGNodeOperationType.COMPUTE),\u001b[90m\u001b[39;49;00m\n",
      "            (\u001b[94m0\u001b[39;49;00m, _DAGNodeOperationType.WRITE),\u001b[90m\u001b[39;49;00m\n",
      "            (\u001b[94m1\u001b[39;49;00m, _DAGNodeOperationType.READ),\u001b[90m\u001b[39;49;00m\n",
      "            (\u001b[94m1\u001b[39;49;00m, _DAGNodeOperationType.COMPUTE),\u001b[90m\u001b[39;49;00m\n",
      "            (\u001b[94m1\u001b[39;49;00m, _DAGNodeOperationType.WRITE),\u001b[90m\u001b[39;49;00m\n",
      "            (\u001b[94m2\u001b[39;49;00m, _DAGNodeOperationType.READ),\u001b[90m\u001b[39;49;00m\n",
      "            (\u001b[94m2\u001b[39;49;00m, _DAGNodeOperationType.COMPUTE),\u001b[90m\u001b[39;49;00m\n",
      "            (\u001b[94m3\u001b[39;49;00m, _DAGNodeOperationType.READ),\u001b[90m\u001b[39;49;00m\n",
      "            (\u001b[94m2\u001b[39;49;00m, _DAGNodeOperationType.WRITE),\u001b[90m\u001b[39;49;00m\n",
      "            (\u001b[94m3\u001b[39;49;00m, _DAGNodeOperationType.COMPUTE),\u001b[90m\u001b[39;49;00m\n",
      "            (\u001b[94m3\u001b[39;49;00m, _DAGNodeOperationType.WRITE),\u001b[90m\u001b[39;49;00m\n",
      "            (\u001b[94m4\u001b[39;49;00m, _DAGNodeOperationType.READ),\u001b[90m\u001b[39;49;00m\n",
      "            (\u001b[94m4\u001b[39;49;00m, _DAGNodeOperationType.COMPUTE),\u001b[90m\u001b[39;49;00m\n",
      "            (\u001b[94m4\u001b[39;49;00m, _DAGNodeOperationType.WRITE),\u001b[90m\u001b[39;49;00m\n",
      "        ]\u001b[90m\u001b[39;49;00m\n",
      "        w2_expected_schedule = [\u001b[90m\u001b[39;49;00m\n",
      "            (\u001b[94m0\u001b[39;49;00m, _DAGNodeOperationType.READ),\u001b[90m\u001b[39;49;00m\n",
      "            (\u001b[94m0\u001b[39;49;00m, _DAGNodeOperationType.COMPUTE),\u001b[90m\u001b[39;49;00m\n",
      "            (\u001b[94m0\u001b[39;49;00m, _DAGNodeOperationType.WRITE),\u001b[90m\u001b[39;49;00m\n",
      "            (\u001b[94m1\u001b[39;49;00m, _DAGNodeOperationType.READ),\u001b[90m\u001b[39;49;00m\n",
      "            (\u001b[94m1\u001b[39;49;00m, _DAGNodeOperationType.COMPUTE),\u001b[90m\u001b[39;49;00m\n",
      "            (\u001b[94m1\u001b[39;49;00m, _DAGNodeOperationType.WRITE),\u001b[90m\u001b[39;49;00m\n",
      "            (\u001b[94m2\u001b[39;49;00m, _DAGNodeOperationType.READ),\u001b[90m\u001b[39;49;00m\n",
      "            (\u001b[94m2\u001b[39;49;00m, _DAGNodeOperationType.COMPUTE),\u001b[90m\u001b[39;49;00m\n",
      "            (\u001b[94m2\u001b[39;49;00m, _DAGNodeOperationType.WRITE),\u001b[90m\u001b[39;49;00m\n",
      "            (\u001b[94m3\u001b[39;49;00m, _DAGNodeOperationType.READ),\u001b[90m\u001b[39;49;00m\n",
      "            (\u001b[94m3\u001b[39;49;00m, _DAGNodeOperationType.COMPUTE),\u001b[90m\u001b[39;49;00m\n",
      "            (\u001b[94m3\u001b[39;49;00m, _DAGNodeOperationType.WRITE),\u001b[90m\u001b[39;49;00m\n",
      "        ]\u001b[90m\u001b[39;49;00m\n",
      "        w1_schedule = compiled_dag.actor_to_execution_schedule[w1]\u001b[90m\u001b[39;49;00m\n",
      "        w2_schedule = compiled_dag.actor_to_execution_schedule[w2]\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "        \u001b[94mfor\u001b[39;49;00m schedule, expected_schedule \u001b[95min\u001b[39;49;00m \u001b[96mzip\u001b[39;49;00m(\u001b[90m\u001b[39;49;00m\n",
      "            [w1_schedule, w2_schedule], [w1_expected_schedule, w2_expected_schedule]\u001b[90m\u001b[39;49;00m\n",
      "        ):\u001b[90m\u001b[39;49;00m\n",
      "            \u001b[94massert\u001b[39;49;00m \u001b[96mlen\u001b[39;49;00m(schedule) == \u001b[96mlen\u001b[39;49;00m(expected_schedule)\u001b[90m\u001b[39;49;00m\n",
      "            \u001b[94mfor\u001b[39;49;00m i, operation \u001b[95min\u001b[39;49;00m \u001b[96menumerate\u001b[39;49;00m(schedule):\u001b[90m\u001b[39;49;00m\n",
      ">               \u001b[94massert\u001b[39;49;00m operation.exec_task_idx == expected_schedule[i][\u001b[94m0\u001b[39;49;00m]\u001b[90m\u001b[39;49;00m\n",
      "\u001b[1m\u001b[31mE               assert 3 == 2\u001b[0m\n",
      "\u001b[1m\u001b[31mE                +  where 3 = _DAGNodeOperation(exec_task_idx: 3, type: _DAGNodeOperationType.READ, method_name: bwd).exec_task_idx\u001b[0m\n",
      "\n",
      "\u001b[1m\u001b[31mtest_execution_schedule_gpu.py\u001b[0m:187: AssertionError\n",
      "\u001b[31m\u001b[1m______ test_simulate_pp_2workers_2batches_1f1b[False-ray_start_regular0] _______\u001b[0m\n",
      "\n",
      "ray_start_regular = RayContext(dashboard_url='127.0.0.1:8265', python_version='3.9.22', ray_version='3.0.0.dev0', ray_commit='{{RAY_COMMIT_SHA}}')\n",
      "single_fetch = False\n",
      "monkeypatch = <_pytest.monkeypatch.MonkeyPatch object at 0x74df5cd6a670>\n",
      "\n",
      "    \u001b[0m\u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mray_start_regular\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [{\u001b[33m\"\u001b[39;49;00m\u001b[33mnum_gpus\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m: \u001b[94m2\u001b[39;49;00m}], indirect=\u001b[94mTrue\u001b[39;49;00m)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33msingle_fetch\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94mTrue\u001b[39;49;00m, \u001b[94mFalse\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mtest_simulate_pp_2workers_2batches_1f1b\u001b[39;49;00m(\u001b[90m\u001b[39;49;00m\n",
      "        ray_start_regular, single_fetch, monkeypatch\u001b[90m\u001b[39;49;00m\n",
      "    ):\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m    \u001b[39;49;00m\u001b[33m\"\"\"\u001b[39;49;00m\n",
      "    \u001b[33m    This test simulates a simple 1F1B pipeline parallelism for training with\u001b[39;49;00m\n",
      "    \u001b[33m    2 workers and 2 batches.\u001b[39;49;00m\n",
      "    \u001b[33m\u001b[39;49;00m\n",
      "    \u001b[33m    w1: fwd_b1  fwd_b2          bwd_b1          bwd_b2\u001b[39;49;00m\n",
      "    \u001b[33m    w2:         fwd_b1  bwd_b1  fwd_b2  bwd_b2\u001b[39;49;00m\n",
      "    \u001b[33m\u001b[39;49;00m\n",
      "    \u001b[33m    The communication between workers is done using NCCL. The communication\u001b[39;49;00m\n",
      "    \u001b[33m    within the worker actor is done using IntraProcessChannel.\u001b[39;49;00m\n",
      "    \u001b[33m    \"\"\"\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[94mif\u001b[39;49;00m \u001b[95mnot\u001b[39;49;00m USE_GPU:\u001b[90m\u001b[39;49;00m\n",
      "            pytest.skip(\u001b[33m\"\u001b[39;49;00m\u001b[33mNCCL tests require GPUs\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "        w1 = Worker.remote()\u001b[90m\u001b[39;49;00m\n",
      "        w2 = Worker.remote()\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "        \u001b[94mwith\u001b[39;49;00m InputNode() \u001b[94mas\u001b[39;49;00m inp:\u001b[90m\u001b[39;49;00m\n",
      "            w1_input = w1.read_input.bind(inp)\u001b[90m\u001b[39;49;00m\n",
      "            batch_1 = w1.fwd.bind(w1_input)\u001b[90m\u001b[39;49;00m\n",
      "            batch_1.with_tensor_transport(transport=\u001b[33m\"\u001b[39;49;00m\u001b[33mnccl\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\u001b[90m\u001b[39;49;00m\n",
      "            batch_2 = w1.fwd.bind(w1_input)\u001b[90m\u001b[39;49;00m\n",
      "            batch_2.with_tensor_transport(transport=\u001b[33m\"\u001b[39;49;00m\u001b[33mnccl\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\u001b[90m\u001b[39;49;00m\n",
      "            batch_1 = w2.fwd.bind(batch_1)\u001b[90m\u001b[39;49;00m\n",
      "            batch_1 = w2.bwd.bind(batch_1)\u001b[90m\u001b[39;49;00m\n",
      "            batch_1.with_tensor_transport(transport=\u001b[33m\"\u001b[39;49;00m\u001b[33mnccl\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\u001b[90m\u001b[39;49;00m\n",
      "            batch_2 = w2.fwd.bind(batch_2)\u001b[90m\u001b[39;49;00m\n",
      "            batch_1 = w1.bwd.bind(batch_1)\u001b[90m\u001b[39;49;00m\n",
      "            batch_2 = w2.bwd.bind(batch_2)\u001b[90m\u001b[39;49;00m\n",
      "            batch_2.with_tensor_transport(transport=\u001b[33m\"\u001b[39;49;00m\u001b[33mnccl\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\u001b[90m\u001b[39;49;00m\n",
      "            batch_2 = w1.bwd.bind(batch_2)\u001b[90m\u001b[39;49;00m\n",
      "            dag = MultiOutputNode([batch_1, batch_2])\u001b[90m\u001b[39;49;00m\n",
      "        compiled_dag = dag.experimental_compile()\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "        w1_expected_schedule = [\u001b[90m\u001b[39;49;00m\n",
      "            (\u001b[94m0\u001b[39;49;00m, _DAGNodeOperationType.READ),\u001b[90m\u001b[39;49;00m\n",
      "            (\u001b[94m0\u001b[39;49;00m, _DAGNodeOperationType.COMPUTE),\u001b[90m\u001b[39;49;00m\n",
      "            (\u001b[94m0\u001b[39;49;00m, _DAGNodeOperationType.WRITE),\u001b[90m\u001b[39;49;00m\n",
      "            (\u001b[94m1\u001b[39;49;00m, _DAGNodeOperationType.READ),\u001b[90m\u001b[39;49;00m\n",
      "            (\u001b[94m1\u001b[39;49;00m, _DAGNodeOperationType.COMPUTE),\u001b[90m\u001b[39;49;00m\n",
      "            (\u001b[94m1\u001b[39;49;00m, _DAGNodeOperationType.WRITE),\u001b[90m\u001b[39;49;00m\n",
      "            (\u001b[94m2\u001b[39;49;00m, _DAGNodeOperationType.READ),\u001b[90m\u001b[39;49;00m\n",
      "            (\u001b[94m2\u001b[39;49;00m, _DAGNodeOperationType.COMPUTE),\u001b[90m\u001b[39;49;00m\n",
      "            (\u001b[94m3\u001b[39;49;00m, _DAGNodeOperationType.READ),\u001b[90m\u001b[39;49;00m\n",
      "            (\u001b[94m2\u001b[39;49;00m, _DAGNodeOperationType.WRITE),\u001b[90m\u001b[39;49;00m\n",
      "            (\u001b[94m3\u001b[39;49;00m, _DAGNodeOperationType.COMPUTE),\u001b[90m\u001b[39;49;00m\n",
      "            (\u001b[94m3\u001b[39;49;00m, _DAGNodeOperationType.WRITE),\u001b[90m\u001b[39;49;00m\n",
      "            (\u001b[94m4\u001b[39;49;00m, _DAGNodeOperationType.READ),\u001b[90m\u001b[39;49;00m\n",
      "            (\u001b[94m4\u001b[39;49;00m, _DAGNodeOperationType.COMPUTE),\u001b[90m\u001b[39;49;00m\n",
      "            (\u001b[94m4\u001b[39;49;00m, _DAGNodeOperationType.WRITE),\u001b[90m\u001b[39;49;00m\n",
      "        ]\u001b[90m\u001b[39;49;00m\n",
      "        w2_expected_schedule = [\u001b[90m\u001b[39;49;00m\n",
      "            (\u001b[94m0\u001b[39;49;00m, _DAGNodeOperationType.READ),\u001b[90m\u001b[39;49;00m\n",
      "            (\u001b[94m0\u001b[39;49;00m, _DAGNodeOperationType.COMPUTE),\u001b[90m\u001b[39;49;00m\n",
      "            (\u001b[94m0\u001b[39;49;00m, _DAGNodeOperationType.WRITE),\u001b[90m\u001b[39;49;00m\n",
      "            (\u001b[94m1\u001b[39;49;00m, _DAGNodeOperationType.READ),\u001b[90m\u001b[39;49;00m\n",
      "            (\u001b[94m1\u001b[39;49;00m, _DAGNodeOperationType.COMPUTE),\u001b[90m\u001b[39;49;00m\n",
      "            (\u001b[94m1\u001b[39;49;00m, _DAGNodeOperationType.WRITE),\u001b[90m\u001b[39;49;00m\n",
      "            (\u001b[94m2\u001b[39;49;00m, _DAGNodeOperationType.READ),\u001b[90m\u001b[39;49;00m\n",
      "            (\u001b[94m2\u001b[39;49;00m, _DAGNodeOperationType.COMPUTE),\u001b[90m\u001b[39;49;00m\n",
      "            (\u001b[94m2\u001b[39;49;00m, _DAGNodeOperationType.WRITE),\u001b[90m\u001b[39;49;00m\n",
      "            (\u001b[94m3\u001b[39;49;00m, _DAGNodeOperationType.READ),\u001b[90m\u001b[39;49;00m\n",
      "            (\u001b[94m3\u001b[39;49;00m, _DAGNodeOperationType.COMPUTE),\u001b[90m\u001b[39;49;00m\n",
      "            (\u001b[94m3\u001b[39;49;00m, _DAGNodeOperationType.WRITE),\u001b[90m\u001b[39;49;00m\n",
      "        ]\u001b[90m\u001b[39;49;00m\n",
      "        w1_schedule = compiled_dag.actor_to_execution_schedule[w1]\u001b[90m\u001b[39;49;00m\n",
      "        w2_schedule = compiled_dag.actor_to_execution_schedule[w2]\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "        \u001b[94mfor\u001b[39;49;00m schedule, expected_schedule \u001b[95min\u001b[39;49;00m \u001b[96mzip\u001b[39;49;00m(\u001b[90m\u001b[39;49;00m\n",
      "            [w1_schedule, w2_schedule], [w1_expected_schedule, w2_expected_schedule]\u001b[90m\u001b[39;49;00m\n",
      "        ):\u001b[90m\u001b[39;49;00m\n",
      "            \u001b[94massert\u001b[39;49;00m \u001b[96mlen\u001b[39;49;00m(schedule) == \u001b[96mlen\u001b[39;49;00m(expected_schedule)\u001b[90m\u001b[39;49;00m\n",
      "            \u001b[94mfor\u001b[39;49;00m i, operation \u001b[95min\u001b[39;49;00m \u001b[96menumerate\u001b[39;49;00m(schedule):\u001b[90m\u001b[39;49;00m\n",
      ">               \u001b[94massert\u001b[39;49;00m operation.exec_task_idx == expected_schedule[i][\u001b[94m0\u001b[39;49;00m]\u001b[90m\u001b[39;49;00m\n",
      "\u001b[1m\u001b[31mE               assert 3 == 2\u001b[0m\n",
      "\u001b[1m\u001b[31mE                +  where 3 = _DAGNodeOperation(exec_task_idx: 3, type: _DAGNodeOperationType.READ, method_name: bwd).exec_task_idx\u001b[0m\n",
      "\n",
      "\u001b[1m\u001b[31mtest_execution_schedule_gpu.py\u001b[0m:187: AssertionError\n",
      "\u001b[31m\u001b[1m_________ test_simulate_pp_4workers_8batches_1f1b[ray_start_regular0] __________\u001b[0m\n",
      "\n",
      "ray_start_regular = RayContext(dashboard_url='127.0.0.1:8265', python_version='3.9.22', ray_version='3.0.0.dev0', ray_commit='{{RAY_COMMIT_SHA}}')\n",
      "monkeypatch = <_pytest.monkeypatch.MonkeyPatch object at 0x74df5c4acc10>\n",
      "\n",
      "    \u001b[0m\u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mray_start_regular\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [{\u001b[33m\"\u001b[39;49;00m\u001b[33mnum_gpus\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m: \u001b[94m4\u001b[39;49;00m}], indirect=\u001b[94mTrue\u001b[39;49;00m)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mtest_simulate_pp_4workers_8batches_1f1b\u001b[39;49;00m(ray_start_regular, monkeypatch):\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m    \u001b[39;49;00m\u001b[33m\"\"\"\u001b[39;49;00m\n",
      "    \u001b[33m    This test simulates a 1F1B pipeline parallelism for training with\u001b[39;49;00m\n",
      "    \u001b[33m    4 workers and 8 batches.\u001b[39;49;00m\n",
      "    \u001b[33m    \"\"\"\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[94mif\u001b[39;49;00m \u001b[95mnot\u001b[39;49;00m USE_GPU:\u001b[90m\u001b[39;49;00m\n",
      "            pytest.skip(\u001b[33m\"\u001b[39;49;00m\u001b[33mNCCL tests require GPUs\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "        num_workers, num_microbatches, num_lead_microbatches = \u001b[94m4\u001b[39;49;00m, \u001b[94m8\u001b[39;49;00m, \u001b[94m4\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      ">       compiled_dag = generate_1f1b_dag(\u001b[90m\u001b[39;49;00m\n",
      "            num_workers, num_microbatches, num_lead_microbatches\u001b[90m\u001b[39;49;00m\n",
      "        )\u001b[90m\u001b[39;49;00m\n",
      "\n",
      "\u001b[1m\u001b[31mtest_execution_schedule_gpu.py\u001b[0m:216: \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\u001b[1m\u001b[31mtest_execution_schedule_gpu.py\u001b[0m:106: in generate_1f1b_dag\n",
      "    \u001b[0mcompiled_dag = dag.experimental_compile()\u001b[90m\u001b[39;49;00m\n",
      "\u001b[1m\u001b[31m../../dag_node.py\u001b[0m:340: in experimental_compile\n",
      "    \u001b[0m\u001b[94mreturn\u001b[39;49;00m build_compiled_dag_from_ray_dag(\u001b[90m\u001b[39;49;00m\n",
      "\u001b[1m\u001b[31m../../compiled_dag_node.py\u001b[0m:3315: in build_compiled_dag_from_ray_dag\n",
      "    \u001b[0mcompiled_dag._get_or_compile()\u001b[90m\u001b[39;49;00m\n",
      "\u001b[1m\u001b[31m../../compiled_dag_node.py\u001b[0m:1564: in _get_or_compile\n",
      "    \u001b[0m\u001b[96mself\u001b[39;49;00m._preprocess()\u001b[90m\u001b[39;49;00m\n",
      "\u001b[1m\u001b[31m../../compiled_dag_node.py\u001b[0m:1299: in _preprocess\n",
      "    \u001b[0m\u001b[96mself\u001b[39;49;00m._init_communicators()\u001b[90m\u001b[39;49;00m\n",
      "\u001b[1m\u001b[31m../../compiled_dag_node.py\u001b[0m:1357: in _init_communicators\n",
      "    \u001b[0mp2p_communicator_id = _init_communicator(\u001b[90m\u001b[39;49;00m\n",
      "\u001b[1m\u001b[31m../../../experimental/channel/torch_tensor_nccl_channel.py\u001b[0m:790: in _init_communicator\n",
      "    \u001b[0mray.get(init_tasks, timeout=\u001b[94m30\u001b[39;49;00m)\u001b[90m\u001b[39;49;00m\n",
      "\u001b[1m\u001b[31m../../../_private/auto_init_hook.py\u001b[0m:21: in auto_init_wrapper\n",
      "    \u001b[0m\u001b[94mreturn\u001b[39;49;00m fn(*args, **kwargs)\u001b[90m\u001b[39;49;00m\n",
      "\u001b[1m\u001b[31m../../../_private/client_mode_hook.py\u001b[0m:103: in wrapper\n",
      "    \u001b[0m\u001b[94mreturn\u001b[39;49;00m func(*args, **kwargs)\u001b[90m\u001b[39;49;00m\n",
      "\u001b[1m\u001b[31m../../../_private/worker.py\u001b[0m:2820: in get\n",
      "    \u001b[0mvalues, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\u001b[90m\u001b[39;49;00m\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\n",
      "self = <ray._private.worker.Worker object at 0x74e02fddca90>\n",
      "object_refs = [ObjectRef(878b2330f6be64f982094e576d3030a64c3379dc0100000001000000), ObjectRef(38e143cc5dfd1165aa43d5b6e6916ac2dda40c...cd603f1abd5233fe1d56954737e41bf60100000001000000), ObjectRef(9f166bdc2b6b5ec7357474c514730ae97f1e9a470100000001000000)]\n",
      "timeout = 30, return_exceptions = False, skip_deserialization = False\n",
      "\n",
      "    \u001b[0m\u001b[94mdef\u001b[39;49;00m \u001b[92mget_objects\u001b[39;49;00m(\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[96mself\u001b[39;49;00m,\u001b[90m\u001b[39;49;00m\n",
      "        object_refs: \u001b[96mlist\u001b[39;49;00m,\u001b[90m\u001b[39;49;00m\n",
      "        timeout: Optional[\u001b[96mfloat\u001b[39;49;00m] = \u001b[94mNone\u001b[39;49;00m,\u001b[90m\u001b[39;49;00m\n",
      "        return_exceptions: \u001b[96mbool\u001b[39;49;00m = \u001b[94mFalse\u001b[39;49;00m,\u001b[90m\u001b[39;49;00m\n",
      "        skip_deserialization: \u001b[96mbool\u001b[39;49;00m = \u001b[94mFalse\u001b[39;49;00m,\u001b[90m\u001b[39;49;00m\n",
      "    ):\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m    \u001b[39;49;00m\u001b[33m\"\"\"Get the values in the object store associated with the IDs.\u001b[39;49;00m\n",
      "    \u001b[33m\u001b[39;49;00m\n",
      "    \u001b[33m    Return the values from the local object store for object_refs. This\u001b[39;49;00m\n",
      "    \u001b[33m    will block until all the values for object_refs have been written to\u001b[39;49;00m\n",
      "    \u001b[33m    the local object store.\u001b[39;49;00m\n",
      "    \u001b[33m\u001b[39;49;00m\n",
      "    \u001b[33m    Args:\u001b[39;49;00m\n",
      "    \u001b[33m        object_refs: A list of the object refs\u001b[39;49;00m\n",
      "    \u001b[33m            whose values should be retrieved.\u001b[39;49;00m\n",
      "    \u001b[33m        timeout: The maximum amount of time in\u001b[39;49;00m\n",
      "    \u001b[33m            seconds to wait before returning.\u001b[39;49;00m\n",
      "    \u001b[33m        return_exceptions: If any of the objects deserialize to an\u001b[39;49;00m\n",
      "    \u001b[33m            Exception object, whether to return them as values in the\u001b[39;49;00m\n",
      "    \u001b[33m            returned list. If False, then the first found exception will be\u001b[39;49;00m\n",
      "    \u001b[33m            raised.\u001b[39;49;00m\n",
      "    \u001b[33m        skip_deserialization: If true, only the buffer will be released and\u001b[39;49;00m\n",
      "    \u001b[33m            the object associated with the buffer will not be deserialized.\u001b[39;49;00m\n",
      "    \u001b[33m    Returns:\u001b[39;49;00m\n",
      "    \u001b[33m        list: List of deserialized objects or None if skip_deserialization is True.\u001b[39;49;00m\n",
      "    \u001b[33m        bytes: UUID of the debugger breakpoint we should drop\u001b[39;49;00m\n",
      "    \u001b[33m            into or b\"\" if there is no breakpoint.\u001b[39;49;00m\n",
      "    \u001b[33m    \"\"\"\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[90m# Make sure that the values are object refs.\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[94mfor\u001b[39;49;00m object_ref \u001b[95min\u001b[39;49;00m object_refs:\u001b[90m\u001b[39;49;00m\n",
      "            \u001b[94mif\u001b[39;49;00m \u001b[95mnot\u001b[39;49;00m \u001b[96misinstance\u001b[39;49;00m(object_ref, ObjectRef):\u001b[90m\u001b[39;49;00m\n",
      "                \u001b[94mraise\u001b[39;49;00m \u001b[96mTypeError\u001b[39;49;00m(\u001b[90m\u001b[39;49;00m\n",
      "                    \u001b[33mf\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\u001b[33mAttempting to call `get` on the value \u001b[39;49;00m\u001b[33m{\u001b[39;49;00mobject_ref\u001b[33m}\u001b[39;49;00m\u001b[33m, \u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      "                    \u001b[33m\"\u001b[39;49;00m\u001b[33mwhich is not an ray.ObjectRef.\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      "                )\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "        timeout_ms = (\u001b[90m\u001b[39;49;00m\n",
      "            \u001b[96mint\u001b[39;49;00m(timeout * \u001b[94m1000\u001b[39;49;00m) \u001b[94mif\u001b[39;49;00m timeout \u001b[95mis\u001b[39;49;00m \u001b[95mnot\u001b[39;49;00m \u001b[94mNone\u001b[39;49;00m \u001b[95mand\u001b[39;49;00m timeout != -\u001b[94m1\u001b[39;49;00m \u001b[94melse\u001b[39;49;00m -\u001b[94m1\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      "        )\u001b[90m\u001b[39;49;00m\n",
      "        data_metadata_pairs: List[\u001b[90m\u001b[39;49;00m\n",
      "            Tuple[ray._raylet.Buffer, \u001b[96mbytes\u001b[39;49;00m]\u001b[90m\u001b[39;49;00m\n",
      "        ] = \u001b[96mself\u001b[39;49;00m.core_worker.get_objects(\u001b[90m\u001b[39;49;00m\n",
      "            object_refs,\u001b[90m\u001b[39;49;00m\n",
      "            timeout_ms,\u001b[90m\u001b[39;49;00m\n",
      "        )\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "        debugger_breakpoint = \u001b[33mb\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[94mfor\u001b[39;49;00m data, metadata \u001b[95min\u001b[39;49;00m data_metadata_pairs:\u001b[90m\u001b[39;49;00m\n",
      "            \u001b[94mif\u001b[39;49;00m metadata:\u001b[90m\u001b[39;49;00m\n",
      "                metadata_fields = metadata.split(\u001b[33mb\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\u001b[33m,\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\u001b[90m\u001b[39;49;00m\n",
      "                \u001b[94mif\u001b[39;49;00m \u001b[96mlen\u001b[39;49;00m(metadata_fields) >= \u001b[94m2\u001b[39;49;00m \u001b[95mand\u001b[39;49;00m metadata_fields[\u001b[94m1\u001b[39;49;00m].startswith(\u001b[90m\u001b[39;49;00m\n",
      "                    ray_constants.OBJECT_METADATA_DEBUG_PREFIX\u001b[90m\u001b[39;49;00m\n",
      "                ):\u001b[90m\u001b[39;49;00m\n",
      "                    debugger_breakpoint = metadata_fields[\u001b[94m1\u001b[39;49;00m][\u001b[90m\u001b[39;49;00m\n",
      "                        \u001b[96mlen\u001b[39;49;00m(ray_constants.OBJECT_METADATA_DEBUG_PREFIX) :\u001b[90m\u001b[39;49;00m\n",
      "                    ]\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[94mif\u001b[39;49;00m skip_deserialization:\u001b[90m\u001b[39;49;00m\n",
      "            \u001b[94mreturn\u001b[39;49;00m \u001b[94mNone\u001b[39;49;00m, debugger_breakpoint\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "        values = \u001b[96mself\u001b[39;49;00m.deserialize_objects(data_metadata_pairs, object_refs)\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[94mif\u001b[39;49;00m \u001b[95mnot\u001b[39;49;00m return_exceptions:\u001b[90m\u001b[39;49;00m\n",
      "            \u001b[90m# Raise exceptions instead of returning them to the user.\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      "            \u001b[94mfor\u001b[39;49;00m i, value \u001b[95min\u001b[39;49;00m \u001b[96menumerate\u001b[39;49;00m(values):\u001b[90m\u001b[39;49;00m\n",
      "                \u001b[94mif\u001b[39;49;00m \u001b[96misinstance\u001b[39;49;00m(value, RayError):\u001b[90m\u001b[39;49;00m\n",
      "                    \u001b[94mif\u001b[39;49;00m \u001b[96misinstance\u001b[39;49;00m(value, ray.exceptions.ObjectLostError):\u001b[90m\u001b[39;49;00m\n",
      "                        global_worker.core_worker.dump_object_store_memory_usage()\u001b[90m\u001b[39;49;00m\n",
      "                    \u001b[94mif\u001b[39;49;00m \u001b[96misinstance\u001b[39;49;00m(value, RayTaskError):\u001b[90m\u001b[39;49;00m\n",
      ">                       \u001b[94mraise\u001b[39;49;00m value.as_instanceof_cause()\u001b[90m\u001b[39;49;00m\n",
      "\u001b[1m\u001b[31mE                       ray.exceptions.RayTaskError(RuntimeError): \u001b[36mray::Worker.__ray_call__()\u001b[39m (pid=416824, ip=128.208.3.124, actor_id=357474c514730ae97f1e9a4701000000, repr=<test_execution_schedule_gpu.Worker object at 0x75fca80da7c0>)\u001b[0m\n",
      "\u001b[1m\u001b[31mE                         File \"/home/wxdeng/stbr-min/python/ray/actor.py\", line 1742, in __ray_call__\u001b[0m\n",
      "\u001b[1m\u001b[31mE                           return fn(self, *args, **kwargs)\u001b[0m\n",
      "\u001b[1m\u001b[31mE                         File \"/home/wxdeng/stbr-min/python/ray/experimental/channel/torch_tensor_nccl_channel.py\", line 663, in _do_init_communicator\u001b[0m\n",
      "\u001b[1m\u001b[31mE                           torch.cuda.current_stream().cuda_stream,\u001b[0m\n",
      "\u001b[1m\u001b[31mE                         File \"/home/wxdeng/miniconda3/envs/coll-sched-0512-py39/lib/python3.9/site-packages/torch/cuda/__init__.py\", line 1065, in current_stream\u001b[0m\n",
      "\u001b[1m\u001b[31mE                           _lazy_init()\u001b[0m\n",
      "\u001b[1m\u001b[31mE                         File \"/home/wxdeng/miniconda3/envs/coll-sched-0512-py39/lib/python3.9/site-packages/torch/cuda/__init__.py\", line 372, in _lazy_init\u001b[0m\n",
      "\u001b[1m\u001b[31mE                           torch._C._cuda_init()\u001b[0m\n",
      "\u001b[1m\u001b[31mE                       RuntimeError: No CUDA GPUs are available\u001b[0m\n",
      "\n",
      "\u001b[1m\u001b[31m../../../_private/worker.py\u001b[0m:930: RayTaskError(RuntimeError)\n",
      "\u001b[31m\u001b[1m______________ test_three_actors_with_nccl_1[ray_start_regular0] _______________\u001b[0m\n",
      "\n",
      "ray_start_regular = RayContext(dashboard_url='127.0.0.1:8265', python_version='3.9.22', ray_version='3.0.0.dev0', ray_commit='{{RAY_COMMIT_SHA}}')\n",
      "\n",
      "    \u001b[0m\u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mray_start_regular\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [{\u001b[33m\"\u001b[39;49;00m\u001b[33mnum_gpus\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m: \u001b[94m3\u001b[39;49;00m}], indirect=\u001b[94mTrue\u001b[39;49;00m)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mtest_three_actors_with_nccl_1\u001b[39;49;00m(ray_start_regular):\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m    \u001b[39;49;00m\u001b[33m\"\"\"\u001b[39;49;00m\n",
      "    \u001b[33m    Driver -> a.no_op -> b.no_op -> a.no_op_two -> Driver\u001b[39;49;00m\n",
      "    \u001b[33m                      |          |\u001b[39;49;00m\n",
      "    \u001b[33m                      -> c.no_op -\u001b[39;49;00m\n",
      "    \u001b[33m    \"\"\"\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[94mif\u001b[39;49;00m \u001b[95mnot\u001b[39;49;00m USE_GPU:\u001b[90m\u001b[39;49;00m\n",
      "            pytest.skip(\u001b[33m\"\u001b[39;49;00m\u001b[33mNCCL tests require GPUs\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "        a = Worker.remote()\u001b[90m\u001b[39;49;00m\n",
      "        b = Worker.remote()\u001b[90m\u001b[39;49;00m\n",
      "        c = Worker.remote()\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "        \u001b[94mwith\u001b[39;49;00m InputNode() \u001b[94mas\u001b[39;49;00m inp:\u001b[90m\u001b[39;49;00m\n",
      "            dag = a.no_op.bind(inp)\u001b[90m\u001b[39;49;00m\n",
      "            dag.with_tensor_transport(transport=\u001b[33m\"\u001b[39;49;00m\u001b[33mnccl\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\u001b[90m\u001b[39;49;00m\n",
      "            branch1 = b.no_op.bind(dag)\u001b[90m\u001b[39;49;00m\n",
      "            branch1.with_tensor_transport(transport=\u001b[33m\"\u001b[39;49;00m\u001b[33mnccl\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\u001b[90m\u001b[39;49;00m\n",
      "            branch2 = c.no_op.bind(dag)\u001b[90m\u001b[39;49;00m\n",
      "            branch2.with_tensor_transport(transport=\u001b[33m\"\u001b[39;49;00m\u001b[33mnccl\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\u001b[90m\u001b[39;49;00m\n",
      "            dag = a.no_op_two.bind(branch1, branch2)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      ">       compiled_dag = dag.experimental_compile()\u001b[90m\u001b[39;49;00m\n",
      "\n",
      "\u001b[1m\u001b[31mtest_execution_schedule_gpu.py\u001b[0m:252: \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\u001b[1m\u001b[31m../../dag_node.py\u001b[0m:340: in experimental_compile\n",
      "    \u001b[0m\u001b[94mreturn\u001b[39;49;00m build_compiled_dag_from_ray_dag(\u001b[90m\u001b[39;49;00m\n",
      "\u001b[1m\u001b[31m../../compiled_dag_node.py\u001b[0m:3315: in build_compiled_dag_from_ray_dag\n",
      "    \u001b[0mcompiled_dag._get_or_compile()\u001b[90m\u001b[39;49;00m\n",
      "\u001b[1m\u001b[31m../../compiled_dag_node.py\u001b[0m:1564: in _get_or_compile\n",
      "    \u001b[0m\u001b[96mself\u001b[39;49;00m._preprocess()\u001b[90m\u001b[39;49;00m\n",
      "\u001b[1m\u001b[31m../../compiled_dag_node.py\u001b[0m:1299: in _preprocess\n",
      "    \u001b[0m\u001b[96mself\u001b[39;49;00m._init_communicators()\u001b[90m\u001b[39;49;00m\n",
      "\u001b[1m\u001b[31m../../compiled_dag_node.py\u001b[0m:1357: in _init_communicators\n",
      "    \u001b[0mp2p_communicator_id = _init_communicator(\u001b[90m\u001b[39;49;00m\n",
      "\u001b[1m\u001b[31m../../../experimental/channel/torch_tensor_nccl_channel.py\u001b[0m:790: in _init_communicator\n",
      "    \u001b[0mray.get(init_tasks, timeout=\u001b[94m30\u001b[39;49;00m)\u001b[90m\u001b[39;49;00m\n",
      "\u001b[1m\u001b[31m../../../_private/auto_init_hook.py\u001b[0m:21: in auto_init_wrapper\n",
      "    \u001b[0m\u001b[94mreturn\u001b[39;49;00m fn(*args, **kwargs)\u001b[90m\u001b[39;49;00m\n",
      "\u001b[1m\u001b[31m../../../_private/client_mode_hook.py\u001b[0m:103: in wrapper\n",
      "    \u001b[0m\u001b[94mreturn\u001b[39;49;00m func(*args, **kwargs)\u001b[90m\u001b[39;49;00m\n",
      "\u001b[1m\u001b[31m../../../_private/worker.py\u001b[0m:2820: in get\n",
      "    \u001b[0mvalues, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\u001b[90m\u001b[39;49;00m\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\n",
      "self = <ray._private.worker.Worker object at 0x74e02fddca90>\n",
      "object_refs = [ObjectRef(7486c9c5cb2b345ec297d2cc111cd39c0543cf4b0100000001000000), ObjectRef(9a667646e288b2528bb1f07097880644884e44060100000001000000), ObjectRef(058595f16dc6f278d8cd084a8b9a1cab27e98ef00100000001000000)]\n",
      "timeout = 30, return_exceptions = False, skip_deserialization = False\n",
      "\n",
      "    \u001b[0m\u001b[94mdef\u001b[39;49;00m \u001b[92mget_objects\u001b[39;49;00m(\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[96mself\u001b[39;49;00m,\u001b[90m\u001b[39;49;00m\n",
      "        object_refs: \u001b[96mlist\u001b[39;49;00m,\u001b[90m\u001b[39;49;00m\n",
      "        timeout: Optional[\u001b[96mfloat\u001b[39;49;00m] = \u001b[94mNone\u001b[39;49;00m,\u001b[90m\u001b[39;49;00m\n",
      "        return_exceptions: \u001b[96mbool\u001b[39;49;00m = \u001b[94mFalse\u001b[39;49;00m,\u001b[90m\u001b[39;49;00m\n",
      "        skip_deserialization: \u001b[96mbool\u001b[39;49;00m = \u001b[94mFalse\u001b[39;49;00m,\u001b[90m\u001b[39;49;00m\n",
      "    ):\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m    \u001b[39;49;00m\u001b[33m\"\"\"Get the values in the object store associated with the IDs.\u001b[39;49;00m\n",
      "    \u001b[33m\u001b[39;49;00m\n",
      "    \u001b[33m    Return the values from the local object store for object_refs. This\u001b[39;49;00m\n",
      "    \u001b[33m    will block until all the values for object_refs have been written to\u001b[39;49;00m\n",
      "    \u001b[33m    the local object store.\u001b[39;49;00m\n",
      "    \u001b[33m\u001b[39;49;00m\n",
      "    \u001b[33m    Args:\u001b[39;49;00m\n",
      "    \u001b[33m        object_refs: A list of the object refs\u001b[39;49;00m\n",
      "    \u001b[33m            whose values should be retrieved.\u001b[39;49;00m\n",
      "    \u001b[33m        timeout: The maximum amount of time in\u001b[39;49;00m\n",
      "    \u001b[33m            seconds to wait before returning.\u001b[39;49;00m\n",
      "    \u001b[33m        return_exceptions: If any of the objects deserialize to an\u001b[39;49;00m\n",
      "    \u001b[33m            Exception object, whether to return them as values in the\u001b[39;49;00m\n",
      "    \u001b[33m            returned list. If False, then the first found exception will be\u001b[39;49;00m\n",
      "    \u001b[33m            raised.\u001b[39;49;00m\n",
      "    \u001b[33m        skip_deserialization: If true, only the buffer will be released and\u001b[39;49;00m\n",
      "    \u001b[33m            the object associated with the buffer will not be deserialized.\u001b[39;49;00m\n",
      "    \u001b[33m    Returns:\u001b[39;49;00m\n",
      "    \u001b[33m        list: List of deserialized objects or None if skip_deserialization is True.\u001b[39;49;00m\n",
      "    \u001b[33m        bytes: UUID of the debugger breakpoint we should drop\u001b[39;49;00m\n",
      "    \u001b[33m            into or b\"\" if there is no breakpoint.\u001b[39;49;00m\n",
      "    \u001b[33m    \"\"\"\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[90m# Make sure that the values are object refs.\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[94mfor\u001b[39;49;00m object_ref \u001b[95min\u001b[39;49;00m object_refs:\u001b[90m\u001b[39;49;00m\n",
      "            \u001b[94mif\u001b[39;49;00m \u001b[95mnot\u001b[39;49;00m \u001b[96misinstance\u001b[39;49;00m(object_ref, ObjectRef):\u001b[90m\u001b[39;49;00m\n",
      "                \u001b[94mraise\u001b[39;49;00m \u001b[96mTypeError\u001b[39;49;00m(\u001b[90m\u001b[39;49;00m\n",
      "                    \u001b[33mf\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\u001b[33mAttempting to call `get` on the value \u001b[39;49;00m\u001b[33m{\u001b[39;49;00mobject_ref\u001b[33m}\u001b[39;49;00m\u001b[33m, \u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      "                    \u001b[33m\"\u001b[39;49;00m\u001b[33mwhich is not an ray.ObjectRef.\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      "                )\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "        timeout_ms = (\u001b[90m\u001b[39;49;00m\n",
      "            \u001b[96mint\u001b[39;49;00m(timeout * \u001b[94m1000\u001b[39;49;00m) \u001b[94mif\u001b[39;49;00m timeout \u001b[95mis\u001b[39;49;00m \u001b[95mnot\u001b[39;49;00m \u001b[94mNone\u001b[39;49;00m \u001b[95mand\u001b[39;49;00m timeout != -\u001b[94m1\u001b[39;49;00m \u001b[94melse\u001b[39;49;00m -\u001b[94m1\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      "        )\u001b[90m\u001b[39;49;00m\n",
      "        data_metadata_pairs: List[\u001b[90m\u001b[39;49;00m\n",
      "            Tuple[ray._raylet.Buffer, \u001b[96mbytes\u001b[39;49;00m]\u001b[90m\u001b[39;49;00m\n",
      "        ] = \u001b[96mself\u001b[39;49;00m.core_worker.get_objects(\u001b[90m\u001b[39;49;00m\n",
      "            object_refs,\u001b[90m\u001b[39;49;00m\n",
      "            timeout_ms,\u001b[90m\u001b[39;49;00m\n",
      "        )\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "        debugger_breakpoint = \u001b[33mb\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[94mfor\u001b[39;49;00m data, metadata \u001b[95min\u001b[39;49;00m data_metadata_pairs:\u001b[90m\u001b[39;49;00m\n",
      "            \u001b[94mif\u001b[39;49;00m metadata:\u001b[90m\u001b[39;49;00m\n",
      "                metadata_fields = metadata.split(\u001b[33mb\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\u001b[33m,\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\u001b[90m\u001b[39;49;00m\n",
      "                \u001b[94mif\u001b[39;49;00m \u001b[96mlen\u001b[39;49;00m(metadata_fields) >= \u001b[94m2\u001b[39;49;00m \u001b[95mand\u001b[39;49;00m metadata_fields[\u001b[94m1\u001b[39;49;00m].startswith(\u001b[90m\u001b[39;49;00m\n",
      "                    ray_constants.OBJECT_METADATA_DEBUG_PREFIX\u001b[90m\u001b[39;49;00m\n",
      "                ):\u001b[90m\u001b[39;49;00m\n",
      "                    debugger_breakpoint = metadata_fields[\u001b[94m1\u001b[39;49;00m][\u001b[90m\u001b[39;49;00m\n",
      "                        \u001b[96mlen\u001b[39;49;00m(ray_constants.OBJECT_METADATA_DEBUG_PREFIX) :\u001b[90m\u001b[39;49;00m\n",
      "                    ]\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[94mif\u001b[39;49;00m skip_deserialization:\u001b[90m\u001b[39;49;00m\n",
      "            \u001b[94mreturn\u001b[39;49;00m \u001b[94mNone\u001b[39;49;00m, debugger_breakpoint\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "        values = \u001b[96mself\u001b[39;49;00m.deserialize_objects(data_metadata_pairs, object_refs)\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[94mif\u001b[39;49;00m \u001b[95mnot\u001b[39;49;00m return_exceptions:\u001b[90m\u001b[39;49;00m\n",
      "            \u001b[90m# Raise exceptions instead of returning them to the user.\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      "            \u001b[94mfor\u001b[39;49;00m i, value \u001b[95min\u001b[39;49;00m \u001b[96menumerate\u001b[39;49;00m(values):\u001b[90m\u001b[39;49;00m\n",
      "                \u001b[94mif\u001b[39;49;00m \u001b[96misinstance\u001b[39;49;00m(value, RayError):\u001b[90m\u001b[39;49;00m\n",
      "                    \u001b[94mif\u001b[39;49;00m \u001b[96misinstance\u001b[39;49;00m(value, ray.exceptions.ObjectLostError):\u001b[90m\u001b[39;49;00m\n",
      "                        global_worker.core_worker.dump_object_store_memory_usage()\u001b[90m\u001b[39;49;00m\n",
      "                    \u001b[94mif\u001b[39;49;00m \u001b[96misinstance\u001b[39;49;00m(value, RayTaskError):\u001b[90m\u001b[39;49;00m\n",
      ">                       \u001b[94mraise\u001b[39;49;00m value.as_instanceof_cause()\u001b[90m\u001b[39;49;00m\n",
      "\u001b[1m\u001b[31mE                       ray.exceptions.RayTaskError(RuntimeError): \u001b[36mray::Worker.__ray_call__()\u001b[39m (pid=418108, ip=128.208.3.124, actor_id=d8cd084a8b9a1cab27e98ef001000000, repr=<test_execution_schedule_gpu.Worker object at 0x76f56d79a190>)\u001b[0m\n",
      "\u001b[1m\u001b[31mE                         File \"/home/wxdeng/stbr-min/python/ray/actor.py\", line 1742, in __ray_call__\u001b[0m\n",
      "\u001b[1m\u001b[31mE                           return fn(self, *args, **kwargs)\u001b[0m\n",
      "\u001b[1m\u001b[31mE                         File \"/home/wxdeng/stbr-min/python/ray/experimental/channel/torch_tensor_nccl_channel.py\", line 663, in _do_init_communicator\u001b[0m\n",
      "\u001b[1m\u001b[31mE                           torch.cuda.current_stream().cuda_stream,\u001b[0m\n",
      "\u001b[1m\u001b[31mE                         File \"/home/wxdeng/miniconda3/envs/coll-sched-0512-py39/lib/python3.9/site-packages/torch/cuda/__init__.py\", line 1065, in current_stream\u001b[0m\n",
      "\u001b[1m\u001b[31mE                           _lazy_init()\u001b[0m\n",
      "\u001b[1m\u001b[31mE                         File \"/home/wxdeng/miniconda3/envs/coll-sched-0512-py39/lib/python3.9/site-packages/torch/cuda/__init__.py\", line 372, in _lazy_init\u001b[0m\n",
      "\u001b[1m\u001b[31mE                           torch._C._cuda_init()\u001b[0m\n",
      "\u001b[1m\u001b[31mE                       RuntimeError: No CUDA GPUs are available\u001b[0m\n",
      "\n",
      "\u001b[1m\u001b[31m../../../_private/worker.py\u001b[0m:930: RayTaskError(RuntimeError)\n",
      "\u001b[31m\u001b[1m____________ test_three_actors_with_nccl_2[True-ray_start_regular0] ____________\u001b[0m\n",
      "\n",
      "ray_start_regular = RayContext(dashboard_url='127.0.0.1:8265', python_version='3.9.22', ray_version='3.0.0.dev0', ray_commit='{{RAY_COMMIT_SHA}}')\n",
      "single_fetch = True\n",
      "monkeypatch = <_pytest.monkeypatch.MonkeyPatch object at 0x74df5c16ebb0>\n",
      "\n",
      "    \u001b[0m\u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mray_start_regular\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [{\u001b[33m\"\u001b[39;49;00m\u001b[33mnum_gpus\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m: \u001b[94m3\u001b[39;49;00m}], indirect=\u001b[94mTrue\u001b[39;49;00m)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33msingle_fetch\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94mTrue\u001b[39;49;00m, \u001b[94mFalse\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mtest_three_actors_with_nccl_2\u001b[39;49;00m(ray_start_regular, single_fetch, monkeypatch):\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[94mif\u001b[39;49;00m \u001b[95mnot\u001b[39;49;00m USE_GPU:\u001b[90m\u001b[39;49;00m\n",
      "            pytest.skip(\u001b[33m\"\u001b[39;49;00m\u001b[33mNCCL tests require GPUs\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "        a = Worker.remote()\u001b[90m\u001b[39;49;00m\n",
      "        b = Worker.remote()\u001b[90m\u001b[39;49;00m\n",
      "        c = Worker.remote()\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "        \u001b[94mwith\u001b[39;49;00m InputNode() \u001b[94mas\u001b[39;49;00m inp:\u001b[90m\u001b[39;49;00m\n",
      "            branch1 = a.no_op.bind(inp)\u001b[90m\u001b[39;49;00m\n",
      "            branch1.with_tensor_transport(transport=\u001b[33m\"\u001b[39;49;00m\u001b[33mnccl\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\u001b[90m\u001b[39;49;00m\n",
      "            branch2 = b.no_op.bind(inp)\u001b[90m\u001b[39;49;00m\n",
      "            branch2.with_tensor_transport(transport=\u001b[33m\"\u001b[39;49;00m\u001b[33mnccl\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\u001b[90m\u001b[39;49;00m\n",
      "            branch3 = c.no_op.bind(inp)\u001b[90m\u001b[39;49;00m\n",
      "            branch3.with_tensor_transport(transport=\u001b[33m\"\u001b[39;49;00m\u001b[33mnccl\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\u001b[90m\u001b[39;49;00m\n",
      "            dag = MultiOutputNode(\u001b[90m\u001b[39;49;00m\n",
      "                [\u001b[90m\u001b[39;49;00m\n",
      "                    a.no_op.bind(branch3),\u001b[90m\u001b[39;49;00m\n",
      "                    b.no_op.bind(branch1),\u001b[90m\u001b[39;49;00m\n",
      "                    c.no_op.bind(branch2),\u001b[90m\u001b[39;49;00m\n",
      "                ]\u001b[90m\u001b[39;49;00m\n",
      "            )\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      ">       compiled_dag = dag.experimental_compile()\u001b[90m\u001b[39;49;00m\n",
      "\n",
      "\u001b[1m\u001b[31mtest_execution_schedule_gpu.py\u001b[0m:320: \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\u001b[1m\u001b[31m../../dag_node.py\u001b[0m:340: in experimental_compile\n",
      "    \u001b[0m\u001b[94mreturn\u001b[39;49;00m build_compiled_dag_from_ray_dag(\u001b[90m\u001b[39;49;00m\n",
      "\u001b[1m\u001b[31m../../compiled_dag_node.py\u001b[0m:3315: in build_compiled_dag_from_ray_dag\n",
      "    \u001b[0mcompiled_dag._get_or_compile()\u001b[90m\u001b[39;49;00m\n",
      "\u001b[1m\u001b[31m../../compiled_dag_node.py\u001b[0m:1564: in _get_or_compile\n",
      "    \u001b[0m\u001b[96mself\u001b[39;49;00m._preprocess()\u001b[90m\u001b[39;49;00m\n",
      "\u001b[1m\u001b[31m../../compiled_dag_node.py\u001b[0m:1299: in _preprocess\n",
      "    \u001b[0m\u001b[96mself\u001b[39;49;00m._init_communicators()\u001b[90m\u001b[39;49;00m\n",
      "\u001b[1m\u001b[31m../../compiled_dag_node.py\u001b[0m:1357: in _init_communicators\n",
      "    \u001b[0mp2p_communicator_id = _init_communicator(\u001b[90m\u001b[39;49;00m\n",
      "\u001b[1m\u001b[31m../../../experimental/channel/torch_tensor_nccl_channel.py\u001b[0m:790: in _init_communicator\n",
      "    \u001b[0mray.get(init_tasks, timeout=\u001b[94m30\u001b[39;49;00m)\u001b[90m\u001b[39;49;00m\n",
      "\u001b[1m\u001b[31m../../../_private/auto_init_hook.py\u001b[0m:21: in auto_init_wrapper\n",
      "    \u001b[0m\u001b[94mreturn\u001b[39;49;00m fn(*args, **kwargs)\u001b[90m\u001b[39;49;00m\n",
      "\u001b[1m\u001b[31m../../../_private/client_mode_hook.py\u001b[0m:103: in wrapper\n",
      "    \u001b[0m\u001b[94mreturn\u001b[39;49;00m func(*args, **kwargs)\u001b[90m\u001b[39;49;00m\n",
      "\u001b[1m\u001b[31m../../../_private/worker.py\u001b[0m:2820: in get\n",
      "    \u001b[0mvalues, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\u001b[90m\u001b[39;49;00m\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\n",
      "self = <ray._private.worker.Worker object at 0x74e02fddca90>\n",
      "object_refs = [ObjectRef(8c5fdac94fbfecf42ae7edf2979813840f8f2cf20100000001000000), ObjectRef(b812cdadd8a780aae76fd021efb1266bfc4680d30100000001000000), ObjectRef(eb9952338cd40cb89c97d33f0607880cba102a8e0100000001000000)]\n",
      "timeout = 30, return_exceptions = False, skip_deserialization = False\n",
      "\n",
      "    \u001b[0m\u001b[94mdef\u001b[39;49;00m \u001b[92mget_objects\u001b[39;49;00m(\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[96mself\u001b[39;49;00m,\u001b[90m\u001b[39;49;00m\n",
      "        object_refs: \u001b[96mlist\u001b[39;49;00m,\u001b[90m\u001b[39;49;00m\n",
      "        timeout: Optional[\u001b[96mfloat\u001b[39;49;00m] = \u001b[94mNone\u001b[39;49;00m,\u001b[90m\u001b[39;49;00m\n",
      "        return_exceptions: \u001b[96mbool\u001b[39;49;00m = \u001b[94mFalse\u001b[39;49;00m,\u001b[90m\u001b[39;49;00m\n",
      "        skip_deserialization: \u001b[96mbool\u001b[39;49;00m = \u001b[94mFalse\u001b[39;49;00m,\u001b[90m\u001b[39;49;00m\n",
      "    ):\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m    \u001b[39;49;00m\u001b[33m\"\"\"Get the values in the object store associated with the IDs.\u001b[39;49;00m\n",
      "    \u001b[33m\u001b[39;49;00m\n",
      "    \u001b[33m    Return the values from the local object store for object_refs. This\u001b[39;49;00m\n",
      "    \u001b[33m    will block until all the values for object_refs have been written to\u001b[39;49;00m\n",
      "    \u001b[33m    the local object store.\u001b[39;49;00m\n",
      "    \u001b[33m\u001b[39;49;00m\n",
      "    \u001b[33m    Args:\u001b[39;49;00m\n",
      "    \u001b[33m        object_refs: A list of the object refs\u001b[39;49;00m\n",
      "    \u001b[33m            whose values should be retrieved.\u001b[39;49;00m\n",
      "    \u001b[33m        timeout: The maximum amount of time in\u001b[39;49;00m\n",
      "    \u001b[33m            seconds to wait before returning.\u001b[39;49;00m\n",
      "    \u001b[33m        return_exceptions: If any of the objects deserialize to an\u001b[39;49;00m\n",
      "    \u001b[33m            Exception object, whether to return them as values in the\u001b[39;49;00m\n",
      "    \u001b[33m            returned list. If False, then the first found exception will be\u001b[39;49;00m\n",
      "    \u001b[33m            raised.\u001b[39;49;00m\n",
      "    \u001b[33m        skip_deserialization: If true, only the buffer will be released and\u001b[39;49;00m\n",
      "    \u001b[33m            the object associated with the buffer will not be deserialized.\u001b[39;49;00m\n",
      "    \u001b[33m    Returns:\u001b[39;49;00m\n",
      "    \u001b[33m        list: List of deserialized objects or None if skip_deserialization is True.\u001b[39;49;00m\n",
      "    \u001b[33m        bytes: UUID of the debugger breakpoint we should drop\u001b[39;49;00m\n",
      "    \u001b[33m            into or b\"\" if there is no breakpoint.\u001b[39;49;00m\n",
      "    \u001b[33m    \"\"\"\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[90m# Make sure that the values are object refs.\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[94mfor\u001b[39;49;00m object_ref \u001b[95min\u001b[39;49;00m object_refs:\u001b[90m\u001b[39;49;00m\n",
      "            \u001b[94mif\u001b[39;49;00m \u001b[95mnot\u001b[39;49;00m \u001b[96misinstance\u001b[39;49;00m(object_ref, ObjectRef):\u001b[90m\u001b[39;49;00m\n",
      "                \u001b[94mraise\u001b[39;49;00m \u001b[96mTypeError\u001b[39;49;00m(\u001b[90m\u001b[39;49;00m\n",
      "                    \u001b[33mf\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\u001b[33mAttempting to call `get` on the value \u001b[39;49;00m\u001b[33m{\u001b[39;49;00mobject_ref\u001b[33m}\u001b[39;49;00m\u001b[33m, \u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      "                    \u001b[33m\"\u001b[39;49;00m\u001b[33mwhich is not an ray.ObjectRef.\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      "                )\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "        timeout_ms = (\u001b[90m\u001b[39;49;00m\n",
      "            \u001b[96mint\u001b[39;49;00m(timeout * \u001b[94m1000\u001b[39;49;00m) \u001b[94mif\u001b[39;49;00m timeout \u001b[95mis\u001b[39;49;00m \u001b[95mnot\u001b[39;49;00m \u001b[94mNone\u001b[39;49;00m \u001b[95mand\u001b[39;49;00m timeout != -\u001b[94m1\u001b[39;49;00m \u001b[94melse\u001b[39;49;00m -\u001b[94m1\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      "        )\u001b[90m\u001b[39;49;00m\n",
      "        data_metadata_pairs: List[\u001b[90m\u001b[39;49;00m\n",
      "            Tuple[ray._raylet.Buffer, \u001b[96mbytes\u001b[39;49;00m]\u001b[90m\u001b[39;49;00m\n",
      "        ] = \u001b[96mself\u001b[39;49;00m.core_worker.get_objects(\u001b[90m\u001b[39;49;00m\n",
      "            object_refs,\u001b[90m\u001b[39;49;00m\n",
      "            timeout_ms,\u001b[90m\u001b[39;49;00m\n",
      "        )\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "        debugger_breakpoint = \u001b[33mb\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[94mfor\u001b[39;49;00m data, metadata \u001b[95min\u001b[39;49;00m data_metadata_pairs:\u001b[90m\u001b[39;49;00m\n",
      "            \u001b[94mif\u001b[39;49;00m metadata:\u001b[90m\u001b[39;49;00m\n",
      "                metadata_fields = metadata.split(\u001b[33mb\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\u001b[33m,\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\u001b[90m\u001b[39;49;00m\n",
      "                \u001b[94mif\u001b[39;49;00m \u001b[96mlen\u001b[39;49;00m(metadata_fields) >= \u001b[94m2\u001b[39;49;00m \u001b[95mand\u001b[39;49;00m metadata_fields[\u001b[94m1\u001b[39;49;00m].startswith(\u001b[90m\u001b[39;49;00m\n",
      "                    ray_constants.OBJECT_METADATA_DEBUG_PREFIX\u001b[90m\u001b[39;49;00m\n",
      "                ):\u001b[90m\u001b[39;49;00m\n",
      "                    debugger_breakpoint = metadata_fields[\u001b[94m1\u001b[39;49;00m][\u001b[90m\u001b[39;49;00m\n",
      "                        \u001b[96mlen\u001b[39;49;00m(ray_constants.OBJECT_METADATA_DEBUG_PREFIX) :\u001b[90m\u001b[39;49;00m\n",
      "                    ]\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[94mif\u001b[39;49;00m skip_deserialization:\u001b[90m\u001b[39;49;00m\n",
      "            \u001b[94mreturn\u001b[39;49;00m \u001b[94mNone\u001b[39;49;00m, debugger_breakpoint\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "        values = \u001b[96mself\u001b[39;49;00m.deserialize_objects(data_metadata_pairs, object_refs)\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[94mif\u001b[39;49;00m \u001b[95mnot\u001b[39;49;00m return_exceptions:\u001b[90m\u001b[39;49;00m\n",
      "            \u001b[90m# Raise exceptions instead of returning them to the user.\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      "            \u001b[94mfor\u001b[39;49;00m i, value \u001b[95min\u001b[39;49;00m \u001b[96menumerate\u001b[39;49;00m(values):\u001b[90m\u001b[39;49;00m\n",
      "                \u001b[94mif\u001b[39;49;00m \u001b[96misinstance\u001b[39;49;00m(value, RayError):\u001b[90m\u001b[39;49;00m\n",
      "                    \u001b[94mif\u001b[39;49;00m \u001b[96misinstance\u001b[39;49;00m(value, ray.exceptions.ObjectLostError):\u001b[90m\u001b[39;49;00m\n",
      "                        global_worker.core_worker.dump_object_store_memory_usage()\u001b[90m\u001b[39;49;00m\n",
      "                    \u001b[94mif\u001b[39;49;00m \u001b[96misinstance\u001b[39;49;00m(value, RayTaskError):\u001b[90m\u001b[39;49;00m\n",
      ">                       \u001b[94mraise\u001b[39;49;00m value.as_instanceof_cause()\u001b[90m\u001b[39;49;00m\n",
      "\u001b[1m\u001b[31mE                       ray.exceptions.RayTaskError(RuntimeError): \u001b[36mray::Worker.__ray_call__()\u001b[39m (pid=419352, ip=128.208.3.124, actor_id=e76fd021efb1266bfc4680d301000000, repr=<test_execution_schedule_gpu.Worker object at 0x7c35bd6d9100>)\u001b[0m\n",
      "\u001b[1m\u001b[31mE                         File \"/home/wxdeng/stbr-min/python/ray/actor.py\", line 1742, in __ray_call__\u001b[0m\n",
      "\u001b[1m\u001b[31mE                           return fn(self, *args, **kwargs)\u001b[0m\n",
      "\u001b[1m\u001b[31mE                         File \"/home/wxdeng/stbr-min/python/ray/experimental/channel/torch_tensor_nccl_channel.py\", line 663, in _do_init_communicator\u001b[0m\n",
      "\u001b[1m\u001b[31mE                           torch.cuda.current_stream().cuda_stream,\u001b[0m\n",
      "\u001b[1m\u001b[31mE                         File \"/home/wxdeng/miniconda3/envs/coll-sched-0512-py39/lib/python3.9/site-packages/torch/cuda/__init__.py\", line 1065, in current_stream\u001b[0m\n",
      "\u001b[1m\u001b[31mE                           _lazy_init()\u001b[0m\n",
      "\u001b[1m\u001b[31mE                         File \"/home/wxdeng/miniconda3/envs/coll-sched-0512-py39/lib/python3.9/site-packages/torch/cuda/__init__.py\", line 372, in _lazy_init\u001b[0m\n",
      "\u001b[1m\u001b[31mE                           torch._C._cuda_init()\u001b[0m\n",
      "\u001b[1m\u001b[31mE                       RuntimeError: No CUDA GPUs are available\u001b[0m\n",
      "\n",
      "\u001b[1m\u001b[31m../../../_private/worker.py\u001b[0m:930: RayTaskError(RuntimeError)\n",
      "\u001b[31m\u001b[1m___________ test_three_actors_with_nccl_2[False-ray_start_regular0] ____________\u001b[0m\n",
      "\n",
      "ray_start_regular = RayContext(dashboard_url='127.0.0.1:8265', python_version='3.9.22', ray_version='3.0.0.dev0', ray_commit='{{RAY_COMMIT_SHA}}')\n",
      "single_fetch = False\n",
      "monkeypatch = <_pytest.monkeypatch.MonkeyPatch object at 0x74df716a6af0>\n",
      "\n",
      "    \u001b[0m\u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mray_start_regular\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [{\u001b[33m\"\u001b[39;49;00m\u001b[33mnum_gpus\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m: \u001b[94m3\u001b[39;49;00m}], indirect=\u001b[94mTrue\u001b[39;49;00m)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33msingle_fetch\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94mTrue\u001b[39;49;00m, \u001b[94mFalse\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mtest_three_actors_with_nccl_2\u001b[39;49;00m(ray_start_regular, single_fetch, monkeypatch):\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[94mif\u001b[39;49;00m \u001b[95mnot\u001b[39;49;00m USE_GPU:\u001b[90m\u001b[39;49;00m\n",
      "            pytest.skip(\u001b[33m\"\u001b[39;49;00m\u001b[33mNCCL tests require GPUs\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "        a = Worker.remote()\u001b[90m\u001b[39;49;00m\n",
      "        b = Worker.remote()\u001b[90m\u001b[39;49;00m\n",
      "        c = Worker.remote()\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "        \u001b[94mwith\u001b[39;49;00m InputNode() \u001b[94mas\u001b[39;49;00m inp:\u001b[90m\u001b[39;49;00m\n",
      "            branch1 = a.no_op.bind(inp)\u001b[90m\u001b[39;49;00m\n",
      "            branch1.with_tensor_transport(transport=\u001b[33m\"\u001b[39;49;00m\u001b[33mnccl\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\u001b[90m\u001b[39;49;00m\n",
      "            branch2 = b.no_op.bind(inp)\u001b[90m\u001b[39;49;00m\n",
      "            branch2.with_tensor_transport(transport=\u001b[33m\"\u001b[39;49;00m\u001b[33mnccl\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\u001b[90m\u001b[39;49;00m\n",
      "            branch3 = c.no_op.bind(inp)\u001b[90m\u001b[39;49;00m\n",
      "            branch3.with_tensor_transport(transport=\u001b[33m\"\u001b[39;49;00m\u001b[33mnccl\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\u001b[90m\u001b[39;49;00m\n",
      "            dag = MultiOutputNode(\u001b[90m\u001b[39;49;00m\n",
      "                [\u001b[90m\u001b[39;49;00m\n",
      "                    a.no_op.bind(branch3),\u001b[90m\u001b[39;49;00m\n",
      "                    b.no_op.bind(branch1),\u001b[90m\u001b[39;49;00m\n",
      "                    c.no_op.bind(branch2),\u001b[90m\u001b[39;49;00m\n",
      "                ]\u001b[90m\u001b[39;49;00m\n",
      "            )\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      ">       compiled_dag = dag.experimental_compile()\u001b[90m\u001b[39;49;00m\n",
      "\n",
      "\u001b[1m\u001b[31mtest_execution_schedule_gpu.py\u001b[0m:320: \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\u001b[1m\u001b[31m../../dag_node.py\u001b[0m:340: in experimental_compile\n",
      "    \u001b[0m\u001b[94mreturn\u001b[39;49;00m build_compiled_dag_from_ray_dag(\u001b[90m\u001b[39;49;00m\n",
      "\u001b[1m\u001b[31m../../compiled_dag_node.py\u001b[0m:3315: in build_compiled_dag_from_ray_dag\n",
      "    \u001b[0mcompiled_dag._get_or_compile()\u001b[90m\u001b[39;49;00m\n",
      "\u001b[1m\u001b[31m../../compiled_dag_node.py\u001b[0m:1564: in _get_or_compile\n",
      "    \u001b[0m\u001b[96mself\u001b[39;49;00m._preprocess()\u001b[90m\u001b[39;49;00m\n",
      "\u001b[1m\u001b[31m../../compiled_dag_node.py\u001b[0m:1299: in _preprocess\n",
      "    \u001b[0m\u001b[96mself\u001b[39;49;00m._init_communicators()\u001b[90m\u001b[39;49;00m\n",
      "\u001b[1m\u001b[31m../../compiled_dag_node.py\u001b[0m:1357: in _init_communicators\n",
      "    \u001b[0mp2p_communicator_id = _init_communicator(\u001b[90m\u001b[39;49;00m\n",
      "\u001b[1m\u001b[31m../../../experimental/channel/torch_tensor_nccl_channel.py\u001b[0m:790: in _init_communicator\n",
      "    \u001b[0mray.get(init_tasks, timeout=\u001b[94m30\u001b[39;49;00m)\u001b[90m\u001b[39;49;00m\n",
      "\u001b[1m\u001b[31m../../../_private/auto_init_hook.py\u001b[0m:21: in auto_init_wrapper\n",
      "    \u001b[0m\u001b[94mreturn\u001b[39;49;00m fn(*args, **kwargs)\u001b[90m\u001b[39;49;00m\n",
      "\u001b[1m\u001b[31m../../../_private/client_mode_hook.py\u001b[0m:103: in wrapper\n",
      "    \u001b[0m\u001b[94mreturn\u001b[39;49;00m func(*args, **kwargs)\u001b[90m\u001b[39;49;00m\n",
      "\u001b[1m\u001b[31m../../../_private/worker.py\u001b[0m:2820: in get\n",
      "    \u001b[0mvalues, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\u001b[90m\u001b[39;49;00m\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\n",
      "self = <ray._private.worker.Worker object at 0x74e02fddca90>\n",
      "object_refs = [ObjectRef(77dc56ecd9a0549a7b76b108b090ec43c422bbbf0100000001000000), ObjectRef(667bfe78fab65b03a12c19ec766a86cc0c4414090100000001000000), ObjectRef(ed8d71ff7ae1bec4abdc44fa5b00ebe71bc33f7f0100000001000000)]\n",
      "timeout = 30, return_exceptions = False, skip_deserialization = False\n",
      "\n",
      "    \u001b[0m\u001b[94mdef\u001b[39;49;00m \u001b[92mget_objects\u001b[39;49;00m(\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[96mself\u001b[39;49;00m,\u001b[90m\u001b[39;49;00m\n",
      "        object_refs: \u001b[96mlist\u001b[39;49;00m,\u001b[90m\u001b[39;49;00m\n",
      "        timeout: Optional[\u001b[96mfloat\u001b[39;49;00m] = \u001b[94mNone\u001b[39;49;00m,\u001b[90m\u001b[39;49;00m\n",
      "        return_exceptions: \u001b[96mbool\u001b[39;49;00m = \u001b[94mFalse\u001b[39;49;00m,\u001b[90m\u001b[39;49;00m\n",
      "        skip_deserialization: \u001b[96mbool\u001b[39;49;00m = \u001b[94mFalse\u001b[39;49;00m,\u001b[90m\u001b[39;49;00m\n",
      "    ):\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m    \u001b[39;49;00m\u001b[33m\"\"\"Get the values in the object store associated with the IDs.\u001b[39;49;00m\n",
      "    \u001b[33m\u001b[39;49;00m\n",
      "    \u001b[33m    Return the values from the local object store for object_refs. This\u001b[39;49;00m\n",
      "    \u001b[33m    will block until all the values for object_refs have been written to\u001b[39;49;00m\n",
      "    \u001b[33m    the local object store.\u001b[39;49;00m\n",
      "    \u001b[33m\u001b[39;49;00m\n",
      "    \u001b[33m    Args:\u001b[39;49;00m\n",
      "    \u001b[33m        object_refs: A list of the object refs\u001b[39;49;00m\n",
      "    \u001b[33m            whose values should be retrieved.\u001b[39;49;00m\n",
      "    \u001b[33m        timeout: The maximum amount of time in\u001b[39;49;00m\n",
      "    \u001b[33m            seconds to wait before returning.\u001b[39;49;00m\n",
      "    \u001b[33m        return_exceptions: If any of the objects deserialize to an\u001b[39;49;00m\n",
      "    \u001b[33m            Exception object, whether to return them as values in the\u001b[39;49;00m\n",
      "    \u001b[33m            returned list. If False, then the first found exception will be\u001b[39;49;00m\n",
      "    \u001b[33m            raised.\u001b[39;49;00m\n",
      "    \u001b[33m        skip_deserialization: If true, only the buffer will be released and\u001b[39;49;00m\n",
      "    \u001b[33m            the object associated with the buffer will not be deserialized.\u001b[39;49;00m\n",
      "    \u001b[33m    Returns:\u001b[39;49;00m\n",
      "    \u001b[33m        list: List of deserialized objects or None if skip_deserialization is True.\u001b[39;49;00m\n",
      "    \u001b[33m        bytes: UUID of the debugger breakpoint we should drop\u001b[39;49;00m\n",
      "    \u001b[33m            into or b\"\" if there is no breakpoint.\u001b[39;49;00m\n",
      "    \u001b[33m    \"\"\"\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[90m# Make sure that the values are object refs.\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[94mfor\u001b[39;49;00m object_ref \u001b[95min\u001b[39;49;00m object_refs:\u001b[90m\u001b[39;49;00m\n",
      "            \u001b[94mif\u001b[39;49;00m \u001b[95mnot\u001b[39;49;00m \u001b[96misinstance\u001b[39;49;00m(object_ref, ObjectRef):\u001b[90m\u001b[39;49;00m\n",
      "                \u001b[94mraise\u001b[39;49;00m \u001b[96mTypeError\u001b[39;49;00m(\u001b[90m\u001b[39;49;00m\n",
      "                    \u001b[33mf\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\u001b[33mAttempting to call `get` on the value \u001b[39;49;00m\u001b[33m{\u001b[39;49;00mobject_ref\u001b[33m}\u001b[39;49;00m\u001b[33m, \u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      "                    \u001b[33m\"\u001b[39;49;00m\u001b[33mwhich is not an ray.ObjectRef.\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      "                )\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "        timeout_ms = (\u001b[90m\u001b[39;49;00m\n",
      "            \u001b[96mint\u001b[39;49;00m(timeout * \u001b[94m1000\u001b[39;49;00m) \u001b[94mif\u001b[39;49;00m timeout \u001b[95mis\u001b[39;49;00m \u001b[95mnot\u001b[39;49;00m \u001b[94mNone\u001b[39;49;00m \u001b[95mand\u001b[39;49;00m timeout != -\u001b[94m1\u001b[39;49;00m \u001b[94melse\u001b[39;49;00m -\u001b[94m1\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      "        )\u001b[90m\u001b[39;49;00m\n",
      "        data_metadata_pairs: List[\u001b[90m\u001b[39;49;00m\n",
      "            Tuple[ray._raylet.Buffer, \u001b[96mbytes\u001b[39;49;00m]\u001b[90m\u001b[39;49;00m\n",
      "        ] = \u001b[96mself\u001b[39;49;00m.core_worker.get_objects(\u001b[90m\u001b[39;49;00m\n",
      "            object_refs,\u001b[90m\u001b[39;49;00m\n",
      "            timeout_ms,\u001b[90m\u001b[39;49;00m\n",
      "        )\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "        debugger_breakpoint = \u001b[33mb\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[94mfor\u001b[39;49;00m data, metadata \u001b[95min\u001b[39;49;00m data_metadata_pairs:\u001b[90m\u001b[39;49;00m\n",
      "            \u001b[94mif\u001b[39;49;00m metadata:\u001b[90m\u001b[39;49;00m\n",
      "                metadata_fields = metadata.split(\u001b[33mb\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\u001b[33m,\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\u001b[90m\u001b[39;49;00m\n",
      "                \u001b[94mif\u001b[39;49;00m \u001b[96mlen\u001b[39;49;00m(metadata_fields) >= \u001b[94m2\u001b[39;49;00m \u001b[95mand\u001b[39;49;00m metadata_fields[\u001b[94m1\u001b[39;49;00m].startswith(\u001b[90m\u001b[39;49;00m\n",
      "                    ray_constants.OBJECT_METADATA_DEBUG_PREFIX\u001b[90m\u001b[39;49;00m\n",
      "                ):\u001b[90m\u001b[39;49;00m\n",
      "                    debugger_breakpoint = metadata_fields[\u001b[94m1\u001b[39;49;00m][\u001b[90m\u001b[39;49;00m\n",
      "                        \u001b[96mlen\u001b[39;49;00m(ray_constants.OBJECT_METADATA_DEBUG_PREFIX) :\u001b[90m\u001b[39;49;00m\n",
      "                    ]\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[94mif\u001b[39;49;00m skip_deserialization:\u001b[90m\u001b[39;49;00m\n",
      "            \u001b[94mreturn\u001b[39;49;00m \u001b[94mNone\u001b[39;49;00m, debugger_breakpoint\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "        values = \u001b[96mself\u001b[39;49;00m.deserialize_objects(data_metadata_pairs, object_refs)\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[94mif\u001b[39;49;00m \u001b[95mnot\u001b[39;49;00m return_exceptions:\u001b[90m\u001b[39;49;00m\n",
      "            \u001b[90m# Raise exceptions instead of returning them to the user.\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      "            \u001b[94mfor\u001b[39;49;00m i, value \u001b[95min\u001b[39;49;00m \u001b[96menumerate\u001b[39;49;00m(values):\u001b[90m\u001b[39;49;00m\n",
      "                \u001b[94mif\u001b[39;49;00m \u001b[96misinstance\u001b[39;49;00m(value, RayError):\u001b[90m\u001b[39;49;00m\n",
      "                    \u001b[94mif\u001b[39;49;00m \u001b[96misinstance\u001b[39;49;00m(value, ray.exceptions.ObjectLostError):\u001b[90m\u001b[39;49;00m\n",
      "                        global_worker.core_worker.dump_object_store_memory_usage()\u001b[90m\u001b[39;49;00m\n",
      "                    \u001b[94mif\u001b[39;49;00m \u001b[96misinstance\u001b[39;49;00m(value, RayTaskError):\u001b[90m\u001b[39;49;00m\n",
      ">                       \u001b[94mraise\u001b[39;49;00m value.as_instanceof_cause()\u001b[90m\u001b[39;49;00m\n",
      "\u001b[1m\u001b[31mE                       ray.exceptions.RayTaskError(RuntimeError): \u001b[36mray::Worker.__ray_call__()\u001b[39m (pid=420589, ip=128.208.3.124, actor_id=7b76b108b090ec43c422bbbf01000000, repr=<test_execution_schedule_gpu.Worker object at 0x73b49a05a100>)\u001b[0m\n",
      "\u001b[1m\u001b[31mE                         File \"/home/wxdeng/stbr-min/python/ray/actor.py\", line 1742, in __ray_call__\u001b[0m\n",
      "\u001b[1m\u001b[31mE                           return fn(self, *args, **kwargs)\u001b[0m\n",
      "\u001b[1m\u001b[31mE                         File \"/home/wxdeng/stbr-min/python/ray/experimental/channel/torch_tensor_nccl_channel.py\", line 663, in _do_init_communicator\u001b[0m\n",
      "\u001b[1m\u001b[31mE                           torch.cuda.current_stream().cuda_stream,\u001b[0m\n",
      "\u001b[1m\u001b[31mE                         File \"/home/wxdeng/miniconda3/envs/coll-sched-0512-py39/lib/python3.9/site-packages/torch/cuda/__init__.py\", line 1065, in current_stream\u001b[0m\n",
      "\u001b[1m\u001b[31mE                           _lazy_init()\u001b[0m\n",
      "\u001b[1m\u001b[31mE                         File \"/home/wxdeng/miniconda3/envs/coll-sched-0512-py39/lib/python3.9/site-packages/torch/cuda/__init__.py\", line 372, in _lazy_init\u001b[0m\n",
      "\u001b[1m\u001b[31mE                           torch._C._cuda_init()\u001b[0m\n",
      "\u001b[1m\u001b[31mE                       RuntimeError: No CUDA GPUs are available\u001b[0m\n",
      "\n",
      "\u001b[1m\u001b[31m../../../_private/worker.py\u001b[0m:930: RayTaskError(RuntimeError)\n",
      "\u001b[31m\u001b[1m___________ test_overlap_gpu_communication[True-ray_start_regular0] ____________\u001b[0m\n",
      "\n",
      "ray_start_regular = RayContext(dashboard_url='127.0.0.1:8265', python_version='3.9.22', ray_version='3.0.0.dev0', ray_commit='{{RAY_COMMIT_SHA}}')\n",
      "overlap_gpu_communication = True\n",
      "\n",
      "    \u001b[0m\u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mray_start_regular\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [{\u001b[33m\"\u001b[39;49;00m\u001b[33mnum_gpus\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m: \u001b[94m3\u001b[39;49;00m}], indirect=\u001b[94mTrue\u001b[39;49;00m)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33moverlap_gpu_communication\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94mTrue\u001b[39;49;00m, \u001b[94mFalse\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mtest_overlap_gpu_communication\u001b[39;49;00m(ray_start_regular, overlap_gpu_communication):\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[94mif\u001b[39;49;00m \u001b[95mnot\u001b[39;49;00m USE_GPU:\u001b[90m\u001b[39;49;00m\n",
      "            pytest.skip(\u001b[33m\"\u001b[39;49;00m\u001b[33mNCCL tests require GPUs\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "        sender1 = Worker.remote()\u001b[90m\u001b[39;49;00m\n",
      "        sender2 = Worker.remote()\u001b[90m\u001b[39;49;00m\n",
      "        receiver = Worker.remote()\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "        shape = (\u001b[94m10000\u001b[39;49;00m,)\u001b[90m\u001b[39;49;00m\n",
      "        dtype = torch.float16\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "        \u001b[94mwith\u001b[39;49;00m InputNode() \u001b[94mas\u001b[39;49;00m inp:\u001b[90m\u001b[39;49;00m\n",
      "            branch1 = sender1.send.bind(shape, dtype, inp)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "            branch1 = branch1.with_tensor_transport(\u001b[90m\u001b[39;49;00m\n",
      "                transport=\u001b[33m\"\u001b[39;49;00m\u001b[33mnccl\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, _static_shape=\u001b[94mTrue\u001b[39;49;00m, _direct_return=\u001b[94mTrue\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      "            )\u001b[90m\u001b[39;49;00m\n",
      "            branch1 = receiver.recv.bind(branch1)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "            branch2 = sender2.send.bind(shape, dtype, inp)\u001b[90m\u001b[39;49;00m\n",
      "            branch2 = branch2.with_tensor_transport(\u001b[90m\u001b[39;49;00m\n",
      "                transport=\u001b[33m\"\u001b[39;49;00m\u001b[33mnccl\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, _static_shape=\u001b[94mTrue\u001b[39;49;00m, _direct_return=\u001b[94mTrue\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      "            )\u001b[90m\u001b[39;49;00m\n",
      "            branch2 = receiver.recv.bind(branch2)\u001b[90m\u001b[39;49;00m\n",
      "            dag = MultiOutputNode([branch1, branch2])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "        \u001b[90m# Test normal execution.\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      ">       compiled_dag = dag.experimental_compile(\u001b[90m\u001b[39;49;00m\n",
      "            _overlap_gpu_communication=overlap_gpu_communication\u001b[90m\u001b[39;49;00m\n",
      "        )\u001b[90m\u001b[39;49;00m\n",
      "\n",
      "\u001b[1m\u001b[31mtest_execution_schedule_gpu.py\u001b[0m:405: \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\u001b[1m\u001b[31m../../dag_node.py\u001b[0m:340: in experimental_compile\n",
      "    \u001b[0m\u001b[94mreturn\u001b[39;49;00m build_compiled_dag_from_ray_dag(\u001b[90m\u001b[39;49;00m\n",
      "\u001b[1m\u001b[31m../../compiled_dag_node.py\u001b[0m:3315: in build_compiled_dag_from_ray_dag\n",
      "    \u001b[0mcompiled_dag._get_or_compile()\u001b[90m\u001b[39;49;00m\n",
      "\u001b[1m\u001b[31m../../compiled_dag_node.py\u001b[0m:1564: in _get_or_compile\n",
      "    \u001b[0m\u001b[96mself\u001b[39;49;00m._preprocess()\u001b[90m\u001b[39;49;00m\n",
      "\u001b[1m\u001b[31m../../compiled_dag_node.py\u001b[0m:1299: in _preprocess\n",
      "    \u001b[0m\u001b[96mself\u001b[39;49;00m._init_communicators()\u001b[90m\u001b[39;49;00m\n",
      "\u001b[1m\u001b[31m../../compiled_dag_node.py\u001b[0m:1357: in _init_communicators\n",
      "    \u001b[0mp2p_communicator_id = _init_communicator(\u001b[90m\u001b[39;49;00m\n",
      "\u001b[1m\u001b[31m../../../experimental/channel/torch_tensor_nccl_channel.py\u001b[0m:790: in _init_communicator\n",
      "    \u001b[0mray.get(init_tasks, timeout=\u001b[94m30\u001b[39;49;00m)\u001b[90m\u001b[39;49;00m\n",
      "\u001b[1m\u001b[31m../../../_private/auto_init_hook.py\u001b[0m:21: in auto_init_wrapper\n",
      "    \u001b[0m\u001b[94mreturn\u001b[39;49;00m fn(*args, **kwargs)\u001b[90m\u001b[39;49;00m\n",
      "\u001b[1m\u001b[31m../../../_private/client_mode_hook.py\u001b[0m:103: in wrapper\n",
      "    \u001b[0m\u001b[94mreturn\u001b[39;49;00m func(*args, **kwargs)\u001b[90m\u001b[39;49;00m\n",
      "\u001b[1m\u001b[31m../../../_private/worker.py\u001b[0m:2820: in get\n",
      "    \u001b[0mvalues, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\u001b[90m\u001b[39;49;00m\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\n",
      "self = <ray._private.worker.Worker object at 0x74e02fddca90>\n",
      "object_refs = [ObjectRef(6710354e78dd8e3af6f411bac839ba6621379d990100000001000000), ObjectRef(a2646843e77817382cb5d03b6e4e6d777b196aa80100000001000000), ObjectRef(2f7fdef4e2df3f2b3e3d752bed46aacd9ae35ed10100000001000000)]\n",
      "timeout = 30, return_exceptions = False, skip_deserialization = False\n",
      "\n",
      "    \u001b[0m\u001b[94mdef\u001b[39;49;00m \u001b[92mget_objects\u001b[39;49;00m(\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[96mself\u001b[39;49;00m,\u001b[90m\u001b[39;49;00m\n",
      "        object_refs: \u001b[96mlist\u001b[39;49;00m,\u001b[90m\u001b[39;49;00m\n",
      "        timeout: Optional[\u001b[96mfloat\u001b[39;49;00m] = \u001b[94mNone\u001b[39;49;00m,\u001b[90m\u001b[39;49;00m\n",
      "        return_exceptions: \u001b[96mbool\u001b[39;49;00m = \u001b[94mFalse\u001b[39;49;00m,\u001b[90m\u001b[39;49;00m\n",
      "        skip_deserialization: \u001b[96mbool\u001b[39;49;00m = \u001b[94mFalse\u001b[39;49;00m,\u001b[90m\u001b[39;49;00m\n",
      "    ):\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m    \u001b[39;49;00m\u001b[33m\"\"\"Get the values in the object store associated with the IDs.\u001b[39;49;00m\n",
      "    \u001b[33m\u001b[39;49;00m\n",
      "    \u001b[33m    Return the values from the local object store for object_refs. This\u001b[39;49;00m\n",
      "    \u001b[33m    will block until all the values for object_refs have been written to\u001b[39;49;00m\n",
      "    \u001b[33m    the local object store.\u001b[39;49;00m\n",
      "    \u001b[33m\u001b[39;49;00m\n",
      "    \u001b[33m    Args:\u001b[39;49;00m\n",
      "    \u001b[33m        object_refs: A list of the object refs\u001b[39;49;00m\n",
      "    \u001b[33m            whose values should be retrieved.\u001b[39;49;00m\n",
      "    \u001b[33m        timeout: The maximum amount of time in\u001b[39;49;00m\n",
      "    \u001b[33m            seconds to wait before returning.\u001b[39;49;00m\n",
      "    \u001b[33m        return_exceptions: If any of the objects deserialize to an\u001b[39;49;00m\n",
      "    \u001b[33m            Exception object, whether to return them as values in the\u001b[39;49;00m\n",
      "    \u001b[33m            returned list. If False, then the first found exception will be\u001b[39;49;00m\n",
      "    \u001b[33m            raised.\u001b[39;49;00m\n",
      "    \u001b[33m        skip_deserialization: If true, only the buffer will be released and\u001b[39;49;00m\n",
      "    \u001b[33m            the object associated with the buffer will not be deserialized.\u001b[39;49;00m\n",
      "    \u001b[33m    Returns:\u001b[39;49;00m\n",
      "    \u001b[33m        list: List of deserialized objects or None if skip_deserialization is True.\u001b[39;49;00m\n",
      "    \u001b[33m        bytes: UUID of the debugger breakpoint we should drop\u001b[39;49;00m\n",
      "    \u001b[33m            into or b\"\" if there is no breakpoint.\u001b[39;49;00m\n",
      "    \u001b[33m    \"\"\"\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[90m# Make sure that the values are object refs.\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[94mfor\u001b[39;49;00m object_ref \u001b[95min\u001b[39;49;00m object_refs:\u001b[90m\u001b[39;49;00m\n",
      "            \u001b[94mif\u001b[39;49;00m \u001b[95mnot\u001b[39;49;00m \u001b[96misinstance\u001b[39;49;00m(object_ref, ObjectRef):\u001b[90m\u001b[39;49;00m\n",
      "                \u001b[94mraise\u001b[39;49;00m \u001b[96mTypeError\u001b[39;49;00m(\u001b[90m\u001b[39;49;00m\n",
      "                    \u001b[33mf\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\u001b[33mAttempting to call `get` on the value \u001b[39;49;00m\u001b[33m{\u001b[39;49;00mobject_ref\u001b[33m}\u001b[39;49;00m\u001b[33m, \u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      "                    \u001b[33m\"\u001b[39;49;00m\u001b[33mwhich is not an ray.ObjectRef.\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      "                )\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "        timeout_ms = (\u001b[90m\u001b[39;49;00m\n",
      "            \u001b[96mint\u001b[39;49;00m(timeout * \u001b[94m1000\u001b[39;49;00m) \u001b[94mif\u001b[39;49;00m timeout \u001b[95mis\u001b[39;49;00m \u001b[95mnot\u001b[39;49;00m \u001b[94mNone\u001b[39;49;00m \u001b[95mand\u001b[39;49;00m timeout != -\u001b[94m1\u001b[39;49;00m \u001b[94melse\u001b[39;49;00m -\u001b[94m1\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      "        )\u001b[90m\u001b[39;49;00m\n",
      "        data_metadata_pairs: List[\u001b[90m\u001b[39;49;00m\n",
      "            Tuple[ray._raylet.Buffer, \u001b[96mbytes\u001b[39;49;00m]\u001b[90m\u001b[39;49;00m\n",
      "        ] = \u001b[96mself\u001b[39;49;00m.core_worker.get_objects(\u001b[90m\u001b[39;49;00m\n",
      "            object_refs,\u001b[90m\u001b[39;49;00m\n",
      "            timeout_ms,\u001b[90m\u001b[39;49;00m\n",
      "        )\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "        debugger_breakpoint = \u001b[33mb\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[94mfor\u001b[39;49;00m data, metadata \u001b[95min\u001b[39;49;00m data_metadata_pairs:\u001b[90m\u001b[39;49;00m\n",
      "            \u001b[94mif\u001b[39;49;00m metadata:\u001b[90m\u001b[39;49;00m\n",
      "                metadata_fields = metadata.split(\u001b[33mb\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\u001b[33m,\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\u001b[90m\u001b[39;49;00m\n",
      "                \u001b[94mif\u001b[39;49;00m \u001b[96mlen\u001b[39;49;00m(metadata_fields) >= \u001b[94m2\u001b[39;49;00m \u001b[95mand\u001b[39;49;00m metadata_fields[\u001b[94m1\u001b[39;49;00m].startswith(\u001b[90m\u001b[39;49;00m\n",
      "                    ray_constants.OBJECT_METADATA_DEBUG_PREFIX\u001b[90m\u001b[39;49;00m\n",
      "                ):\u001b[90m\u001b[39;49;00m\n",
      "                    debugger_breakpoint = metadata_fields[\u001b[94m1\u001b[39;49;00m][\u001b[90m\u001b[39;49;00m\n",
      "                        \u001b[96mlen\u001b[39;49;00m(ray_constants.OBJECT_METADATA_DEBUG_PREFIX) :\u001b[90m\u001b[39;49;00m\n",
      "                    ]\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[94mif\u001b[39;49;00m skip_deserialization:\u001b[90m\u001b[39;49;00m\n",
      "            \u001b[94mreturn\u001b[39;49;00m \u001b[94mNone\u001b[39;49;00m, debugger_breakpoint\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "        values = \u001b[96mself\u001b[39;49;00m.deserialize_objects(data_metadata_pairs, object_refs)\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[94mif\u001b[39;49;00m \u001b[95mnot\u001b[39;49;00m return_exceptions:\u001b[90m\u001b[39;49;00m\n",
      "            \u001b[90m# Raise exceptions instead of returning them to the user.\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      "            \u001b[94mfor\u001b[39;49;00m i, value \u001b[95min\u001b[39;49;00m \u001b[96menumerate\u001b[39;49;00m(values):\u001b[90m\u001b[39;49;00m\n",
      "                \u001b[94mif\u001b[39;49;00m \u001b[96misinstance\u001b[39;49;00m(value, RayError):\u001b[90m\u001b[39;49;00m\n",
      "                    \u001b[94mif\u001b[39;49;00m \u001b[96misinstance\u001b[39;49;00m(value, ray.exceptions.ObjectLostError):\u001b[90m\u001b[39;49;00m\n",
      "                        global_worker.core_worker.dump_object_store_memory_usage()\u001b[90m\u001b[39;49;00m\n",
      "                    \u001b[94mif\u001b[39;49;00m \u001b[96misinstance\u001b[39;49;00m(value, RayTaskError):\u001b[90m\u001b[39;49;00m\n",
      ">                       \u001b[94mraise\u001b[39;49;00m value.as_instanceof_cause()\u001b[90m\u001b[39;49;00m\n",
      "\u001b[1m\u001b[31mE                       ray.exceptions.RayTaskError(RuntimeError): \u001b[36mray::Worker.__ray_call__()\u001b[39m (pid=421827, ip=128.208.3.124, actor_id=3e3d752bed46aacd9ae35ed101000000, repr=<test_execution_schedule_gpu.Worker object at 0x7b700ccda280>)\u001b[0m\n",
      "\u001b[1m\u001b[31mE                         File \"/home/wxdeng/stbr-min/python/ray/actor.py\", line 1742, in __ray_call__\u001b[0m\n",
      "\u001b[1m\u001b[31mE                           return fn(self, *args, **kwargs)\u001b[0m\n",
      "\u001b[1m\u001b[31mE                         File \"/home/wxdeng/stbr-min/python/ray/experimental/channel/torch_tensor_nccl_channel.py\", line 663, in _do_init_communicator\u001b[0m\n",
      "\u001b[1m\u001b[31mE                           torch.cuda.current_stream().cuda_stream,\u001b[0m\n",
      "\u001b[1m\u001b[31mE                         File \"/home/wxdeng/miniconda3/envs/coll-sched-0512-py39/lib/python3.9/site-packages/torch/cuda/__init__.py\", line 1065, in current_stream\u001b[0m\n",
      "\u001b[1m\u001b[31mE                           _lazy_init()\u001b[0m\n",
      "\u001b[1m\u001b[31mE                         File \"/home/wxdeng/miniconda3/envs/coll-sched-0512-py39/lib/python3.9/site-packages/torch/cuda/__init__.py\", line 372, in _lazy_init\u001b[0m\n",
      "\u001b[1m\u001b[31mE                           torch._C._cuda_init()\u001b[0m\n",
      "\u001b[1m\u001b[31mE                       RuntimeError: No CUDA GPUs are available\u001b[0m\n",
      "\n",
      "\u001b[1m\u001b[31m../../../_private/worker.py\u001b[0m:930: RayTaskError(RuntimeError)\n",
      "\u001b[31m\u001b[1m___________ test_overlap_gpu_communication[False-ray_start_regular0] ___________\u001b[0m\n",
      "\n",
      "ray_start_regular = RayContext(dashboard_url='127.0.0.1:8265', python_version='3.9.22', ray_version='3.0.0.dev0', ray_commit='{{RAY_COMMIT_SHA}}')\n",
      "overlap_gpu_communication = False\n",
      "\n",
      "    \u001b[0m\u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mray_start_regular\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [{\u001b[33m\"\u001b[39;49;00m\u001b[33mnum_gpus\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m: \u001b[94m3\u001b[39;49;00m}], indirect=\u001b[94mTrue\u001b[39;49;00m)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33moverlap_gpu_communication\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94mTrue\u001b[39;49;00m, \u001b[94mFalse\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mtest_overlap_gpu_communication\u001b[39;49;00m(ray_start_regular, overlap_gpu_communication):\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[94mif\u001b[39;49;00m \u001b[95mnot\u001b[39;49;00m USE_GPU:\u001b[90m\u001b[39;49;00m\n",
      "            pytest.skip(\u001b[33m\"\u001b[39;49;00m\u001b[33mNCCL tests require GPUs\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "        sender1 = Worker.remote()\u001b[90m\u001b[39;49;00m\n",
      "        sender2 = Worker.remote()\u001b[90m\u001b[39;49;00m\n",
      "        receiver = Worker.remote()\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "        shape = (\u001b[94m10000\u001b[39;49;00m,)\u001b[90m\u001b[39;49;00m\n",
      "        dtype = torch.float16\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "        \u001b[94mwith\u001b[39;49;00m InputNode() \u001b[94mas\u001b[39;49;00m inp:\u001b[90m\u001b[39;49;00m\n",
      "            branch1 = sender1.send.bind(shape, dtype, inp)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "            branch1 = branch1.with_tensor_transport(\u001b[90m\u001b[39;49;00m\n",
      "                transport=\u001b[33m\"\u001b[39;49;00m\u001b[33mnccl\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, _static_shape=\u001b[94mTrue\u001b[39;49;00m, _direct_return=\u001b[94mTrue\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      "            )\u001b[90m\u001b[39;49;00m\n",
      "            branch1 = receiver.recv.bind(branch1)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "            branch2 = sender2.send.bind(shape, dtype, inp)\u001b[90m\u001b[39;49;00m\n",
      "            branch2 = branch2.with_tensor_transport(\u001b[90m\u001b[39;49;00m\n",
      "                transport=\u001b[33m\"\u001b[39;49;00m\u001b[33mnccl\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, _static_shape=\u001b[94mTrue\u001b[39;49;00m, _direct_return=\u001b[94mTrue\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      "            )\u001b[90m\u001b[39;49;00m\n",
      "            branch2 = receiver.recv.bind(branch2)\u001b[90m\u001b[39;49;00m\n",
      "            dag = MultiOutputNode([branch1, branch2])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "        \u001b[90m# Test normal execution.\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      ">       compiled_dag = dag.experimental_compile(\u001b[90m\u001b[39;49;00m\n",
      "            _overlap_gpu_communication=overlap_gpu_communication\u001b[90m\u001b[39;49;00m\n",
      "        )\u001b[90m\u001b[39;49;00m\n",
      "\n",
      "\u001b[1m\u001b[31mtest_execution_schedule_gpu.py\u001b[0m:405: \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\u001b[1m\u001b[31m../../dag_node.py\u001b[0m:340: in experimental_compile\n",
      "    \u001b[0m\u001b[94mreturn\u001b[39;49;00m build_compiled_dag_from_ray_dag(\u001b[90m\u001b[39;49;00m\n",
      "\u001b[1m\u001b[31m../../compiled_dag_node.py\u001b[0m:3315: in build_compiled_dag_from_ray_dag\n",
      "    \u001b[0mcompiled_dag._get_or_compile()\u001b[90m\u001b[39;49;00m\n",
      "\u001b[1m\u001b[31m../../compiled_dag_node.py\u001b[0m:1564: in _get_or_compile\n",
      "    \u001b[0m\u001b[96mself\u001b[39;49;00m._preprocess()\u001b[90m\u001b[39;49;00m\n",
      "\u001b[1m\u001b[31m../../compiled_dag_node.py\u001b[0m:1299: in _preprocess\n",
      "    \u001b[0m\u001b[96mself\u001b[39;49;00m._init_communicators()\u001b[90m\u001b[39;49;00m\n",
      "\u001b[1m\u001b[31m../../compiled_dag_node.py\u001b[0m:1357: in _init_communicators\n",
      "    \u001b[0mp2p_communicator_id = _init_communicator(\u001b[90m\u001b[39;49;00m\n",
      "\u001b[1m\u001b[31m../../../experimental/channel/torch_tensor_nccl_channel.py\u001b[0m:790: in _init_communicator\n",
      "    \u001b[0mray.get(init_tasks, timeout=\u001b[94m30\u001b[39;49;00m)\u001b[90m\u001b[39;49;00m\n",
      "\u001b[1m\u001b[31m../../../_private/auto_init_hook.py\u001b[0m:21: in auto_init_wrapper\n",
      "    \u001b[0m\u001b[94mreturn\u001b[39;49;00m fn(*args, **kwargs)\u001b[90m\u001b[39;49;00m\n",
      "\u001b[1m\u001b[31m../../../_private/client_mode_hook.py\u001b[0m:103: in wrapper\n",
      "    \u001b[0m\u001b[94mreturn\u001b[39;49;00m func(*args, **kwargs)\u001b[90m\u001b[39;49;00m\n",
      "\u001b[1m\u001b[31m../../../_private/worker.py\u001b[0m:2820: in get\n",
      "    \u001b[0mvalues, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\u001b[90m\u001b[39;49;00m\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\n",
      "self = <ray._private.worker.Worker object at 0x74e02fddca90>\n",
      "object_refs = [ObjectRef(0fbe19ab9c441a0c6b435e5e859d90f5c9882a9b0100000001000000), ObjectRef(21619651c3041122a7ed62182b0670a406fb65fa0100000001000000), ObjectRef(708fb13a673cee7e7dd980fd8c18871c6ad9e4ac0100000001000000)]\n",
      "timeout = 30, return_exceptions = False, skip_deserialization = False\n",
      "\n",
      "    \u001b[0m\u001b[94mdef\u001b[39;49;00m \u001b[92mget_objects\u001b[39;49;00m(\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[96mself\u001b[39;49;00m,\u001b[90m\u001b[39;49;00m\n",
      "        object_refs: \u001b[96mlist\u001b[39;49;00m,\u001b[90m\u001b[39;49;00m\n",
      "        timeout: Optional[\u001b[96mfloat\u001b[39;49;00m] = \u001b[94mNone\u001b[39;49;00m,\u001b[90m\u001b[39;49;00m\n",
      "        return_exceptions: \u001b[96mbool\u001b[39;49;00m = \u001b[94mFalse\u001b[39;49;00m,\u001b[90m\u001b[39;49;00m\n",
      "        skip_deserialization: \u001b[96mbool\u001b[39;49;00m = \u001b[94mFalse\u001b[39;49;00m,\u001b[90m\u001b[39;49;00m\n",
      "    ):\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m    \u001b[39;49;00m\u001b[33m\"\"\"Get the values in the object store associated with the IDs.\u001b[39;49;00m\n",
      "    \u001b[33m\u001b[39;49;00m\n",
      "    \u001b[33m    Return the values from the local object store for object_refs. This\u001b[39;49;00m\n",
      "    \u001b[33m    will block until all the values for object_refs have been written to\u001b[39;49;00m\n",
      "    \u001b[33m    the local object store.\u001b[39;49;00m\n",
      "    \u001b[33m\u001b[39;49;00m\n",
      "    \u001b[33m    Args:\u001b[39;49;00m\n",
      "    \u001b[33m        object_refs: A list of the object refs\u001b[39;49;00m\n",
      "    \u001b[33m            whose values should be retrieved.\u001b[39;49;00m\n",
      "    \u001b[33m        timeout: The maximum amount of time in\u001b[39;49;00m\n",
      "    \u001b[33m            seconds to wait before returning.\u001b[39;49;00m\n",
      "    \u001b[33m        return_exceptions: If any of the objects deserialize to an\u001b[39;49;00m\n",
      "    \u001b[33m            Exception object, whether to return them as values in the\u001b[39;49;00m\n",
      "    \u001b[33m            returned list. If False, then the first found exception will be\u001b[39;49;00m\n",
      "    \u001b[33m            raised.\u001b[39;49;00m\n",
      "    \u001b[33m        skip_deserialization: If true, only the buffer will be released and\u001b[39;49;00m\n",
      "    \u001b[33m            the object associated with the buffer will not be deserialized.\u001b[39;49;00m\n",
      "    \u001b[33m    Returns:\u001b[39;49;00m\n",
      "    \u001b[33m        list: List of deserialized objects or None if skip_deserialization is True.\u001b[39;49;00m\n",
      "    \u001b[33m        bytes: UUID of the debugger breakpoint we should drop\u001b[39;49;00m\n",
      "    \u001b[33m            into or b\"\" if there is no breakpoint.\u001b[39;49;00m\n",
      "    \u001b[33m    \"\"\"\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[90m# Make sure that the values are object refs.\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[94mfor\u001b[39;49;00m object_ref \u001b[95min\u001b[39;49;00m object_refs:\u001b[90m\u001b[39;49;00m\n",
      "            \u001b[94mif\u001b[39;49;00m \u001b[95mnot\u001b[39;49;00m \u001b[96misinstance\u001b[39;49;00m(object_ref, ObjectRef):\u001b[90m\u001b[39;49;00m\n",
      "                \u001b[94mraise\u001b[39;49;00m \u001b[96mTypeError\u001b[39;49;00m(\u001b[90m\u001b[39;49;00m\n",
      "                    \u001b[33mf\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\u001b[33mAttempting to call `get` on the value \u001b[39;49;00m\u001b[33m{\u001b[39;49;00mobject_ref\u001b[33m}\u001b[39;49;00m\u001b[33m, \u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      "                    \u001b[33m\"\u001b[39;49;00m\u001b[33mwhich is not an ray.ObjectRef.\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      "                )\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "        timeout_ms = (\u001b[90m\u001b[39;49;00m\n",
      "            \u001b[96mint\u001b[39;49;00m(timeout * \u001b[94m1000\u001b[39;49;00m) \u001b[94mif\u001b[39;49;00m timeout \u001b[95mis\u001b[39;49;00m \u001b[95mnot\u001b[39;49;00m \u001b[94mNone\u001b[39;49;00m \u001b[95mand\u001b[39;49;00m timeout != -\u001b[94m1\u001b[39;49;00m \u001b[94melse\u001b[39;49;00m -\u001b[94m1\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      "        )\u001b[90m\u001b[39;49;00m\n",
      "        data_metadata_pairs: List[\u001b[90m\u001b[39;49;00m\n",
      "            Tuple[ray._raylet.Buffer, \u001b[96mbytes\u001b[39;49;00m]\u001b[90m\u001b[39;49;00m\n",
      "        ] = \u001b[96mself\u001b[39;49;00m.core_worker.get_objects(\u001b[90m\u001b[39;49;00m\n",
      "            object_refs,\u001b[90m\u001b[39;49;00m\n",
      "            timeout_ms,\u001b[90m\u001b[39;49;00m\n",
      "        )\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "        debugger_breakpoint = \u001b[33mb\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[94mfor\u001b[39;49;00m data, metadata \u001b[95min\u001b[39;49;00m data_metadata_pairs:\u001b[90m\u001b[39;49;00m\n",
      "            \u001b[94mif\u001b[39;49;00m metadata:\u001b[90m\u001b[39;49;00m\n",
      "                metadata_fields = metadata.split(\u001b[33mb\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\u001b[33m,\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\u001b[90m\u001b[39;49;00m\n",
      "                \u001b[94mif\u001b[39;49;00m \u001b[96mlen\u001b[39;49;00m(metadata_fields) >= \u001b[94m2\u001b[39;49;00m \u001b[95mand\u001b[39;49;00m metadata_fields[\u001b[94m1\u001b[39;49;00m].startswith(\u001b[90m\u001b[39;49;00m\n",
      "                    ray_constants.OBJECT_METADATA_DEBUG_PREFIX\u001b[90m\u001b[39;49;00m\n",
      "                ):\u001b[90m\u001b[39;49;00m\n",
      "                    debugger_breakpoint = metadata_fields[\u001b[94m1\u001b[39;49;00m][\u001b[90m\u001b[39;49;00m\n",
      "                        \u001b[96mlen\u001b[39;49;00m(ray_constants.OBJECT_METADATA_DEBUG_PREFIX) :\u001b[90m\u001b[39;49;00m\n",
      "                    ]\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[94mif\u001b[39;49;00m skip_deserialization:\u001b[90m\u001b[39;49;00m\n",
      "            \u001b[94mreturn\u001b[39;49;00m \u001b[94mNone\u001b[39;49;00m, debugger_breakpoint\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "        values = \u001b[96mself\u001b[39;49;00m.deserialize_objects(data_metadata_pairs, object_refs)\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[94mif\u001b[39;49;00m \u001b[95mnot\u001b[39;49;00m return_exceptions:\u001b[90m\u001b[39;49;00m\n",
      "            \u001b[90m# Raise exceptions instead of returning them to the user.\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      "            \u001b[94mfor\u001b[39;49;00m i, value \u001b[95min\u001b[39;49;00m \u001b[96menumerate\u001b[39;49;00m(values):\u001b[90m\u001b[39;49;00m\n",
      "                \u001b[94mif\u001b[39;49;00m \u001b[96misinstance\u001b[39;49;00m(value, RayError):\u001b[90m\u001b[39;49;00m\n",
      "                    \u001b[94mif\u001b[39;49;00m \u001b[96misinstance\u001b[39;49;00m(value, ray.exceptions.ObjectLostError):\u001b[90m\u001b[39;49;00m\n",
      "                        global_worker.core_worker.dump_object_store_memory_usage()\u001b[90m\u001b[39;49;00m\n",
      "                    \u001b[94mif\u001b[39;49;00m \u001b[96misinstance\u001b[39;49;00m(value, RayTaskError):\u001b[90m\u001b[39;49;00m\n",
      ">                       \u001b[94mraise\u001b[39;49;00m value.as_instanceof_cause()\u001b[90m\u001b[39;49;00m\n",
      "\u001b[1m\u001b[31mE                       ray.exceptions.RayTaskError(RuntimeError): \u001b[36mray::Worker.__ray_call__()\u001b[39m (pid=423071, ip=128.208.3.124, actor_id=7dd980fd8c18871c6ad9e4ac01000000, repr=<test_execution_schedule_gpu.Worker object at 0x71dbc3f9a190>)\u001b[0m\n",
      "\u001b[1m\u001b[31mE                         File \"/home/wxdeng/stbr-min/python/ray/actor.py\", line 1742, in __ray_call__\u001b[0m\n",
      "\u001b[1m\u001b[31mE                           return fn(self, *args, **kwargs)\u001b[0m\n",
      "\u001b[1m\u001b[31mE                         File \"/home/wxdeng/stbr-min/python/ray/experimental/channel/torch_tensor_nccl_channel.py\", line 663, in _do_init_communicator\u001b[0m\n",
      "\u001b[1m\u001b[31mE                           torch.cuda.current_stream().cuda_stream,\u001b[0m\n",
      "\u001b[1m\u001b[31mE                         File \"/home/wxdeng/miniconda3/envs/coll-sched-0512-py39/lib/python3.9/site-packages/torch/cuda/__init__.py\", line 1065, in current_stream\u001b[0m\n",
      "\u001b[1m\u001b[31mE                           _lazy_init()\u001b[0m\n",
      "\u001b[1m\u001b[31mE                         File \"/home/wxdeng/miniconda3/envs/coll-sched-0512-py39/lib/python3.9/site-packages/torch/cuda/__init__.py\", line 372, in _lazy_init\u001b[0m\n",
      "\u001b[1m\u001b[31mE                           torch._C._cuda_init()\u001b[0m\n",
      "\u001b[1m\u001b[31mE                       RuntimeError: No CUDA GPUs are available\u001b[0m\n",
      "\n",
      "\u001b[1m\u001b[31m../../../_private/worker.py\u001b[0m:930: RayTaskError(RuntimeError)\n",
      "\u001b[36m\u001b[1m=========================== short test summary info ============================\u001b[0m\n",
      "\u001b[31mFAILED\u001b[0m test_execution_schedule_gpu.py::\u001b[1mtest_simulate_pp_2workers_2batches_1f1b[True-ray_start_regular0]\u001b[0m - assert 3 == 2\n",
      "\u001b[31mFAILED\u001b[0m test_execution_schedule_gpu.py::\u001b[1mtest_simulate_pp_2workers_2batches_1f1b[False-ray_start_regular0]\u001b[0m - assert 3 == 2\n",
      "\u001b[31mFAILED\u001b[0m test_execution_schedule_gpu.py::\u001b[1mtest_simulate_pp_4workers_8batches_1f1b[ray_start_regular0]\u001b[0m - ray.exceptions.RayTaskError(RuntimeError): \u001b[36mray::Worker.__ray_call__()\u001b[39m (pid=416824, ip=128.208.3.124, actor_id=357474c514730ae97f1e9a4701000000, repr=<test_execution_schedule_gpu.Worker object at 0x75fca80da7c0>)\n",
      "\u001b[31mFAILED\u001b[0m test_execution_schedule_gpu.py::\u001b[1mtest_three_actors_with_nccl_1[ray_start_regular0]\u001b[0m - ray.exceptions.RayTaskError(RuntimeError): \u001b[36mray::Worker.__ray_call__()\u001b[39m (pid=418108, ip=128.208.3.124, actor_id=d8cd084a8b9a1cab27e98ef001000000, repr=<test_execution_schedule_gpu.Worker object at 0x76f56d79a190>)\n",
      "\u001b[31mFAILED\u001b[0m test_execution_schedule_gpu.py::\u001b[1mtest_three_actors_with_nccl_2[True-ray_start_regular0]\u001b[0m - ray.exceptions.RayTaskError(RuntimeError): \u001b[36mray::Worker.__ray_call__()\u001b[39m (pid=419352, ip=128.208.3.124, actor_id=e76fd021efb1266bfc4680d301000000, repr=<test_execution_schedule_gpu.Worker object at 0x7c35bd6d9100>)\n",
      "\u001b[31mFAILED\u001b[0m test_execution_schedule_gpu.py::\u001b[1mtest_three_actors_with_nccl_2[False-ray_start_regular0]\u001b[0m - ray.exceptions.RayTaskError(RuntimeError): \u001b[36mray::Worker.__ray_call__()\u001b[39m (pid=420589, ip=128.208.3.124, actor_id=7b76b108b090ec43c422bbbf01000000, repr=<test_execution_schedule_gpu.Worker object at 0x73b49a05a100>)\n",
      "\u001b[31mFAILED\u001b[0m test_execution_schedule_gpu.py::\u001b[1mtest_overlap_gpu_communication[True-ray_start_regular0]\u001b[0m - ray.exceptions.RayTaskError(RuntimeError): \u001b[36mray::Worker.__ray_call__()\u001b[39m (pid=421827, ip=128.208.3.124, actor_id=3e3d752bed46aacd9ae35ed101000000, repr=<test_execution_schedule_gpu.Worker object at 0x7b700ccda280>)\n",
      "\u001b[31mFAILED\u001b[0m test_execution_schedule_gpu.py::\u001b[1mtest_overlap_gpu_communication[False-ray_start_regular0]\u001b[0m - ray.exceptions.RayTaskError(RuntimeError): \u001b[36mray::Worker.__ray_call__()\u001b[39m (pid=423071, ip=128.208.3.124, actor_id=7dd980fd8c18871c6ad9e4ac01000000, repr=<test_execution_schedule_gpu.Worker object at 0x71dbc3f9a190>)\n",
      "\u001b[31m========================= \u001b[31m\u001b[1m8 failed\u001b[0m\u001b[31m in 61.77s (0:01:01)\u001b[0m\u001b[31m =========================\u001b[0m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "# !python -m pytest -v -s test_execution_schedule.py\n",
    "\n",
    "!python -m pytest -v -s test_execution_schedule_gpu.py\n",
    "# !python -m pytest -v -s test_torch_tensor_dag.py\n",
    "# !python -m pytest -v -s test_accelerated_dag.py\n",
    "\n",
    "# !python -m pytest -v -s test_cpu_communicator_dag.py"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "stbr-coll-sched-0512",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
