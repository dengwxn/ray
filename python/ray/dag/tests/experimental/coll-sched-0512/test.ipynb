{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.chdir(\"/home/wxdeng/stbr-min/python/ray/dag/tests/experimental\")\n",
    "os.environ[\"RAY_DEDUP_LOGS\"] = \"0\"\n",
    "os.environ[\"RAY_PYTEST_USE_GPU\"] = \"1\"\n",
    "os.environ[\"RAY_CGRAPH_VISUALIZE_SCHEDULE\"] = \"1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "# !python -m coll-sched-0512.ddp --name ddp_bench"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "# !python -m coll-sched-0512.ddp --name ddp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "# !python -m pytest -v -s test_coll_sched.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m============================= test session starts ==============================\u001b[0m\n",
      "platform linux -- Python 3.13.3, pytest-8.3.5, pluggy-1.5.0 -- /home/wxdeng/miniconda3/envs/stbr-unify/bin/python\n",
      "cachedir: .pytest_cache\n",
      "rootdir: /home/wxdeng/stbr-min\n",
      "configfile: pytest.ini\n",
      "plugins: anyio-4.9.0\n",
      "collected 1 item                                                               \u001b[0m\n",
      "\n",
      "test_execution_schedule.py::TestSelectNextNodes::test_only_one_nccl_write \u001b[32mPASSED\u001b[0m\n",
      "\n",
      "\u001b[32m============================== \u001b[32m\u001b[1m1 passed\u001b[0m\u001b[32m in 0.07s\u001b[0m\u001b[32m ===============================\u001b[0m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!python -m pytest -v -s test_execution_schedule.py::TestSelectNextNodes::test_only_one_nccl_write"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m============================= test session starts ==============================\u001b[0m\n",
      "platform linux -- Python 3.13.3, pytest-8.3.5, pluggy-1.5.0 -- /home/wxdeng/miniconda3/envs/stbr-unify/bin/python\n",
      "cachedir: .pytest_cache\n",
      "rootdir: /home/wxdeng/stbr-min\n",
      "configfile: pytest.ini\n",
      "plugins: anyio-4.9.0\n",
      "collected 15 items                                                             \u001b[0m\n",
      "\n",
      "test_execution_schedule.py::TestSelectNextNodes::test_two_candidates_on_same_actor \u001b[32mPASSED\u001b[0m\n",
      "test_execution_schedule.py::TestSelectNextNodes::test_only_one_nccl_write \u001b[32mPASSED\u001b[0m\n",
      "test_execution_schedule.py::TestSelectNextNodes::test_two_nccl_writes \u001b[32mPASSED\u001b[0m\n",
      "test_execution_schedule.py::TestSelectNextNodes::test_only_one_nccl_collective \u001b[32mPASSED\u001b[0m\n",
      "test_execution_schedule.py::TestSelectNextNodes::test_two_nccl_collectives \u001b[32mPASSED\u001b[0m\n",
      "test_execution_schedule.py::TestBuildDAGNodeOperationGraph::test_edges_between_read_compute_write \u001b[32mPASSED\u001b[0m\n",
      "test_execution_schedule.py::TestBuildDAGNodeOperationGraph::test_edge_between_writer_and_reader \u001b[32mPASSED\u001b[0m\n",
      "test_execution_schedule.py::TestBuildDAGNodeOperationGraph::test_edge_between_compute_nodes \u001b[32mPASSED\u001b[0m\n",
      "test_execution_schedule.py::TestBuildDAGNodeOperationGraph::test_two_actors \u001b[32mPASSED\u001b[0m\n",
      "test_execution_schedule.py::TestGenerateActorToExecutionSchedule::test_single_actor_1 \u001b[32mPASSED\u001b[0m\n",
      "test_execution_schedule.py::TestGenerateActorToExecutionSchedule::test_single_actor_2 \u001b[32mPASSED\u001b[0m\n",
      "test_execution_schedule.py::TestGenerateActorToExecutionSchedule::test_two_actors_no_nccl \u001b[32mPASSED\u001b[0m\n",
      "test_execution_schedule.py::TestGenerateActorToExecutionSchedule::test_two_actors_with_nccl \u001b[32mPASSED\u001b[0m\n",
      "test_execution_schedule.py::TestGenerateActorToExecutionSchedule::test_simulate_pp_2workers_2batches_1f1b_with_nccl \u001b[32mPASSED\u001b[0m\n",
      "test_execution_schedule.py::TestGenerateActorToExecutionSchedule::test_simulate_pp_2workers_2batches_1f1b_no_nccl \u001b[32mPASSED\u001b[0m\n",
      "\n",
      "\u001b[32m============================== \u001b[32m\u001b[1m15 passed\u001b[0m\u001b[32m in 0.51s\u001b[0m\u001b[32m ==============================\u001b[0m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!python -m pytest -v -s test_execution_schedule.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m============================= test session starts ==============================\u001b[0m\n",
      "platform linux -- Python 3.13.3, pytest-8.3.5, pluggy-1.5.0 -- /home/wxdeng/miniconda3/envs/stbr-unify/bin/python\n",
      "cachedir: .pytest_cache\n",
      "rootdir: /home/wxdeng/stbr-min\n",
      "configfile: pytest.ini\n",
      "plugins: anyio-4.9.0\n",
      "collected 9 items                                                              \u001b[0m\u001b[1m\n",
      "\n",
      "test_collective_dag.py::test_all_reduce_duplicate_actors[ray_start_regular0] 2025-05-15 15:44:34,158\tINFO worker.py:1879 -- Started a local Ray instance. View the dashboard at \u001b[1m\u001b[32mhttp://127.0.0.1:8265 \u001b[39m\u001b[22m\n",
      "\u001b[32mPASSED\u001b[0m\n",
      "test_collective_dag.py::test_all_reduce_custom_comm_wrong_actors[ray_start_regular0] 2025-05-15 15:44:38,988\tINFO worker.py:1879 -- Started a local Ray instance. View the dashboard at \u001b[1m\u001b[32mhttp://127.0.0.1:8265 \u001b[39m\u001b[22m\n",
      "\u001b[32mPASSED\u001b[0m\n",
      "test_collective_dag.py::test_comm_all_reduces[ray_start_regular0] 2025-05-15 15:44:42,474\tINFO worker.py:1879 -- Started a local Ray instance. View the dashboard at \u001b[1m\u001b[32mhttp://127.0.0.1:8265 \u001b[39m\u001b[22m\n",
      "2025-05-15 15:44:45,066\tINFO dag_node_operation.py:634 -- Writing compiled graph schedule visualization to compiled_graph_schedule.png\n",
      "2025-05-15 15:44:45,267\tINFO compiled_dag_node.py:2173 -- Tearing down compiled DAG\n",
      "2025-05-15 15:44:45,270\tINFO compiled_dag_node.py:2178 -- Cancelling compiled worker on actor: Actor(CPUTorchTensorWorker, 12ebf5445f1efd01fddf46b001000000)\n",
      "2025-05-15 15:44:45,270\tINFO compiled_dag_node.py:2178 -- Cancelling compiled worker on actor: Actor(CPUTorchTensorWorker, 28b7936f3be600e6cad54ec701000000)\n",
      "2025-05-15 15:44:45,594\tINFO compiled_dag_node.py:2200 -- Waiting for worker tasks to exit\n",
      "2025-05-15 15:44:45,621\tINFO compiled_dag_node.py:2203 -- Teardown complete\n",
      "\u001b[32mPASSED\u001b[0m\n",
      "test_collective_dag.py::test_comm_deduplicate_all_reduces[ray_start_regular0] 2025-05-15 15:44:50,230\tINFO worker.py:1879 -- Started a local Ray instance. View the dashboard at \u001b[1m\u001b[32mhttp://127.0.0.1:8265 \u001b[39m\u001b[22m\n",
      "2025-05-15 15:44:52,523\tINFO dag_node_operation.py:634 -- Writing compiled graph schedule visualization to compiled_graph_schedule.png\n",
      "2025-05-15 15:44:52,763\tINFO compiled_dag_node.py:2173 -- Tearing down compiled DAG\n",
      "2025-05-15 15:44:52,766\tINFO compiled_dag_node.py:2178 -- Cancelling compiled worker on actor: Actor(CPUTorchTensorWorker, c72819551d21e1fea591c09d01000000)\n",
      "2025-05-15 15:44:52,766\tINFO compiled_dag_node.py:2178 -- Cancelling compiled worker on actor: Actor(CPUTorchTensorWorker, 0b0c79d3d7bef134fe70910201000000)\n",
      "2025-05-15 15:44:53,089\tINFO compiled_dag_node.py:2200 -- Waiting for worker tasks to exit\n",
      "2025-05-15 15:44:53,117\tINFO compiled_dag_node.py:2203 -- Teardown complete\n",
      "\u001b[32mPASSED\u001b[0m\n",
      "test_collective_dag.py::test_comm_deduplicate_p2p_and_collective[ray_start_regular0] 2025-05-15 15:44:56,665\tINFO worker.py:1879 -- Started a local Ray instance. View the dashboard at \u001b[1m\u001b[32mhttp://127.0.0.1:8265 \u001b[39m\u001b[22m\n",
      "2025-05-15 15:44:58,985\tINFO dag_node_operation.py:634 -- Writing compiled graph schedule visualization to compiled_graph_schedule.png\n",
      "2025-05-15 15:44:59,212\tINFO compiled_dag_node.py:2173 -- Tearing down compiled DAG\n",
      "2025-05-15 15:44:59,215\tINFO compiled_dag_node.py:2178 -- Cancelling compiled worker on actor: Actor(CPUTorchTensorWorker, 50198ff947440e15038a2d0a01000000)\n",
      "2025-05-15 15:44:59,216\tINFO compiled_dag_node.py:2178 -- Cancelling compiled worker on actor: Actor(CPUTorchTensorWorker, 64756c39f435c9bcba80255601000000)\n",
      "2025-05-15 15:44:59,528\tINFO compiled_dag_node.py:2200 -- Waiting for worker tasks to exit\n",
      "2025-05-15 15:44:59,555\tINFO compiled_dag_node.py:2203 -- Teardown complete\n",
      "2025-05-15 15:44:59,667\tINFO dag_node_operation.py:634 -- Writing compiled graph schedule visualization to compiled_graph_schedule.png\n",
      "2025-05-15 15:44:59,870\tINFO compiled_dag_node.py:2173 -- Tearing down compiled DAG\n",
      "2025-05-15 15:44:59,874\tINFO compiled_dag_node.py:2178 -- Cancelling compiled worker on actor: Actor(CPUTorchTensorWorker, 50198ff947440e15038a2d0a01000000)\n",
      "2025-05-15 15:44:59,874\tINFO compiled_dag_node.py:2178 -- Cancelling compiled worker on actor: Actor(CPUTorchTensorWorker, 64756c39f435c9bcba80255601000000)\n",
      "2025-05-15 15:44:59,905\tINFO compiled_dag_node.py:2200 -- Waiting for worker tasks to exit\n",
      "2025-05-15 15:44:59,905\tINFO compiled_dag_node.py:2203 -- Teardown complete\n",
      "\u001b[32mPASSED\u001b[0m\n",
      "test_collective_dag.py::test_custom_comm[ray_start_regular0] 2025-05-15 15:45:04,594\tINFO worker.py:1879 -- Started a local Ray instance. View the dashboard at \u001b[1m\u001b[32mhttp://127.0.0.1:8265 \u001b[39m\u001b[22m\n",
      "2025-05-15 15:45:06,878\tINFO dag_node_operation.py:634 -- Writing compiled graph schedule visualization to compiled_graph_schedule.png\n",
      "2025-05-15 15:45:07,183\tINFO compiled_dag_node.py:2173 -- Tearing down compiled DAG\n",
      "2025-05-15 15:45:07,187\tINFO compiled_dag_node.py:2178 -- Cancelling compiled worker on actor: Actor(CPUTorchTensorWorker, e655102d32f77787e1f2da7d01000000)\n",
      "2025-05-15 15:45:07,187\tINFO compiled_dag_node.py:2178 -- Cancelling compiled worker on actor: Actor(CPUTorchTensorWorker, c3f5033caa9f05784961847601000000)\n",
      "2025-05-15 15:45:07,508\tINFO compiled_dag_node.py:2200 -- Waiting for worker tasks to exit\n",
      "2025-05-15 15:45:07,539\tINFO compiled_dag_node.py:2203 -- Teardown complete\n",
      "2025-05-15 15:45:07,644\tINFO dag_node_operation.py:634 -- Writing compiled graph schedule visualization to compiled_graph_schedule.png\n",
      "2025-05-15 15:45:07,950\tINFO compiled_dag_node.py:2173 -- Tearing down compiled DAG\n",
      "2025-05-15 15:45:07,954\tINFO compiled_dag_node.py:2178 -- Cancelling compiled worker on actor: Actor(CPUTorchTensorWorker, e655102d32f77787e1f2da7d01000000)\n",
      "2025-05-15 15:45:07,954\tINFO compiled_dag_node.py:2178 -- Cancelling compiled worker on actor: Actor(CPUTorchTensorWorker, c3f5033caa9f05784961847601000000)\n",
      "2025-05-15 15:45:07,985\tINFO compiled_dag_node.py:2200 -- Waiting for worker tasks to exit\n",
      "2025-05-15 15:45:07,985\tINFO compiled_dag_node.py:2203 -- Teardown complete\n",
      "\u001b[32mPASSED\u001b[0m\n",
      "test_collective_dag.py::test_custom_comm_init_teardown[ray_start_regular0] 2025-05-15 15:45:11,540\tINFO worker.py:1879 -- Started a local Ray instance. View the dashboard at \u001b[1m\u001b[32mhttp://127.0.0.1:8265 \u001b[39m\u001b[22m\n",
      "2025-05-15 15:45:13,860\tINFO dag_node_operation.py:634 -- Writing compiled graph schedule visualization to compiled_graph_schedule.png\n",
      "2025-05-15 15:45:14,046\tINFO compiled_dag_node.py:2173 -- Tearing down compiled DAG\n",
      "2025-05-15 15:45:14,050\tINFO compiled_dag_node.py:2178 -- Cancelling compiled worker on actor: Actor(CPUTorchTensorWorker, 4e7536666e97f8f71b98317a01000000)\n",
      "2025-05-15 15:45:14,050\tINFO compiled_dag_node.py:2178 -- Cancelling compiled worker on actor: Actor(CPUTorchTensorWorker, 9ae6f0a20de9021fd2f39de601000000)\n",
      "2025-05-15 15:45:14,370\tINFO compiled_dag_node.py:2200 -- Waiting for worker tasks to exit\n",
      "2025-05-15 15:45:14,403\tINFO compiled_dag_node.py:2203 -- Teardown complete\n",
      "2025-05-15 15:45:14,536\tINFO dag_node_operation.py:634 -- Writing compiled graph schedule visualization to compiled_graph_schedule.png\n",
      "2025-05-15 15:45:14,844\tINFO compiled_dag_node.py:2173 -- Tearing down compiled DAG\n",
      "2025-05-15 15:45:14,848\tINFO compiled_dag_node.py:2178 -- Cancelling compiled worker on actor: Actor(CPUTorchTensorWorker, 4e7536666e97f8f71b98317a01000000)\n",
      "2025-05-15 15:45:14,848\tINFO compiled_dag_node.py:2178 -- Cancelling compiled worker on actor: Actor(CPUTorchTensorWorker, 9ae6f0a20de9021fd2f39de601000000)\n",
      "2025-05-15 15:45:14,864\tINFO compiled_dag_node.py:2200 -- Waiting for worker tasks to exit\n",
      "2025-05-15 15:45:14,865\tINFO compiled_dag_node.py:2203 -- Teardown complete\n",
      "\u001b[32mPASSED\u001b[0m\n",
      "test_collective_dag.py::test_exec_schedules_ddp[2-ray_start_regular0] 2025-05-15 15:45:19,284\tINFO worker.py:1879 -- Started a local Ray instance. View the dashboard at \u001b[1m\u001b[32mhttp://127.0.0.1:8265 \u001b[39m\u001b[22m\n",
      "2025-05-15 15:45:19,953\tINFO torch_tensor_nccl_channel.py:770 -- Initializing custom NCCL group 7541bb94-32d9-4d73-a171-74a09ee95f59 on actors: [Actor(DDPWorker, 42802fcfd5485c98fc54178401000000), Actor(DDPWorker, 17d2a6ddc432467e69390d4601000000)]\n",
      "2025-05-15 15:45:21,589\tINFO torch_tensor_nccl_channel.py:797 -- NCCL group initialized.\n",
      "2025-05-15 15:45:21,748\tINFO dag_node_operation.py:634 -- Writing compiled graph schedule visualization to compiled_graph_schedule.png\n",
      "\u001b[32mPASSED\u001b[0m2025-05-15 15:45:22,054\tINFO compiled_dag_node.py:2173 -- Tearing down compiled DAG\n",
      "2025-05-15 15:45:22,061\tINFO compiled_dag_node.py:2178 -- Cancelling compiled worker on actor: Actor(DDPWorker, 42802fcfd5485c98fc54178401000000)\n",
      "2025-05-15 15:45:22,061\tINFO compiled_dag_node.py:2178 -- Cancelling compiled worker on actor: Actor(DDPWorker, 17d2a6ddc432467e69390d4601000000)\n",
      "2025-05-15 15:45:22,069\tINFO compiled_dag_node.py:2200 -- Waiting for worker tasks to exit\n",
      "2025-05-15 15:45:22,069\tINFO compiled_dag_node.py:2161 -- Killing actor: Actor(DDPWorker, 42802fcfd5485c98fc54178401000000)\n",
      "2025-05-15 15:45:22,069\tINFO compiled_dag_node.py:2161 -- Killing actor: Actor(DDPWorker, 17d2a6ddc432467e69390d4601000000)\n",
      "2025-05-15 15:45:22,070\tINFO compiled_dag_node.py:2203 -- Teardown complete\n",
      "\n",
      "test_collective_dag.py::test_exec_schedules_ddp[4-ray_start_regular0] 2025-05-15 15:45:26,587\tINFO worker.py:1879 -- Started a local Ray instance. View the dashboard at \u001b[1m\u001b[32mhttp://127.0.0.1:8265 \u001b[39m\u001b[22m\n",
      "2025-05-15 15:45:27,582\tINFO torch_tensor_nccl_channel.py:770 -- Initializing custom NCCL group 1ea8470b-788a-43a8-a46d-5b6820c6afd4 on actors: [Actor(DDPWorker, 623bee24aa6a414087adb34f01000000), Actor(DDPWorker, 2f09fb3eed74226086228bd801000000), Actor(DDPWorker, 798e017a855dc532ddecfd0101000000), Actor(DDPWorker, a91df1425c43ce0c1c12046c01000000)]\n",
      "2025-05-15 15:45:29,221\tINFO torch_tensor_nccl_channel.py:797 -- NCCL group initialized.\n",
      "2025-05-15 15:45:29,504\tINFO dag_node_operation.py:634 -- Writing compiled graph schedule visualization to compiled_graph_schedule.png\n",
      "\u001b[32mPASSED\u001b[0m2025-05-15 15:45:29,972\tINFO compiled_dag_node.py:2173 -- Tearing down compiled DAG\n",
      "2025-05-15 15:45:29,988\tINFO compiled_dag_node.py:2178 -- Cancelling compiled worker on actor: Actor(DDPWorker, 623bee24aa6a414087adb34f01000000)\n",
      "2025-05-15 15:45:29,988\tINFO compiled_dag_node.py:2178 -- Cancelling compiled worker on actor: Actor(DDPWorker, 2f09fb3eed74226086228bd801000000)\n",
      "2025-05-15 15:45:29,988\tINFO compiled_dag_node.py:2178 -- Cancelling compiled worker on actor: Actor(DDPWorker, 798e017a855dc532ddecfd0101000000)\n",
      "2025-05-15 15:45:29,988\tINFO compiled_dag_node.py:2178 -- Cancelling compiled worker on actor: Actor(DDPWorker, a91df1425c43ce0c1c12046c01000000)\n",
      "2025-05-15 15:45:30,001\tINFO compiled_dag_node.py:2200 -- Waiting for worker tasks to exit\n",
      "2025-05-15 15:45:30,001\tINFO compiled_dag_node.py:2161 -- Killing actor: Actor(DDPWorker, 623bee24aa6a414087adb34f01000000)\n",
      "2025-05-15 15:45:30,001\tINFO compiled_dag_node.py:2161 -- Killing actor: Actor(DDPWorker, 2f09fb3eed74226086228bd801000000)\n",
      "2025-05-15 15:45:30,002\tINFO compiled_dag_node.py:2161 -- Killing actor: Actor(DDPWorker, 798e017a855dc532ddecfd0101000000)\n",
      "2025-05-15 15:45:30,002\tINFO compiled_dag_node.py:2161 -- Killing actor: Actor(DDPWorker, a91df1425c43ce0c1c12046c01000000)\n",
      "2025-05-15 15:45:30,002\tINFO compiled_dag_node.py:2203 -- Teardown complete\n",
      "\n",
      "\n",
      "\u001b[32m========================= \u001b[32m\u001b[1m9 passed\u001b[0m\u001b[32m in 60.91s (0:01:00)\u001b[0m\u001b[32m =========================\u001b[0m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!python -m pytest -v -s test_collective_dag.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m============================= test session starts ==============================\u001b[0m\n",
      "platform linux -- Python 3.13.3, pytest-8.3.5, pluggy-1.5.0 -- /home/wxdeng/miniconda3/envs/stbr-unify/bin/python\n",
      "cachedir: .pytest_cache\n",
      "rootdir: /home/wxdeng/stbr-min\n",
      "configfile: pytest.ini\n",
      "plugins: anyio-4.9.0\n",
      "collected 53 items                                                             \u001b[0m\u001b[1m\n",
      "\n",
      "test_torch_tensor_dag.py::test_torch_tensor_p2p[ray_start_regular0] 2025-05-15 15:45:37,543\tINFO worker.py:1879 -- Started a local Ray instance. View the dashboard at \u001b[1m\u001b[32mhttp://127.0.0.1:8265 \u001b[39m\u001b[22m\n",
      "2025-05-15 15:45:39,824\tINFO torch_tensor_nccl_channel.py:772 -- Creating NCCL group 1d0453dd-04aa-4d68-8c83-429f83f9ad0b on actors: [Actor(TorchTensorWorker, 63b047764a4698844f7ca4f101000000), Actor(TorchTensorWorker, f4a2133416a0db60f3cef6c501000000)]\n",
      "2025-05-15 15:45:40,888\tINFO torch_tensor_nccl_channel.py:797 -- NCCL group initialized.\n",
      "2025-05-15 15:45:41,070\tINFO dag_node_operation.py:634 -- Writing compiled graph schedule visualization to compiled_graph_schedule.png\n",
      "\u001b[32mPASSED\u001b[0m2025-05-15 15:45:41,270\tINFO compiled_dag_node.py:2173 -- Tearing down compiled DAG\n",
      "2025-05-15 15:45:41,271\tINFO compiled_dag_node.py:2178 -- Cancelling compiled worker on actor: Actor(TorchTensorWorker, 63b047764a4698844f7ca4f101000000)\n",
      "2025-05-15 15:45:41,271\tINFO compiled_dag_node.py:2178 -- Cancelling compiled worker on actor: Actor(TorchTensorWorker, f4a2133416a0db60f3cef6c501000000)\n",
      "\u001b[36m(TorchTensorWorker pid=2103847)\u001b[0m Destructing NCCL group on actor: Actor(TorchTensorWorker, f4a2133416a0db60f3cef6c501000000)\n",
      "\u001b[36m(TorchTensorWorker pid=2103848)\u001b[0m Destructing NCCL group on actor: Actor(TorchTensorWorker, 63b047764a4698844f7ca4f101000000)\n",
      "2025-05-15 15:45:41,765\tINFO compiled_dag_node.py:2200 -- Waiting for worker tasks to exit\n",
      "2025-05-15 15:45:41,765\tINFO compiled_dag_node.py:2161 -- Killing actor: Actor(TorchTensorWorker, 63b047764a4698844f7ca4f101000000)\n",
      "2025-05-15 15:45:41,765\tINFO compiled_dag_node.py:2161 -- Killing actor: Actor(TorchTensorWorker, f4a2133416a0db60f3cef6c501000000)\n",
      "2025-05-15 15:45:41,766\tINFO compiled_dag_node.py:2203 -- Teardown complete\n",
      "\n",
      "test_torch_tensor_dag.py::test_torch_tensor_as_dag_input[ray_start_regular0] 2025-05-15 15:45:45,493\tINFO worker.py:1879 -- Started a local Ray instance. View the dashboard at \u001b[1m\u001b[32mhttp://127.0.0.1:8265 \u001b[39m\u001b[22m\n",
      "2025-05-15 15:45:47,341\tINFO dag_node_operation.py:634 -- Writing compiled graph schedule visualization to compiled_graph_schedule.png\n",
      "\u001b[32mPASSED\u001b[0m2025-05-15 15:45:47,656\tINFO compiled_dag_node.py:2173 -- Tearing down compiled DAG\n",
      "2025-05-15 15:45:47,657\tINFO compiled_dag_node.py:2178 -- Cancelling compiled worker on actor: Actor(TorchTensorWorker, ee37f6ff938dad693e66da2501000000)\n",
      "2025-05-15 15:45:47,659\tINFO compiled_dag_node.py:2200 -- Waiting for worker tasks to exit\n",
      "2025-05-15 15:45:47,660\tINFO compiled_dag_node.py:2161 -- Killing actor: Actor(TorchTensorWorker, ee37f6ff938dad693e66da2501000000)\n",
      "2025-05-15 15:45:47,660\tINFO compiled_dag_node.py:2203 -- Teardown complete\n",
      "\n",
      "test_torch_tensor_dag.py::test_torch_tensor_nccl[False-False-ray_start_regular0] 2025-05-15 15:45:50,833\tINFO worker.py:1879 -- Started a local Ray instance. View the dashboard at \u001b[1m\u001b[32mhttp://127.0.0.1:8265 \u001b[39m\u001b[22m\n",
      "2025-05-15 15:45:53,027\tINFO torch_tensor_nccl_channel.py:772 -- Creating NCCL group de57f830-fc64-4e23-baf6-2498bb6d33d8 on actors: [Actor(TorchTensorWorker, 3ab1497ae927a50209685be801000000), Actor(TorchTensorWorker, ca58290dc95c6b3aa8c764c901000000)]\n",
      "2025-05-15 15:45:54,095\tINFO torch_tensor_nccl_channel.py:797 -- NCCL group initialized.\n",
      "2025-05-15 15:45:54,234\tINFO dag_node_operation.py:634 -- Writing compiled graph schedule visualization to compiled_graph_schedule.png\n",
      "2025-05-15 15:45:54,431\tINFO compiled_dag_node.py:2173 -- Tearing down compiled DAG\n",
      "2025-05-15 15:45:54,431\tINFO compiled_dag_node.py:2178 -- Cancelling compiled worker on actor: Actor(TorchTensorWorker, ca58290dc95c6b3aa8c764c901000000)\n",
      "2025-05-15 15:45:54,431\tINFO compiled_dag_node.py:2178 -- Cancelling compiled worker on actor: Actor(TorchTensorWorker, 3ab1497ae927a50209685be801000000)\n",
      "\u001b[36m(TorchTensorWorker pid=2106273)\u001b[0m Destructing NCCL group on actor: Actor(TorchTensorWorker, 3ab1497ae927a50209685be801000000)\n",
      "\u001b[36m(TorchTensorWorker pid=2106274)\u001b[0m Destructing NCCL group on actor: Actor(TorchTensorWorker, ca58290dc95c6b3aa8c764c901000000)\n",
      "2025-05-15 15:45:54,947\tINFO compiled_dag_node.py:2200 -- Waiting for worker tasks to exit\n",
      "2025-05-15 15:45:54,948\tINFO compiled_dag_node.py:2203 -- Teardown complete\n",
      "2025-05-15 15:45:54,958\tINFO torch_tensor_nccl_channel.py:772 -- Creating NCCL group 5cc577dd-49df-4971-a1b1-78518d709632 on actors: [Actor(TorchTensorWorker, 3ab1497ae927a50209685be801000000), Actor(TorchTensorWorker, ca58290dc95c6b3aa8c764c901000000)]\n",
      "2025-05-15 15:45:54,986\tINFO torch_tensor_nccl_channel.py:797 -- NCCL group initialized.\n",
      "2025-05-15 15:45:55,102\tINFO dag_node_operation.py:634 -- Writing compiled graph schedule visualization to compiled_graph_schedule.png\n",
      "\u001b[32mPASSED\u001b[0m2025-05-15 15:45:55,267\tINFO compiled_dag_node.py:2173 -- Tearing down compiled DAG\n",
      "2025-05-15 15:45:55,268\tINFO compiled_dag_node.py:2178 -- Cancelling compiled worker on actor: Actor(TorchTensorWorker, ca58290dc95c6b3aa8c764c901000000)\n",
      "2025-05-15 15:45:55,268\tINFO compiled_dag_node.py:2178 -- Cancelling compiled worker on actor: Actor(TorchTensorWorker, 3ab1497ae927a50209685be801000000)\n",
      "\u001b[36m(TorchTensorWorker pid=2106273)\u001b[0m Destructing NCCL group on actor: Actor(TorchTensorWorker, 3ab1497ae927a50209685be801000000)\n",
      "\u001b[36m(TorchTensorWorker pid=2106274)\u001b[0m Destructing NCCL group on actor: Actor(TorchTensorWorker, ca58290dc95c6b3aa8c764c901000000)\n",
      "2025-05-15 15:45:55,782\tINFO compiled_dag_node.py:2200 -- Waiting for worker tasks to exit\n",
      "2025-05-15 15:45:55,782\tINFO compiled_dag_node.py:2161 -- Killing actor: Actor(TorchTensorWorker, ca58290dc95c6b3aa8c764c901000000)\n",
      "2025-05-15 15:45:55,783\tINFO compiled_dag_node.py:2161 -- Killing actor: Actor(TorchTensorWorker, 3ab1497ae927a50209685be801000000)\n",
      "2025-05-15 15:45:55,783\tINFO compiled_dag_node.py:2203 -- Teardown complete\n",
      "\n",
      "test_torch_tensor_dag.py::test_torch_tensor_nccl[False-True-ray_start_regular0] 2025-05-15 15:46:00,559\tINFO worker.py:1879 -- Started a local Ray instance. View the dashboard at \u001b[1m\u001b[32mhttp://127.0.0.1:8265 \u001b[39m\u001b[22m\n",
      "2025-05-15 15:46:02,762\tINFO torch_tensor_nccl_channel.py:772 -- Creating NCCL group 761cce06-93ef-474e-8a98-70a88d55cb26 on actors: [Actor(TorchTensorWorker, 7ed5c3f77100a600b11b0b6601000000), Actor(TorchTensorWorker, 6b40087a041adfc2b568a64e01000000)]\n",
      "2025-05-15 15:46:03,834\tINFO torch_tensor_nccl_channel.py:797 -- NCCL group initialized.\n",
      "2025-05-15 15:46:03,990\tINFO dag_node_operation.py:634 -- Writing compiled graph schedule visualization to compiled_graph_schedule.png\n",
      "2025-05-15 15:46:04,189\tINFO compiled_dag_node.py:2173 -- Tearing down compiled DAG\n",
      "2025-05-15 15:46:04,189\tINFO compiled_dag_node.py:2178 -- Cancelling compiled worker on actor: Actor(TorchTensorWorker, 7ed5c3f77100a600b11b0b6601000000)\n",
      "2025-05-15 15:46:04,189\tINFO compiled_dag_node.py:2178 -- Cancelling compiled worker on actor: Actor(TorchTensorWorker, 6b40087a041adfc2b568a64e01000000)\n",
      "\u001b[36m(TorchTensorWorker pid=2107608)\u001b[0m Destructing NCCL group on actor: Actor(TorchTensorWorker, 7ed5c3f77100a600b11b0b6601000000)\n",
      "\u001b[36m(TorchTensorWorker pid=2107606)\u001b[0m Destructing NCCL group on actor: Actor(TorchTensorWorker, 6b40087a041adfc2b568a64e01000000)\n",
      "2025-05-15 15:46:04,686\tINFO compiled_dag_node.py:2200 -- Waiting for worker tasks to exit\n",
      "2025-05-15 15:46:04,686\tINFO compiled_dag_node.py:2203 -- Teardown complete\n",
      "2025-05-15 15:46:04,698\tINFO torch_tensor_nccl_channel.py:772 -- Creating NCCL group 5d0610b1-7f77-4610-baa4-ef0b307e9f4a on actors: [Actor(TorchTensorWorker, 7ed5c3f77100a600b11b0b6601000000), Actor(TorchTensorWorker, 6b40087a041adfc2b568a64e01000000)]\n",
      "2025-05-15 15:46:04,736\tINFO torch_tensor_nccl_channel.py:797 -- NCCL group initialized.\n",
      "2025-05-15 15:46:04,853\tINFO dag_node_operation.py:634 -- Writing compiled graph schedule visualization to compiled_graph_schedule.png\n",
      "\u001b[32mPASSED\u001b[0m2025-05-15 15:46:05,018\tINFO compiled_dag_node.py:2173 -- Tearing down compiled DAG\n",
      "2025-05-15 15:46:05,019\tINFO compiled_dag_node.py:2178 -- Cancelling compiled worker on actor: Actor(TorchTensorWorker, 7ed5c3f77100a600b11b0b6601000000)\n",
      "2025-05-15 15:46:05,019\tINFO compiled_dag_node.py:2178 -- Cancelling compiled worker on actor: Actor(TorchTensorWorker, 6b40087a041adfc2b568a64e01000000)\n",
      "\u001b[36m(TorchTensorWorker pid=2107608)\u001b[0m Destructing NCCL group on actor: Actor(TorchTensorWorker, 7ed5c3f77100a600b11b0b6601000000)\n",
      "\u001b[36m(TorchTensorWorker pid=2107606)\u001b[0m Destructing NCCL group on actor: Actor(TorchTensorWorker, 6b40087a041adfc2b568a64e01000000)\n",
      "2025-05-15 15:46:05,522\tINFO compiled_dag_node.py:2200 -- Waiting for worker tasks to exit\n",
      "2025-05-15 15:46:05,522\tINFO compiled_dag_node.py:2161 -- Killing actor: Actor(TorchTensorWorker, 7ed5c3f77100a600b11b0b6601000000)\n",
      "2025-05-15 15:46:05,522\tINFO compiled_dag_node.py:2161 -- Killing actor: Actor(TorchTensorWorker, 6b40087a041adfc2b568a64e01000000)\n",
      "2025-05-15 15:46:05,523\tINFO compiled_dag_node.py:2203 -- Teardown complete\n",
      "\n",
      "test_torch_tensor_dag.py::test_torch_tensor_nccl[True-False-ray_start_regular0] 2025-05-15 15:46:10,123\tINFO worker.py:1879 -- Started a local Ray instance. View the dashboard at \u001b[1m\u001b[32mhttp://127.0.0.1:8265 \u001b[39m\u001b[22m\n",
      "2025-05-15 15:46:12,302\tINFO torch_tensor_nccl_channel.py:772 -- Creating NCCL group 4d9831a0-56ff-43e3-8585-487b48c0e164 on actors: [Actor(TorchTensorWorker, 993389074da9a05b5c837a9901000000), Actor(TorchTensorWorker, b02cf83c90b6121bc0afa58801000000)]\n",
      "2025-05-15 15:46:13,386\tINFO torch_tensor_nccl_channel.py:797 -- NCCL group initialized.\n",
      "2025-05-15 15:46:13,552\tINFO dag_node_operation.py:634 -- Writing compiled graph schedule visualization to compiled_graph_schedule.png\n",
      "2025-05-15 15:46:13,750\tINFO compiled_dag_node.py:2173 -- Tearing down compiled DAG\n",
      "2025-05-15 15:46:13,750\tINFO compiled_dag_node.py:2178 -- Cancelling compiled worker on actor: Actor(TorchTensorWorker, b02cf83c90b6121bc0afa58801000000)\n",
      "2025-05-15 15:46:13,750\tINFO compiled_dag_node.py:2178 -- Cancelling compiled worker on actor: Actor(TorchTensorWorker, 993389074da9a05b5c837a9901000000)\n",
      "\u001b[36m(TorchTensorWorker pid=2108891)\u001b[0m Destructing NCCL group on actor: Actor(TorchTensorWorker, 993389074da9a05b5c837a9901000000)\n",
      "\u001b[36m(TorchTensorWorker pid=2108890)\u001b[0m Destructing NCCL group on actor: Actor(TorchTensorWorker, b02cf83c90b6121bc0afa58801000000)\n",
      "2025-05-15 15:46:14,268\tINFO compiled_dag_node.py:2200 -- Waiting for worker tasks to exit\n",
      "2025-05-15 15:46:14,269\tINFO compiled_dag_node.py:2203 -- Teardown complete\n",
      "2025-05-15 15:46:14,279\tINFO torch_tensor_nccl_channel.py:772 -- Creating NCCL group c5eb6a47-2b00-4546-96cf-aa111e1f3139 on actors: [Actor(TorchTensorWorker, 993389074da9a05b5c837a9901000000), Actor(TorchTensorWorker, b02cf83c90b6121bc0afa58801000000)]\n",
      "2025-05-15 15:46:14,313\tINFO torch_tensor_nccl_channel.py:797 -- NCCL group initialized.\n",
      "2025-05-15 15:46:14,431\tINFO dag_node_operation.py:634 -- Writing compiled graph schedule visualization to compiled_graph_schedule.png\n",
      "\u001b[32mPASSED\u001b[0m2025-05-15 15:46:14,594\tINFO compiled_dag_node.py:2173 -- Tearing down compiled DAG\n",
      "2025-05-15 15:46:14,594\tINFO compiled_dag_node.py:2178 -- Cancelling compiled worker on actor: Actor(TorchTensorWorker, b02cf83c90b6121bc0afa58801000000)\n",
      "2025-05-15 15:46:14,595\tINFO compiled_dag_node.py:2178 -- Cancelling compiled worker on actor: Actor(TorchTensorWorker, 993389074da9a05b5c837a9901000000)\n",
      "\u001b[36m(TorchTensorWorker pid=2108891)\u001b[0m Destructing NCCL group on actor: Actor(TorchTensorWorker, 993389074da9a05b5c837a9901000000)\n",
      "\u001b[36m(TorchTensorWorker pid=2108890)\u001b[0m Destructing NCCL group on actor: Actor(TorchTensorWorker, b02cf83c90b6121bc0afa58801000000)\n",
      "2025-05-15 15:46:15,112\tINFO compiled_dag_node.py:2200 -- Waiting for worker tasks to exit\n",
      "2025-05-15 15:46:15,112\tINFO compiled_dag_node.py:2161 -- Killing actor: Actor(TorchTensorWorker, b02cf83c90b6121bc0afa58801000000)\n",
      "2025-05-15 15:46:15,112\tINFO compiled_dag_node.py:2161 -- Killing actor: Actor(TorchTensorWorker, 993389074da9a05b5c837a9901000000)\n",
      "2025-05-15 15:46:15,113\tINFO compiled_dag_node.py:2203 -- Teardown complete\n",
      "\n",
      "test_torch_tensor_dag.py::test_torch_tensor_nccl[True-True-ray_start_regular0] 2025-05-15 15:46:18,730\tINFO worker.py:1879 -- Started a local Ray instance. View the dashboard at \u001b[1m\u001b[32mhttp://127.0.0.1:8265 \u001b[39m\u001b[22m\n",
      "2025-05-15 15:46:20,915\tINFO torch_tensor_nccl_channel.py:772 -- Creating NCCL group 8758ea63-b5b9-4489-8730-f1781eea52e0 on actors: [Actor(TorchTensorWorker, 9a8e5f6450a76736020961fb01000000), Actor(TorchTensorWorker, 78adb68586299de3e0921b7c01000000)]\n",
      "2025-05-15 15:46:21,989\tINFO torch_tensor_nccl_channel.py:797 -- NCCL group initialized.\n",
      "2025-05-15 15:46:22,152\tINFO dag_node_operation.py:634 -- Writing compiled graph schedule visualization to compiled_graph_schedule.png\n",
      "2025-05-15 15:46:22,351\tINFO compiled_dag_node.py:2173 -- Tearing down compiled DAG\n",
      "2025-05-15 15:46:22,351\tINFO compiled_dag_node.py:2178 -- Cancelling compiled worker on actor: Actor(TorchTensorWorker, 78adb68586299de3e0921b7c01000000)\n",
      "2025-05-15 15:46:22,351\tINFO compiled_dag_node.py:2178 -- Cancelling compiled worker on actor: Actor(TorchTensorWorker, 9a8e5f6450a76736020961fb01000000)\n",
      "\u001b[36m(TorchTensorWorker pid=2110170)\u001b[0m Destructing NCCL group on actor: Actor(TorchTensorWorker, 9a8e5f6450a76736020961fb01000000)\n",
      "\u001b[36m(TorchTensorWorker pid=2110169)\u001b[0m Destructing NCCL group on actor: Actor(TorchTensorWorker, 78adb68586299de3e0921b7c01000000)\n",
      "2025-05-15 15:46:22,869\tINFO compiled_dag_node.py:2200 -- Waiting for worker tasks to exit\n",
      "2025-05-15 15:46:22,869\tINFO compiled_dag_node.py:2203 -- Teardown complete\n",
      "2025-05-15 15:46:22,880\tINFO torch_tensor_nccl_channel.py:772 -- Creating NCCL group da6c738e-e45b-476d-a2cd-381a75e2aaac on actors: [Actor(TorchTensorWorker, 9a8e5f6450a76736020961fb01000000), Actor(TorchTensorWorker, 78adb68586299de3e0921b7c01000000)]\n",
      "2025-05-15 15:46:22,913\tINFO torch_tensor_nccl_channel.py:797 -- NCCL group initialized.\n",
      "2025-05-15 15:46:23,034\tINFO dag_node_operation.py:634 -- Writing compiled graph schedule visualization to compiled_graph_schedule.png\n",
      "\u001b[32mPASSED\u001b[0m2025-05-15 15:46:23,177\tINFO compiled_dag_node.py:2173 -- Tearing down compiled DAG\n",
      "2025-05-15 15:46:23,178\tINFO compiled_dag_node.py:2178 -- Cancelling compiled worker on actor: Actor(TorchTensorWorker, 78adb68586299de3e0921b7c01000000)\n",
      "2025-05-15 15:46:23,178\tINFO compiled_dag_node.py:2178 -- Cancelling compiled worker on actor: Actor(TorchTensorWorker, 9a8e5f6450a76736020961fb01000000)\n",
      "\u001b[36m(TorchTensorWorker pid=2110170)\u001b[0m Destructing NCCL group on actor: Actor(TorchTensorWorker, 9a8e5f6450a76736020961fb01000000)\n",
      "\u001b[36m(TorchTensorWorker pid=2110169)\u001b[0m Destructing NCCL group on actor: Actor(TorchTensorWorker, 78adb68586299de3e0921b7c01000000)\n",
      "2025-05-15 15:46:23,696\tINFO compiled_dag_node.py:2200 -- Waiting for worker tasks to exit\n",
      "2025-05-15 15:46:23,696\tINFO compiled_dag_node.py:2161 -- Killing actor: Actor(TorchTensorWorker, 78adb68586299de3e0921b7c01000000)\n",
      "2025-05-15 15:46:23,696\tINFO compiled_dag_node.py:2161 -- Killing actor: Actor(TorchTensorWorker, 9a8e5f6450a76736020961fb01000000)\n",
      "2025-05-15 15:46:23,696\tINFO compiled_dag_node.py:2203 -- Teardown complete\n",
      "\n",
      "test_torch_tensor_dag.py::test_torch_tensor_shm[ray_start_regular0] 2025-05-15 15:46:27,430\tINFO worker.py:1879 -- Started a local Ray instance. View the dashboard at \u001b[1m\u001b[32mhttp://127.0.0.1:8265 \u001b[39m\u001b[22m\n",
      "2025-05-15 15:46:29,502\tINFO dag_node_operation.py:634 -- Writing compiled graph schedule visualization to compiled_graph_schedule.png\n",
      "2025-05-15 15:46:30,420\tINFO compiled_dag_node.py:2173 -- Tearing down compiled DAG\n",
      "2025-05-15 15:46:30,420\tINFO compiled_dag_node.py:2178 -- Cancelling compiled worker on actor: Actor(TorchTensorWorker, 5f9a13f8327e20a66e60f1b001000000)\n",
      "2025-05-15 15:46:30,420\tINFO compiled_dag_node.py:2178 -- Cancelling compiled worker on actor: Actor(TorchTensorWorker, cd1535002fab1824d36a2d6a01000000)\n",
      "2025-05-15 15:46:30,426\tINFO compiled_dag_node.py:2200 -- Waiting for worker tasks to exit\n",
      "2025-05-15 15:46:30,427\tINFO compiled_dag_node.py:2203 -- Teardown complete\n",
      "2025-05-15 15:46:30,469\tINFO dag_node_operation.py:634 -- Writing compiled graph schedule visualization to compiled_graph_schedule.png\n",
      "2025-05-15 15:46:30,602\tINFO compiled_dag_node.py:2173 -- Tearing down compiled DAG\n",
      "2025-05-15 15:46:30,603\tINFO compiled_dag_node.py:2178 -- Cancelling compiled worker on actor: Actor(TorchTensorWorker, 5f9a13f8327e20a66e60f1b001000000)\n",
      "2025-05-15 15:46:30,603\tINFO compiled_dag_node.py:2178 -- Cancelling compiled worker on actor: Actor(TorchTensorWorker, cd1535002fab1824d36a2d6a01000000)\n",
      "2025-05-15 15:46:30,607\tINFO compiled_dag_node.py:2200 -- Waiting for worker tasks to exit\n",
      "2025-05-15 15:46:30,607\tINFO compiled_dag_node.py:2203 -- Teardown complete\n",
      "\u001b[32mPASSED\u001b[0m\n",
      "test_torch_tensor_dag.py::test_torch_tensor_auto[num_gpus0-ray_start_regular0] 2025-05-15 15:46:34,094\tINFO worker.py:1879 -- Started a local Ray instance. View the dashboard at \u001b[1m\u001b[32mhttp://127.0.0.1:8265 \u001b[39m\u001b[22m\n",
      "2025-05-15 15:46:36,141\tINFO dag_node_operation.py:634 -- Writing compiled graph schedule visualization to compiled_graph_schedule.png\n",
      "2025-05-15 15:46:36,320\tINFO compiled_dag_node.py:2173 -- Tearing down compiled DAG\n",
      "2025-05-15 15:46:36,321\tINFO compiled_dag_node.py:2178 -- Cancelling compiled worker on actor: Actor(TorchTensorWorker, dac536f2343a1e625812c55901000000)\n",
      "2025-05-15 15:46:36,321\tINFO compiled_dag_node.py:2178 -- Cancelling compiled worker on actor: Actor(TorchTensorWorker, 6fb9036815f07fc9165ac09001000000)\n",
      "2025-05-15 15:46:36,328\tINFO compiled_dag_node.py:2200 -- Waiting for worker tasks to exit\n",
      "2025-05-15 15:46:36,328\tINFO compiled_dag_node.py:2203 -- Teardown complete\n",
      "2025-05-15 15:46:36,440\tINFO dag_node_operation.py:634 -- Writing compiled graph schedule visualization to compiled_graph_schedule.png\n",
      "\u001b[32mPASSED\u001b[0m2025-05-15 15:46:36,614\tINFO compiled_dag_node.py:2173 -- Tearing down compiled DAG\n",
      "2025-05-15 15:46:36,615\tINFO compiled_dag_node.py:2178 -- Cancelling compiled worker on actor: Actor(TorchTensorWorker, dac536f2343a1e625812c55901000000)\n",
      "2025-05-15 15:46:36,615\tINFO compiled_dag_node.py:2178 -- Cancelling compiled worker on actor: Actor(TorchTensorWorker, 6fb9036815f07fc9165ac09001000000)\n",
      "2025-05-15 15:46:36,622\tINFO compiled_dag_node.py:2200 -- Waiting for worker tasks to exit\n",
      "2025-05-15 15:46:36,622\tINFO compiled_dag_node.py:2161 -- Killing actor: Actor(TorchTensorWorker, dac536f2343a1e625812c55901000000)\n",
      "2025-05-15 15:46:36,622\tINFO compiled_dag_node.py:2161 -- Killing actor: Actor(TorchTensorWorker, 6fb9036815f07fc9165ac09001000000)\n",
      "2025-05-15 15:46:36,623\tINFO compiled_dag_node.py:2203 -- Teardown complete\n",
      "\n",
      "test_torch_tensor_dag.py::test_torch_tensor_auto[num_gpus1-ray_start_regular0] 2025-05-15 15:46:41,042\tINFO worker.py:1879 -- Started a local Ray instance. View the dashboard at \u001b[1m\u001b[32mhttp://127.0.0.1:8265 \u001b[39m\u001b[22m\n",
      "2025-05-15 15:46:43,270\tINFO dag_node_operation.py:634 -- Writing compiled graph schedule visualization to compiled_graph_schedule.png\n",
      "2025-05-15 15:46:43,807\tINFO compiled_dag_node.py:2173 -- Tearing down compiled DAG\n",
      "2025-05-15 15:46:43,807\tINFO compiled_dag_node.py:2178 -- Cancelling compiled worker on actor: Actor(TorchTensorWorker, e2409a353e092a10d4f887c501000000)\n",
      "2025-05-15 15:46:43,807\tINFO compiled_dag_node.py:2178 -- Cancelling compiled worker on actor: Actor(TorchTensorWorker, b77a0b9ed345716353d9036f01000000)\n",
      "2025-05-15 15:46:43,813\tINFO compiled_dag_node.py:2200 -- Waiting for worker tasks to exit\n",
      "2025-05-15 15:46:43,814\tINFO compiled_dag_node.py:2203 -- Teardown complete\n",
      "2025-05-15 15:46:43,894\tINFO dag_node_operation.py:634 -- Writing compiled graph schedule visualization to compiled_graph_schedule.png\n",
      "\u001b[32mPASSED\u001b[0m2025-05-15 15:46:44,033\tINFO compiled_dag_node.py:2173 -- Tearing down compiled DAG\n",
      "2025-05-15 15:46:44,034\tINFO compiled_dag_node.py:2178 -- Cancelling compiled worker on actor: Actor(TorchTensorWorker, e2409a353e092a10d4f887c501000000)\n",
      "2025-05-15 15:46:44,034\tINFO compiled_dag_node.py:2178 -- Cancelling compiled worker on actor: Actor(TorchTensorWorker, b77a0b9ed345716353d9036f01000000)\n",
      "2025-05-15 15:46:44,041\tINFO compiled_dag_node.py:2200 -- Waiting for worker tasks to exit\n",
      "2025-05-15 15:46:44,041\tINFO compiled_dag_node.py:2161 -- Killing actor: Actor(TorchTensorWorker, e2409a353e092a10d4f887c501000000)\n",
      "2025-05-15 15:46:44,042\tINFO compiled_dag_node.py:2161 -- Killing actor: Actor(TorchTensorWorker, b77a0b9ed345716353d9036f01000000)\n",
      "2025-05-15 15:46:44,042\tINFO compiled_dag_node.py:2203 -- Teardown complete\n",
      "\n",
      "test_torch_tensor_dag.py::test_torch_tensor_auto[num_gpus2-ray_start_regular0] 2025-05-15 15:46:47,468\tINFO worker.py:1879 -- Started a local Ray instance. View the dashboard at \u001b[1m\u001b[32mhttp://127.0.0.1:8265 \u001b[39m\u001b[22m\n",
      "2025-05-15 15:46:49,563\tINFO dag_node_operation.py:634 -- Writing compiled graph schedule visualization to compiled_graph_schedule.png\n",
      "2025-05-15 15:46:49,874\tINFO compiled_dag_node.py:2173 -- Tearing down compiled DAG\n",
      "2025-05-15 15:46:49,874\tINFO compiled_dag_node.py:2178 -- Cancelling compiled worker on actor: Actor(TorchTensorWorker, 443625b8fb75f26bfe3ab7b901000000)\n",
      "2025-05-15 15:46:49,874\tINFO compiled_dag_node.py:2178 -- Cancelling compiled worker on actor: Actor(TorchTensorWorker, 4b2f4ab017aea7e41163d19101000000)\n",
      "2025-05-15 15:46:49,880\tINFO compiled_dag_node.py:2200 -- Waiting for worker tasks to exit\n",
      "2025-05-15 15:46:49,880\tINFO compiled_dag_node.py:2203 -- Teardown complete\n",
      "2025-05-15 15:46:49,977\tINFO dag_node_operation.py:634 -- Writing compiled graph schedule visualization to compiled_graph_schedule.png\n",
      "\u001b[32mPASSED\u001b[0m2025-05-15 15:46:50,137\tINFO compiled_dag_node.py:2173 -- Tearing down compiled DAG\n",
      "2025-05-15 15:46:50,137\tINFO compiled_dag_node.py:2178 -- Cancelling compiled worker on actor: Actor(TorchTensorWorker, 443625b8fb75f26bfe3ab7b901000000)\n",
      "2025-05-15 15:46:50,137\tINFO compiled_dag_node.py:2178 -- Cancelling compiled worker on actor: Actor(TorchTensorWorker, 4b2f4ab017aea7e41163d19101000000)\n",
      "2025-05-15 15:46:50,144\tINFO compiled_dag_node.py:2200 -- Waiting for worker tasks to exit\n",
      "2025-05-15 15:46:50,145\tINFO compiled_dag_node.py:2161 -- Killing actor: Actor(TorchTensorWorker, 443625b8fb75f26bfe3ab7b901000000)\n",
      "2025-05-15 15:46:50,145\tINFO compiled_dag_node.py:2161 -- Killing actor: Actor(TorchTensorWorker, 4b2f4ab017aea7e41163d19101000000)\n",
      "2025-05-15 15:46:50,145\tINFO compiled_dag_node.py:2203 -- Teardown complete\n",
      "\n",
      "test_torch_tensor_dag.py::test_torch_tensor_auto[num_gpus3-ray_start_regular0] 2025-05-15 15:46:53,758\tINFO worker.py:1879 -- Started a local Ray instance. View the dashboard at \u001b[1m\u001b[32mhttp://127.0.0.1:8265 \u001b[39m\u001b[22m\n",
      "2025-05-15 15:46:55,962\tINFO torch_tensor_nccl_channel.py:772 -- Creating NCCL group feb2dbe5-2b5c-4a83-9e9f-ee2def021915 on actors: [Actor(TorchTensorWorker, a482b999fea16f56e477534601000000), Actor(TorchTensorWorker, d8a81498a057c15357f0ea1d01000000)]\n",
      "2025-05-15 15:46:57,035\tINFO torch_tensor_nccl_channel.py:797 -- NCCL group initialized.\n",
      "2025-05-15 15:46:57,195\tINFO dag_node_operation.py:634 -- Writing compiled graph schedule visualization to compiled_graph_schedule.png\n",
      "2025-05-15 15:46:57,391\tINFO compiled_dag_node.py:2173 -- Tearing down compiled DAG\n",
      "2025-05-15 15:46:57,392\tINFO compiled_dag_node.py:2178 -- Cancelling compiled worker on actor: Actor(TorchTensorWorker, d8a81498a057c15357f0ea1d01000000)\n",
      "2025-05-15 15:46:57,392\tINFO compiled_dag_node.py:2178 -- Cancelling compiled worker on actor: Actor(TorchTensorWorker, a482b999fea16f56e477534601000000)\n",
      "\u001b[36m(TorchTensorWorker pid=2116460)\u001b[0m Destructing NCCL group on actor: Actor(TorchTensorWorker, a482b999fea16f56e477534601000000)\n",
      "\u001b[36m(TorchTensorWorker pid=2116466)\u001b[0m Destructing NCCL group on actor: Actor(TorchTensorWorker, d8a81498a057c15357f0ea1d01000000)\n",
      "2025-05-15 15:46:57,907\tINFO compiled_dag_node.py:2200 -- Waiting for worker tasks to exit\n",
      "2025-05-15 15:46:57,907\tINFO compiled_dag_node.py:2203 -- Teardown complete\n",
      "2025-05-15 15:46:57,924\tINFO torch_tensor_nccl_channel.py:772 -- Creating NCCL group 1b46e831-39e9-4865-8a42-e5c48866d3b4 on actors: [Actor(TorchTensorWorker, a482b999fea16f56e477534601000000), Actor(TorchTensorWorker, d8a81498a057c15357f0ea1d01000000)]\n",
      "2025-05-15 15:46:57,963\tINFO torch_tensor_nccl_channel.py:797 -- NCCL group initialized.\n",
      "2025-05-15 15:46:58,079\tINFO dag_node_operation.py:634 -- Writing compiled graph schedule visualization to compiled_graph_schedule.png\n",
      "\u001b[32mPASSED\u001b[0m2025-05-15 15:46:58,246\tINFO compiled_dag_node.py:2173 -- Tearing down compiled DAG\n",
      "2025-05-15 15:46:58,247\tINFO compiled_dag_node.py:2178 -- Cancelling compiled worker on actor: Actor(TorchTensorWorker, d8a81498a057c15357f0ea1d01000000)\n",
      "2025-05-15 15:46:58,247\tINFO compiled_dag_node.py:2178 -- Cancelling compiled worker on actor: Actor(TorchTensorWorker, a482b999fea16f56e477534601000000)\n",
      "\u001b[36m(TorchTensorWorker pid=2116460)\u001b[0m Destructing NCCL group on actor: Actor(TorchTensorWorker, a482b999fea16f56e477534601000000)\n",
      "\u001b[36m(TorchTensorWorker pid=2116466)\u001b[0m Destructing NCCL group on actor: Actor(TorchTensorWorker, d8a81498a057c15357f0ea1d01000000)\n",
      "2025-05-15 15:46:58,749\tINFO compiled_dag_node.py:2200 -- Waiting for worker tasks to exit\n",
      "2025-05-15 15:46:58,749\tINFO compiled_dag_node.py:2161 -- Killing actor: Actor(TorchTensorWorker, d8a81498a057c15357f0ea1d01000000)\n",
      "2025-05-15 15:46:58,750\tINFO compiled_dag_node.py:2161 -- Killing actor: Actor(TorchTensorWorker, a482b999fea16f56e477534601000000)\n",
      "2025-05-15 15:46:58,750\tINFO compiled_dag_node.py:2203 -- Teardown complete\n",
      "\n",
      "test_torch_tensor_dag.py::test_torch_tensor_auto[num_gpus4-ray_start_regular0] 2025-05-15 15:47:02,408\tINFO worker.py:1879 -- Started a local Ray instance. View the dashboard at \u001b[1m\u001b[32mhttp://127.0.0.1:8265 \u001b[39m\u001b[22m\n",
      "2025-05-15 15:47:04,551\tINFO dag_node_operation.py:634 -- Writing compiled graph schedule visualization to compiled_graph_schedule.png\n",
      "2025-05-15 15:47:05,379\tINFO compiled_dag_node.py:2173 -- Tearing down compiled DAG\n",
      "2025-05-15 15:47:05,379\tINFO compiled_dag_node.py:2178 -- Cancelling compiled worker on actor: Actor(TorchTensorWorker, 34860ab52d1711dc012a096601000000)\n",
      "2025-05-15 15:47:05,379\tINFO compiled_dag_node.py:2178 -- Cancelling compiled worker on actor: Actor(TorchTensorWorker, d14bf322bec7d1f6bc25a28401000000)\n",
      "2025-05-15 15:47:05,386\tINFO compiled_dag_node.py:2200 -- Waiting for worker tasks to exit\n",
      "2025-05-15 15:47:05,386\tINFO compiled_dag_node.py:2203 -- Teardown complete\n",
      "2025-05-15 15:47:05,466\tINFO dag_node_operation.py:634 -- Writing compiled graph schedule visualization to compiled_graph_schedule.png\n",
      "\u001b[32mPASSED\u001b[0m2025-05-15 15:47:05,643\tINFO compiled_dag_node.py:2173 -- Tearing down compiled DAG\n",
      "2025-05-15 15:47:05,643\tINFO compiled_dag_node.py:2178 -- Cancelling compiled worker on actor: Actor(TorchTensorWorker, 34860ab52d1711dc012a096601000000)\n",
      "2025-05-15 15:47:05,644\tINFO compiled_dag_node.py:2178 -- Cancelling compiled worker on actor: Actor(TorchTensorWorker, d14bf322bec7d1f6bc25a28401000000)\n",
      "2025-05-15 15:47:05,650\tINFO compiled_dag_node.py:2200 -- Waiting for worker tasks to exit\n",
      "2025-05-15 15:47:05,650\tINFO compiled_dag_node.py:2161 -- Killing actor: Actor(TorchTensorWorker, 34860ab52d1711dc012a096601000000)\n",
      "2025-05-15 15:47:05,650\tINFO compiled_dag_node.py:2161 -- Killing actor: Actor(TorchTensorWorker, d14bf322bec7d1f6bc25a28401000000)\n",
      "2025-05-15 15:47:05,651\tINFO compiled_dag_node.py:2203 -- Teardown complete\n",
      "\n",
      "test_torch_tensor_dag.py::test_torch_tensor_nccl_overlap_timed[ray_start_regular0-False] 2025-05-15 15:47:10,234\tINFO worker.py:1879 -- Started a local Ray instance. View the dashboard at \u001b[1m\u001b[32mhttp://127.0.0.1:8265 \u001b[39m\u001b[22m\n",
      "\u001b[31mFAILED\u001b[0m\n",
      "test_torch_tensor_dag.py::test_torch_tensor_nccl_overlap_timed[ray_start_regular1-True] 2025-05-15 15:47:13,897\tINFO worker.py:1879 -- Started a local Ray instance. View the dashboard at \u001b[1m\u001b[32mhttp://127.0.0.1:8265 \u001b[39m\u001b[22m\n",
      "\u001b[31mFAILED\u001b[0m\n",
      "test_torch_tensor_dag.py::test_torch_tensor_nccl_disallows_driver 2025-05-15 15:47:17,528\tINFO worker.py:1879 -- Started a local Ray instance. View the dashboard at \u001b[1m\u001b[32mhttp://127.0.0.1:8265 \u001b[39m\u001b[22m\n",
      "\u001b[32mPASSED\u001b[0m\n",
      "test_torch_tensor_dag.py::test_torch_tensor_custom_comm[ray_start_regular0] 2025-05-15 15:47:23,968\tINFO worker.py:1879 -- Started a local Ray instance. View the dashboard at \u001b[1m\u001b[32mhttp://127.0.0.1:8265 \u001b[39m\u001b[22m\n",
      "2025-05-15 15:47:26,214\tINFO torch_tensor_nccl_channel.py:770 -- Initializing custom NCCL group e5b0bcf2-00a8-4da9-bf50-a0c1050d95f7 on actors: [Actor(TorchTensorWorker, ff574d0a66eebf3c6768dacf01000000), Actor(TorchTensorWorker, 82a9fcd8ae8f93e6d472f9ec01000000)]\n",
      "2025-05-15 15:47:27,113\tINFO torch_tensor_nccl_channel.py:797 -- NCCL group initialized.\n",
      "2025-05-15 15:47:27,280\tINFO dag_node_operation.py:634 -- Writing compiled graph schedule visualization to compiled_graph_schedule.png\n",
      "\u001b[32mPASSED\u001b[0m2025-05-15 15:47:27,488\tINFO compiled_dag_node.py:2173 -- Tearing down compiled DAG\n",
      "2025-05-15 15:47:27,489\tINFO compiled_dag_node.py:2178 -- Cancelling compiled worker on actor: Actor(TorchTensorWorker, ff574d0a66eebf3c6768dacf01000000)\n",
      "2025-05-15 15:47:27,489\tINFO compiled_dag_node.py:2178 -- Cancelling compiled worker on actor: Actor(TorchTensorWorker, 82a9fcd8ae8f93e6d472f9ec01000000)\n",
      "\u001b[36m(TorchTensorWorker pid=2122411)\u001b[0m Destructing NCCL group on actor: Actor(TorchTensorWorker, ff574d0a66eebf3c6768dacf01000000)\n",
      "\u001b[36m(TorchTensorWorker pid=2122410)\u001b[0m Destructing NCCL group on actor: Actor(TorchTensorWorker, 82a9fcd8ae8f93e6d472f9ec01000000)\n",
      "2025-05-15 15:47:28,009\tINFO compiled_dag_node.py:2200 -- Waiting for worker tasks to exit\n",
      "2025-05-15 15:47:28,010\tINFO compiled_dag_node.py:2161 -- Killing actor: Actor(TorchTensorWorker, ff574d0a66eebf3c6768dacf01000000)\n",
      "2025-05-15 15:47:28,010\tINFO compiled_dag_node.py:2161 -- Killing actor: Actor(TorchTensorWorker, 82a9fcd8ae8f93e6d472f9ec01000000)\n",
      "2025-05-15 15:47:28,010\tINFO compiled_dag_node.py:2203 -- Teardown complete\n",
      "\n",
      "test_torch_tensor_dag.py::test_torch_tensor_custom_comm_inited[ray_start_regular0] 2025-05-15 15:47:31,365\tINFO worker.py:1879 -- Started a local Ray instance. View the dashboard at \u001b[1m\u001b[32mhttp://127.0.0.1:8265 \u001b[39m\u001b[22m\n",
      "\u001b[33mSKIPPED\u001b[0m\n",
      "test_torch_tensor_dag.py::test_torch_tensor_default_comm[transports0-ray_start_regular0] 2025-05-15 15:47:34,952\tINFO worker.py:1879 -- Started a local Ray instance. View the dashboard at \u001b[1m\u001b[32mhttp://127.0.0.1:8265 \u001b[39m\u001b[22m\n",
      "\u001b[31mFAILED\u001b[0m\n",
      "test_torch_tensor_dag.py::test_torch_tensor_default_comm[transports1-ray_start_regular0] 2025-05-15 15:47:39,643\tINFO worker.py:1879 -- Started a local Ray instance. View the dashboard at \u001b[1m\u001b[32mhttp://127.0.0.1:8265 \u001b[39m\u001b[22m\n",
      "\u001b[31mFAILED\u001b[0m\n",
      "test_torch_tensor_dag.py::test_torch_tensor_default_comm[transports2-ray_start_regular0] 2025-05-15 15:47:43,544\tINFO worker.py:1879 -- Started a local Ray instance. View the dashboard at \u001b[1m\u001b[32mhttp://127.0.0.1:8265 \u001b[39m\u001b[22m\n",
      "\u001b[31mFAILED\u001b[0m\n",
      "test_torch_tensor_dag.py::test_torch_tensor_default_comm[transports3-ray_start_regular0] 2025-05-15 15:47:47,118\tINFO worker.py:1879 -- Started a local Ray instance. View the dashboard at \u001b[1m\u001b[32mhttp://127.0.0.1:8265 \u001b[39m\u001b[22m\n",
      "\u001b[31mFAILED\u001b[0m\n",
      "test_torch_tensor_dag.py::test_torch_tensor_invalid_custom_comm[ray_start_regular0] 2025-05-15 15:47:50,612\tINFO worker.py:1879 -- Started a local Ray instance. View the dashboard at \u001b[1m\u001b[32mhttp://127.0.0.1:8265 \u001b[39m\u001b[22m\n",
      "\u001b[33mSKIPPED\u001b[0m\n",
      "test_torch_tensor_dag.py::test_torch_tensor_nccl_static_shape[ray_start_regular0] 2025-05-15 15:47:54,221\tINFO worker.py:1879 -- Started a local Ray instance. View the dashboard at \u001b[1m\u001b[32mhttp://127.0.0.1:8265 \u001b[39m\u001b[22m\n",
      "2025-05-15 15:47:56,373\tINFO torch_tensor_nccl_channel.py:772 -- Creating NCCL group dbd52569-b14c-4e6b-8872-aaf1caa96d83 on actors: [Actor(TorchTensorWorker, 9d4ee7d18c34005eddb1e58401000000), Actor(TorchTensorWorker, 58cd3d436779cd05d566e4f701000000)]\n",
      "2025-05-15 15:47:57,450\tINFO torch_tensor_nccl_channel.py:797 -- NCCL group initialized.\n",
      "2025-05-15 15:47:57,581\tINFO dag_node_operation.py:634 -- Writing compiled graph schedule visualization to compiled_graph_schedule.png\n",
      "2025-05-15 15:47:57,790\tINFO compiled_dag_node.py:2173 -- Tearing down compiled DAG\n",
      "\u001b[32mPASSED\u001b[0m2025-05-15 15:47:57,792\tINFO compiled_dag_node.py:2178 -- Cancelling compiled worker on actor: Actor(TorchTensorWorker, 58cd3d436779cd05d566e4f701000000)\n",
      "2025-05-15 15:47:57,792\tINFO compiled_dag_node.py:2178 -- Cancelling compiled worker on actor: Actor(TorchTensorWorker, 9d4ee7d18c34005eddb1e58401000000)\n",
      "\u001b[36m(TorchTensorWorker pid=2130365)\u001b[0m ERROR:root:Compiled DAG task exited with exception\n",
      "\u001b[36m(TorchTensorWorker pid=2130365)\u001b[0m Traceback (most recent call last):\n",
      "\u001b[36m(TorchTensorWorker pid=2130365)\u001b[0m   File \"/home/wxdeng/stbr-min/python/ray/dag/compiled_dag_node.py\", line 253, in do_exec_tasks\n",
      "\u001b[36m(TorchTensorWorker pid=2130365)\u001b[0m     done = tasks[operation.exec_task_idx].exec_operation(\n",
      "\u001b[36m(TorchTensorWorker pid=2130365)\u001b[0m         self, operation.type, overlap_gpu_communication\n",
      "\u001b[36m(TorchTensorWorker pid=2130365)\u001b[0m     )\n",
      "\u001b[36m(TorchTensorWorker pid=2130365)\u001b[0m   File \"/home/wxdeng/stbr-min/python/ray/dag/compiled_dag_node.py\", line 786, in exec_operation\n",
      "\u001b[36m(TorchTensorWorker pid=2130365)\u001b[0m     return self._write()\n",
      "\u001b[36m(TorchTensorWorker pid=2130365)\u001b[0m            ~~~~~~~~~~~^^\n",
      "\u001b[36m(TorchTensorWorker pid=2130365)\u001b[0m   File \"/home/wxdeng/stbr-min/python/ray/dag/compiled_dag_node.py\", line 751, in _write\n",
      "\u001b[36m(TorchTensorWorker pid=2130365)\u001b[0m     self.output_writer.write(output_val)\n",
      "\u001b[36m(TorchTensorWorker pid=2130365)\u001b[0m     ~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^\n",
      "\u001b[36m(TorchTensorWorker pid=2130365)\u001b[0m   File \"/home/wxdeng/stbr-min/python/ray/experimental/channel/common.py\", line 617, in write\n",
      "\u001b[36m(TorchTensorWorker pid=2130365)\u001b[0m     channel.write(val_i, timeout)\n",
      "\u001b[36m(TorchTensorWorker pid=2130365)\u001b[0m     ~~~~~~~~~~~~~^^^^^^^^^^^^^^^^\n",
      "\u001b[36m(TorchTensorWorker pid=2130365)\u001b[0m   File \"/home/wxdeng/stbr-min/python/ray/experimental/channel/torch_tensor_nccl_channel.py\", line 270, in write\n",
      "\u001b[36m(TorchTensorWorker pid=2130365)\u001b[0m     self._send_cpu_and_gpu_data(value, timeout)\n",
      "\u001b[36m(TorchTensorWorker pid=2130365)\u001b[0m     ~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^\n",
      "\u001b[36m(TorchTensorWorker pid=2130365)\u001b[0m   File \"/home/wxdeng/stbr-min/python/ray/experimental/channel/torch_tensor_nccl_channel.py\", line 203, in _send_cpu_and_gpu_data\n",
      "\u001b[36m(TorchTensorWorker pid=2130365)\u001b[0m     self._gpu_data_channel.write(gpu_tensors)\n",
      "\u001b[36m(TorchTensorWorker pid=2130365)\u001b[0m     ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^\n",
      "\u001b[36m(TorchTensorWorker pid=2130365)\u001b[0m   File \"/home/wxdeng/stbr-min/python/ray/experimental/channel/torch_tensor_nccl_channel.py\", line 559, in write\n",
      "\u001b[36m(TorchTensorWorker pid=2130365)\u001b[0m     metadata = self._get_send_tensors_metadata(tensors)\n",
      "\u001b[36m(TorchTensorWorker pid=2130365)\u001b[0m   File \"/home/wxdeng/stbr-min/python/ray/experimental/channel/torch_tensor_nccl_channel.py\", line 514, in _get_send_tensors_metadata\n",
      "\u001b[36m(TorchTensorWorker pid=2130365)\u001b[0m     raise ValueError(\n",
      "\u001b[36m(TorchTensorWorker pid=2130365)\u001b[0m     ...<4 lines>...\n",
      "\u001b[36m(TorchTensorWorker pid=2130365)\u001b[0m     )\n",
      "\u001b[36m(TorchTensorWorker pid=2130365)\u001b[0m ValueError: Expected torch.Tensors with shapes and dtypes: [(shape=torch.Size([10]), dtype=torch.float16)], found: [(shape=torch.Size([20]), dtype=torch.float16)]. DAG will shut down.\n",
      "\u001b[36m(TorchTensorWorker pid=2130365)\u001b[0m Destructing NCCL group on actor: Actor(TorchTensorWorker, 58cd3d436779cd05d566e4f701000000)\n",
      "\u001b[36m(TorchTensorWorker pid=2130366)\u001b[0m Destructing NCCL group on actor: Actor(TorchTensorWorker, 9d4ee7d18c34005eddb1e58401000000)\n",
      "2025-05-15 15:47:58,280\tINFO compiled_dag_node.py:2200 -- Waiting for worker tasks to exit\n",
      "2025-05-15 15:47:58,280\tINFO compiled_dag_node.py:2203 -- Teardown complete\n",
      "\n",
      "test_torch_tensor_dag.py::test_torch_tensor_nccl_direct_return[ray_start_regular0] 2025-05-15 15:48:02,013\tINFO worker.py:1879 -- Started a local Ray instance. View the dashboard at \u001b[1m\u001b[32mhttp://127.0.0.1:8265 \u001b[39m\u001b[22m\n",
      "2025-05-15 15:48:04,342\tINFO torch_tensor_nccl_channel.py:772 -- Creating NCCL group 1a111bee-6742-4bc7-ace3-eca7f95ddb7c on actors: [Actor(TorchTensorWorker, 4f31a03be3a895b4e46e8a1a01000000), Actor(TorchTensorWorker, 0b2c7fde4e4cf66587bc16fc01000000)]\n",
      "2025-05-15 15:48:05,436\tINFO torch_tensor_nccl_channel.py:797 -- NCCL group initialized.\n",
      "2025-05-15 15:48:05,595\tINFO dag_node_operation.py:634 -- Writing compiled graph schedule visualization to compiled_graph_schedule.png\n",
      "2025-05-15 15:48:05,836\tINFO compiled_dag_node.py:2173 -- Tearing down compiled DAG\n",
      "\u001b[32mPASSED\u001b[0m2025-05-15 15:48:05,837\tINFO compiled_dag_node.py:2178 -- Cancelling compiled worker on actor: Actor(TorchTensorWorker, 4f31a03be3a895b4e46e8a1a01000000)\n",
      "2025-05-15 15:48:05,837\tINFO compiled_dag_node.py:2178 -- Cancelling compiled worker on actor: Actor(TorchTensorWorker, 0b2c7fde4e4cf66587bc16fc01000000)\n",
      "\u001b[36m(TorchTensorWorker pid=2131607)\u001b[0m Destructing NCCL group on actor: Actor(TorchTensorWorker, 0b2c7fde4e4cf66587bc16fc01000000)\n",
      "\u001b[36m(TorchTensorWorker pid=2131608)\u001b[0m ERROR:root:Compiled DAG task exited with exception\n",
      "\u001b[36m(TorchTensorWorker pid=2131608)\u001b[0m Traceback (most recent call last):\n",
      "\u001b[36m(TorchTensorWorker pid=2131608)\u001b[0m   File \"/home/wxdeng/stbr-min/python/ray/dag/compiled_dag_node.py\", line 253, in do_exec_tasks\n",
      "\u001b[36m(TorchTensorWorker pid=2131608)\u001b[0m     done = tasks[operation.exec_task_idx].exec_operation(\n",
      "\u001b[36m(TorchTensorWorker pid=2131608)\u001b[0m         self, operation.type, overlap_gpu_communication\n",
      "\u001b[36m(TorchTensorWorker pid=2131608)\u001b[0m     )\n",
      "\u001b[36m(TorchTensorWorker pid=2131608)\u001b[0m   File \"/home/wxdeng/stbr-min/python/ray/dag/compiled_dag_node.py\", line 786, in exec_operation\n",
      "\u001b[36m(TorchTensorWorker pid=2131608)\u001b[0m     return self._write()\n",
      "\u001b[36m(TorchTensorWorker pid=2131608)\u001b[0m            ~~~~~~~~~~~^^\n",
      "\u001b[36m(TorchTensorWorker pid=2131608)\u001b[0m   File \"/home/wxdeng/stbr-min/python/ray/dag/compiled_dag_node.py\", line 751, in _write\n",
      "\u001b[36m(TorchTensorWorker pid=2131608)\u001b[0m     self.output_writer.write(output_val)\n",
      "\u001b[36m(TorchTensorWorker pid=2131608)\u001b[0m     ~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^\n",
      "\u001b[36m(TorchTensorWorker pid=2131608)\u001b[0m   File \"/home/wxdeng/stbr-min/python/ray/experimental/channel/common.py\", line 617, in write\n",
      "\u001b[36m(TorchTensorWorker pid=2131608)\u001b[0m     channel.write(val_i, timeout)\n",
      "\u001b[36m(TorchTensorWorker pid=2131608)\u001b[0m     ~~~~~~~~~~~~~^^^^^^^^^^^^^^^^\n",
      "\u001b[36m(TorchTensorWorker pid=2131608)\u001b[0m   File \"/home/wxdeng/stbr-min/python/ray/experimental/channel/torch_tensor_nccl_channel.py\", line 257, in write\n",
      "\u001b[36m(TorchTensorWorker pid=2131608)\u001b[0m     raise ValueError(\n",
      "\u001b[36m(TorchTensorWorker pid=2131608)\u001b[0m     ...<3 lines>...\n",
      "\u001b[36m(TorchTensorWorker pid=2131608)\u001b[0m     )\n",
      "\u001b[36m(TorchTensorWorker pid=2131608)\u001b[0m ValueError: Task annotated with _direct_return=True must return a CUDA torch.Tensor, instead found value `1`. DAG will shut down.\n",
      "\u001b[36m(TorchTensorWorker pid=2131608)\u001b[0m Destructing NCCL group on actor: Actor(TorchTensorWorker, 4f31a03be3a895b4e46e8a1a01000000)\n",
      "2025-05-15 15:48:06,325\tINFO compiled_dag_node.py:2200 -- Waiting for worker tasks to exit\n",
      "2025-05-15 15:48:06,325\tINFO compiled_dag_node.py:2203 -- Teardown complete\n",
      "\n",
      "test_torch_tensor_dag.py::test_torch_tensor_nccl_nested_dynamic[ray_start_regular0] 2025-05-15 15:48:09,870\tINFO worker.py:1879 -- Started a local Ray instance. View the dashboard at \u001b[1m\u001b[32mhttp://127.0.0.1:8265 \u001b[39m\u001b[22m\n",
      "2025-05-15 15:48:12,138\tINFO torch_tensor_nccl_channel.py:772 -- Creating NCCL group 589ac19d-709e-4336-822f-fc1801242064 on actors: [Actor(TorchTensorWorker, 50499e16fdd70102e70e1c7701000000), Actor(TorchTensorWorker, 419e58c6a7cff37595acdd5701000000)]\n",
      "2025-05-15 15:48:13,231\tINFO torch_tensor_nccl_channel.py:797 -- NCCL group initialized.\n",
      "2025-05-15 15:48:13,299\tINFO dag_node_operation.py:634 -- Writing compiled graph schedule visualization to compiled_graph_schedule.png\n",
      "\u001b[32mPASSED\u001b[0m2025-05-15 15:48:13,443\tINFO compiled_dag_node.py:2173 -- Tearing down compiled DAG\n",
      "2025-05-15 15:48:13,443\tINFO compiled_dag_node.py:2178 -- Cancelling compiled worker on actor: Actor(TorchTensorWorker, 50499e16fdd70102e70e1c7701000000)\n",
      "2025-05-15 15:48:13,443\tINFO compiled_dag_node.py:2178 -- Cancelling compiled worker on actor: Actor(TorchTensorWorker, 419e58c6a7cff37595acdd5701000000)\n",
      "\u001b[36m(TorchTensorWorker pid=2132874)\u001b[0m Destructing NCCL group on actor: Actor(TorchTensorWorker, 50499e16fdd70102e70e1c7701000000)\n",
      "\u001b[36m(TorchTensorWorker pid=2132875)\u001b[0m Destructing NCCL group on actor: Actor(TorchTensorWorker, 419e58c6a7cff37595acdd5701000000)\n",
      "2025-05-15 15:48:14,238\tINFO compiled_dag_node.py:2200 -- Waiting for worker tasks to exit\n",
      "2025-05-15 15:48:14,239\tINFO compiled_dag_node.py:2161 -- Killing actor: Actor(TorchTensorWorker, 50499e16fdd70102e70e1c7701000000)\n",
      "2025-05-15 15:48:14,239\tINFO compiled_dag_node.py:2161 -- Killing actor: Actor(TorchTensorWorker, 419e58c6a7cff37595acdd5701000000)\n",
      "2025-05-15 15:48:14,239\tINFO compiled_dag_node.py:2203 -- Teardown complete\n",
      "\n",
      "test_torch_tensor_dag.py::test_torch_tensor_exceptions[False-False-False-ray_start_regular0] 2025-05-15 15:48:18,941\tINFO worker.py:1879 -- Started a local Ray instance. View the dashboard at \u001b[1m\u001b[32mhttp://127.0.0.1:8265 \u001b[39m\u001b[22m\n",
      "2025-05-15 15:48:21,188\tINFO torch_tensor_nccl_channel.py:772 -- Creating NCCL group 66785575-7b76-483b-b7e3-4049cf5b8711 on actors: [Actor(TorchTensorWorker, fcdd1eb61bf2b1981966fc6701000000), Actor(TorchTensorWorker, 95e1a8e49dc152004d2b2ad501000000)]\n",
      "2025-05-15 15:48:22,261\tINFO torch_tensor_nccl_channel.py:797 -- NCCL group initialized.\n",
      "2025-05-15 15:48:22,446\tINFO dag_node_operation.py:634 -- Writing compiled graph schedule visualization to compiled_graph_schedule.png\n",
      "\u001b[32mPASSED\u001b[0m2025-05-15 15:48:22,684\tINFO compiled_dag_node.py:2173 -- Tearing down compiled DAG\n",
      "2025-05-15 15:48:22,685\tINFO compiled_dag_node.py:2178 -- Cancelling compiled worker on actor: Actor(TorchTensorWorker, fcdd1eb61bf2b1981966fc6701000000)\n",
      "2025-05-15 15:48:22,685\tINFO compiled_dag_node.py:2178 -- Cancelling compiled worker on actor: Actor(TorchTensorWorker, 95e1a8e49dc152004d2b2ad501000000)\n",
      "\u001b[36m(TorchTensorWorker pid=2134166)\u001b[0m Destructing NCCL group on actor: Actor(TorchTensorWorker, 95e1a8e49dc152004d2b2ad501000000)\n",
      "\u001b[36m(TorchTensorWorker pid=2134168)\u001b[0m Destructing NCCL group on actor: Actor(TorchTensorWorker, fcdd1eb61bf2b1981966fc6701000000)\n",
      "2025-05-15 15:48:23,204\tINFO compiled_dag_node.py:2200 -- Waiting for worker tasks to exit\n",
      "2025-05-15 15:48:23,204\tINFO compiled_dag_node.py:2161 -- Killing actor: Actor(TorchTensorWorker, fcdd1eb61bf2b1981966fc6701000000)\n",
      "2025-05-15 15:48:23,205\tINFO compiled_dag_node.py:2161 -- Killing actor: Actor(TorchTensorWorker, 95e1a8e49dc152004d2b2ad501000000)\n",
      "2025-05-15 15:48:23,205\tINFO compiled_dag_node.py:2203 -- Teardown complete\n",
      "\n",
      "test_torch_tensor_dag.py::test_torch_tensor_exceptions[False-False-True-ray_start_regular0] 2025-05-15 15:48:26,945\tINFO worker.py:1879 -- Started a local Ray instance. View the dashboard at \u001b[1m\u001b[32mhttp://127.0.0.1:8265 \u001b[39m\u001b[22m\n",
      "2025-05-15 15:48:29,188\tINFO torch_tensor_nccl_channel.py:772 -- Creating NCCL group ae5ce110-2a34-4e13-ad50-91080d131de0 on actors: [Actor(TorchTensorWorker, a29702dc629330000ed45f4101000000), Actor(TorchTensorWorker, f7fd8b52d213826a28e057da01000000)]\n",
      "2025-05-15 15:48:30,272\tINFO torch_tensor_nccl_channel.py:797 -- NCCL group initialized.\n",
      "2025-05-15 15:48:30,457\tINFO dag_node_operation.py:634 -- Writing compiled graph schedule visualization to compiled_graph_schedule.png\n",
      "2025-05-15 15:48:30,665\tINFO compiled_dag_node.py:2173 -- Tearing down compiled DAG\n",
      "\u001b[32mPASSED\u001b[0m2025-05-15 15:48:30,667\tINFO compiled_dag_node.py:2178 -- Cancelling compiled worker on actor: Actor(TorchTensorWorker, a29702dc629330000ed45f4101000000)\n",
      "2025-05-15 15:48:30,667\tINFO compiled_dag_node.py:2178 -- Cancelling compiled worker on actor: Actor(TorchTensorWorker, f7fd8b52d213826a28e057da01000000)\n",
      "\u001b[36m(TorchTensorWorker pid=2135385)\u001b[0m ERROR:root:Compiled DAG task exited with exception\n",
      "\u001b[36m(TorchTensorWorker pid=2135385)\u001b[0m Traceback (most recent call last):\n",
      "\u001b[36m(TorchTensorWorker pid=2135385)\u001b[0m   File \"/home/wxdeng/stbr-min/python/ray/dag/compiled_dag_node.py\", line 253, in do_exec_tasks\n",
      "\u001b[36m(TorchTensorWorker pid=2135385)\u001b[0m     done = tasks[operation.exec_task_idx].exec_operation(\n",
      "\u001b[36m(TorchTensorWorker pid=2135385)\u001b[0m         self, operation.type, overlap_gpu_communication\n",
      "\u001b[36m(TorchTensorWorker pid=2135385)\u001b[0m     )\n",
      "\u001b[36m(TorchTensorWorker pid=2135385)\u001b[0m   File \"/home/wxdeng/stbr-min/python/ray/dag/compiled_dag_node.py\", line 786, in exec_operation\n",
      "\u001b[36m(TorchTensorWorker pid=2135385)\u001b[0m     return self._write()\n",
      "\u001b[36m(TorchTensorWorker pid=2135385)\u001b[0m            ~~~~~~~~~~~^^\n",
      "\u001b[36m(TorchTensorWorker pid=2135385)\u001b[0m   File \"/home/wxdeng/stbr-min/python/ray/dag/compiled_dag_node.py\", line 751, in _write\n",
      "\u001b[36m(TorchTensorWorker pid=2135385)\u001b[0m     self.output_writer.write(output_val)\n",
      "\u001b[36m(TorchTensorWorker pid=2135385)\u001b[0m     ~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^\n",
      "\u001b[36m(TorchTensorWorker pid=2135385)\u001b[0m   File \"/home/wxdeng/stbr-min/python/ray/experimental/channel/common.py\", line 617, in write\n",
      "\u001b[36m(TorchTensorWorker pid=2135385)\u001b[0m     channel.write(val_i, timeout)\n",
      "\u001b[36m(TorchTensorWorker pid=2135385)\u001b[0m     ~~~~~~~~~~~~~^^^^^^^^^^^^^^^^\n",
      "\u001b[36m(TorchTensorWorker pid=2135385)\u001b[0m   File \"/home/wxdeng/stbr-min/python/ray/experimental/channel/torch_tensor_nccl_channel.py\", line 243, in write\n",
      "\u001b[36m(TorchTensorWorker pid=2135385)\u001b[0m     raise value\n",
      "\u001b[36m(TorchTensorWorker pid=2135385)\u001b[0m ray.exceptions.RayTaskError: \u001b[36mray::TorchTensorWorker.__ray_call__()\u001b[39m (pid=2135385, ip=128.208.3.124)\n",
      "\u001b[36m(TorchTensorWorker pid=2135385)\u001b[0m   File \"/home/wxdeng/stbr-min/python/ray/dag/tests/experimental/test_torch_tensor_dag.py\", line 67, in send_or_raise\n",
      "\u001b[36m(TorchTensorWorker pid=2135385)\u001b[0m     raise RuntimeError()\n",
      "\u001b[36m(TorchTensorWorker pid=2135385)\u001b[0m RuntimeError\n",
      "\u001b[36m(TorchTensorWorker pid=2135385)\u001b[0m Destructing NCCL group on actor: Actor(TorchTensorWorker, a29702dc629330000ed45f4101000000)\n",
      "\u001b[36m(TorchTensorWorker pid=2135386)\u001b[0m Destructing NCCL group on actor: Actor(TorchTensorWorker, f7fd8b52d213826a28e057da01000000)\n",
      "2025-05-15 15:48:31,153\tINFO compiled_dag_node.py:2200 -- Waiting for worker tasks to exit\n",
      "2025-05-15 15:48:31,154\tINFO compiled_dag_node.py:2203 -- Teardown complete\n",
      "\n",
      "test_torch_tensor_dag.py::test_torch_tensor_exceptions[False-True-False-ray_start_regular0] 2025-05-15 15:48:34,648\tINFO worker.py:1879 -- Started a local Ray instance. View the dashboard at \u001b[1m\u001b[32mhttp://127.0.0.1:8265 \u001b[39m\u001b[22m\n",
      "2025-05-15 15:48:36,873\tINFO torch_tensor_nccl_channel.py:772 -- Creating NCCL group 61a68411-1ad6-49c6-b539-cdf23acfe773 on actors: [Actor(TorchTensorWorker, 1f5eb4e64b2d0f420dc6ba2701000000), Actor(TorchTensorWorker, 8f758ca94d734b5722ed6efb01000000)]\n",
      "2025-05-15 15:48:37,942\tINFO torch_tensor_nccl_channel.py:797 -- NCCL group initialized.\n",
      "2025-05-15 15:48:38,124\tINFO dag_node_operation.py:634 -- Writing compiled graph schedule visualization to compiled_graph_schedule.png\n",
      "2025-05-15 15:48:38,334\tINFO compiled_dag_node.py:2173 -- Tearing down compiled DAG\n",
      "\u001b[32mPASSED\u001b[0m2025-05-15 15:48:38,336\tINFO compiled_dag_node.py:2178 -- Cancelling compiled worker on actor: Actor(TorchTensorWorker, 1f5eb4e64b2d0f420dc6ba2701000000)\n",
      "2025-05-15 15:48:38,336\tINFO compiled_dag_node.py:2178 -- Cancelling compiled worker on actor: Actor(TorchTensorWorker, 8f758ca94d734b5722ed6efb01000000)\n",
      "\u001b[36m(TorchTensorWorker pid=2136618)\u001b[0m Destructing NCCL group on actor: Actor(TorchTensorWorker, 8f758ca94d734b5722ed6efb01000000)\n",
      "\u001b[36m(TorchTensorWorker pid=2136619)\u001b[0m ERROR:root:Compiled DAG task exited with exception\n",
      "\u001b[36m(TorchTensorWorker pid=2136619)\u001b[0m Traceback (most recent call last):\n",
      "\u001b[36m(TorchTensorWorker pid=2136619)\u001b[0m   File \"/home/wxdeng/stbr-min/python/ray/dag/compiled_dag_node.py\", line 253, in do_exec_tasks\n",
      "\u001b[36m(TorchTensorWorker pid=2136619)\u001b[0m     done = tasks[operation.exec_task_idx].exec_operation(\n",
      "\u001b[36m(TorchTensorWorker pid=2136619)\u001b[0m         self, operation.type, overlap_gpu_communication\n",
      "\u001b[36m(TorchTensorWorker pid=2136619)\u001b[0m     )\n",
      "\u001b[36m(TorchTensorWorker pid=2136619)\u001b[0m   File \"/home/wxdeng/stbr-min/python/ray/dag/compiled_dag_node.py\", line 786, in exec_operation\n",
      "\u001b[36m(TorchTensorWorker pid=2136619)\u001b[0m     return self._write()\n",
      "\u001b[36m(TorchTensorWorker pid=2136619)\u001b[0m            ~~~~~~~~~~~^^\n",
      "\u001b[36m(TorchTensorWorker pid=2136619)\u001b[0m   File \"/home/wxdeng/stbr-min/python/ray/dag/compiled_dag_node.py\", line 751, in _write\n",
      "\u001b[36m(TorchTensorWorker pid=2136619)\u001b[0m     self.output_writer.write(output_val)\n",
      "\u001b[36m(TorchTensorWorker pid=2136619)\u001b[0m     ~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^\n",
      "\u001b[36m(TorchTensorWorker pid=2136619)\u001b[0m   File \"/home/wxdeng/stbr-min/python/ray/experimental/channel/common.py\", line 617, in write\n",
      "\u001b[36m(TorchTensorWorker pid=2136619)\u001b[0m     channel.write(val_i, timeout)\n",
      "\u001b[36m(TorchTensorWorker pid=2136619)\u001b[0m     ~~~~~~~~~~~~~^^^^^^^^^^^^^^^^\n",
      "\u001b[36m(TorchTensorWorker pid=2136619)\u001b[0m   File \"/home/wxdeng/stbr-min/python/ray/experimental/channel/torch_tensor_nccl_channel.py\", line 243, in write\n",
      "\u001b[36m(TorchTensorWorker pid=2136619)\u001b[0m     raise value\n",
      "\u001b[36m(TorchTensorWorker pid=2136619)\u001b[0m ray.exceptions.RayTaskError: \u001b[36mray::TorchTensorWorker.__ray_call__()\u001b[39m (pid=2136619, ip=128.208.3.124)\n",
      "\u001b[36m(TorchTensorWorker pid=2136619)\u001b[0m   File \"/home/wxdeng/stbr-min/python/ray/dag/tests/experimental/test_torch_tensor_dag.py\", line 67, in send_or_raise\n",
      "\u001b[36m(TorchTensorWorker pid=2136619)\u001b[0m     raise RuntimeError()\n",
      "\u001b[36m(TorchTensorWorker pid=2136619)\u001b[0m RuntimeError\n",
      "\u001b[36m(TorchTensorWorker pid=2136619)\u001b[0m Destructing NCCL group on actor: Actor(TorchTensorWorker, 1f5eb4e64b2d0f420dc6ba2701000000)\n",
      "2025-05-15 15:48:38,854\tINFO compiled_dag_node.py:2200 -- Waiting for worker tasks to exit\n",
      "2025-05-15 15:48:38,855\tINFO compiled_dag_node.py:2203 -- Teardown complete\n",
      "\n",
      "test_torch_tensor_dag.py::test_torch_tensor_exceptions[False-True-True-ray_start_regular0] 2025-05-15 15:48:42,448\tINFO worker.py:1879 -- Started a local Ray instance. View the dashboard at \u001b[1m\u001b[32mhttp://127.0.0.1:8265 \u001b[39m\u001b[22m\n",
      "2025-05-15 15:48:44,637\tINFO torch_tensor_nccl_channel.py:772 -- Creating NCCL group a47984f3-8d8b-46a1-ad73-d872798b6664 on actors: [Actor(TorchTensorWorker, 39fc44c1478eb2ecee0fa4b201000000), Actor(TorchTensorWorker, f1597c5fb6dedd7861c7154601000000)]\n",
      "2025-05-15 15:48:45,709\tINFO torch_tensor_nccl_channel.py:797 -- NCCL group initialized.\n",
      "2025-05-15 15:48:45,881\tINFO dag_node_operation.py:634 -- Writing compiled graph schedule visualization to compiled_graph_schedule.png\n",
      "2025-05-15 15:48:46,089\tINFO compiled_dag_node.py:2173 -- Tearing down compiled DAG\n",
      "\u001b[32mPASSED\u001b[0m2025-05-15 15:48:46,091\tINFO compiled_dag_node.py:2178 -- Cancelling compiled worker on actor: Actor(TorchTensorWorker, f1597c5fb6dedd7861c7154601000000)\n",
      "2025-05-15 15:48:46,092\tINFO compiled_dag_node.py:2178 -- Cancelling compiled worker on actor: Actor(TorchTensorWorker, 39fc44c1478eb2ecee0fa4b201000000)\n",
      "\u001b[36m(TorchTensorWorker pid=2137859)\u001b[0m Destructing NCCL group on actor: Actor(TorchTensorWorker, 39fc44c1478eb2ecee0fa4b201000000)\n",
      "\u001b[36m(TorchTensorWorker pid=2137860)\u001b[0m ERROR:root:Compiled DAG task exited with exception\n",
      "\u001b[36m(TorchTensorWorker pid=2137860)\u001b[0m Traceback (most recent call last):\n",
      "\u001b[36m(TorchTensorWorker pid=2137860)\u001b[0m   File \"/home/wxdeng/stbr-min/python/ray/dag/compiled_dag_node.py\", line 253, in do_exec_tasks\n",
      "\u001b[36m(TorchTensorWorker pid=2137860)\u001b[0m     done = tasks[operation.exec_task_idx].exec_operation(\n",
      "\u001b[36m(TorchTensorWorker pid=2137860)\u001b[0m         self, operation.type, overlap_gpu_communication\n",
      "\u001b[36m(TorchTensorWorker pid=2137860)\u001b[0m     )\n",
      "\u001b[36m(TorchTensorWorker pid=2137860)\u001b[0m   File \"/home/wxdeng/stbr-min/python/ray/dag/compiled_dag_node.py\", line 786, in exec_operation\n",
      "\u001b[36m(TorchTensorWorker pid=2137860)\u001b[0m     return self._write()\n",
      "\u001b[36m(TorchTensorWorker pid=2137860)\u001b[0m            ~~~~~~~~~~~^^\n",
      "\u001b[36m(TorchTensorWorker pid=2137860)\u001b[0m   File \"/home/wxdeng/stbr-min/python/ray/dag/compiled_dag_node.py\", line 751, in _write\n",
      "\u001b[36m(TorchTensorWorker pid=2137860)\u001b[0m     self.output_writer.write(output_val)\n",
      "\u001b[36m(TorchTensorWorker pid=2137860)\u001b[0m     ~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^\n",
      "\u001b[36m(TorchTensorWorker pid=2137860)\u001b[0m   File \"/home/wxdeng/stbr-min/python/ray/experimental/channel/common.py\", line 617, in write\n",
      "\u001b[36m(TorchTensorWorker pid=2137860)\u001b[0m     channel.write(val_i, timeout)\n",
      "\u001b[36m(TorchTensorWorker pid=2137860)\u001b[0m     ~~~~~~~~~~~~~^^^^^^^^^^^^^^^^\n",
      "\u001b[36m(TorchTensorWorker pid=2137860)\u001b[0m   File \"/home/wxdeng/stbr-min/python/ray/experimental/channel/torch_tensor_nccl_channel.py\", line 243, in write\n",
      "\u001b[36m(TorchTensorWorker pid=2137860)\u001b[0m     raise value\n",
      "\u001b[36m(TorchTensorWorker pid=2137860)\u001b[0m ray.exceptions.RayTaskError: \u001b[36mray::TorchTensorWorker.__ray_call__()\u001b[39m (pid=2137860, ip=128.208.3.124)\n",
      "\u001b[36m(TorchTensorWorker pid=2137860)\u001b[0m   File \"/home/wxdeng/stbr-min/python/ray/dag/tests/experimental/test_torch_tensor_dag.py\", line 67, in send_or_raise\n",
      "\u001b[36m(TorchTensorWorker pid=2137860)\u001b[0m     raise RuntimeError()\n",
      "\u001b[36m(TorchTensorWorker pid=2137860)\u001b[0m RuntimeError\n",
      "\u001b[36m(TorchTensorWorker pid=2137860)\u001b[0m Destructing NCCL group on actor: Actor(TorchTensorWorker, f1597c5fb6dedd7861c7154601000000)\n",
      "2025-05-15 15:48:46,579\tINFO compiled_dag_node.py:2200 -- Waiting for worker tasks to exit\n",
      "2025-05-15 15:48:46,579\tINFO compiled_dag_node.py:2203 -- Teardown complete\n",
      "\n",
      "test_torch_tensor_dag.py::test_torch_tensor_exceptions[True-False-False-ray_start_regular0] 2025-05-15 15:48:50,041\tINFO worker.py:1879 -- Started a local Ray instance. View the dashboard at \u001b[1m\u001b[32mhttp://127.0.0.1:8265 \u001b[39m\u001b[22m\n",
      "2025-05-15 15:48:52,295\tINFO torch_tensor_nccl_channel.py:772 -- Creating NCCL group 7f988e9e-73f5-4bf9-8e74-5448b3b03759 on actors: [Actor(TorchTensorWorker, 37769b7d53b6287e41282dc601000000), Actor(TorchTensorWorker, 85006c8763699da43c36922401000000)]\n",
      "2025-05-15 15:48:53,364\tINFO torch_tensor_nccl_channel.py:797 -- NCCL group initialized.\n",
      "2025-05-15 15:48:53,549\tINFO dag_node_operation.py:634 -- Writing compiled graph schedule visualization to compiled_graph_schedule.png\n",
      "\u001b[32mPASSED\u001b[0m2025-05-15 15:48:53,753\tINFO compiled_dag_node.py:2173 -- Tearing down compiled DAG\n",
      "2025-05-15 15:48:53,754\tINFO compiled_dag_node.py:2178 -- Cancelling compiled worker on actor: Actor(TorchTensorWorker, 37769b7d53b6287e41282dc601000000)\n",
      "2025-05-15 15:48:53,754\tINFO compiled_dag_node.py:2178 -- Cancelling compiled worker on actor: Actor(TorchTensorWorker, 85006c8763699da43c36922401000000)\n",
      "\u001b[36m(TorchTensorWorker pid=2139130)\u001b[0m Destructing NCCL group on actor: Actor(TorchTensorWorker, 37769b7d53b6287e41282dc601000000)\n",
      "\u001b[36m(TorchTensorWorker pid=2139131)\u001b[0m Destructing NCCL group on actor: Actor(TorchTensorWorker, 85006c8763699da43c36922401000000)\n",
      "2025-05-15 15:48:54,269\tINFO compiled_dag_node.py:2200 -- Waiting for worker tasks to exit\n",
      "2025-05-15 15:48:54,269\tINFO compiled_dag_node.py:2161 -- Killing actor: Actor(TorchTensorWorker, 37769b7d53b6287e41282dc601000000)\n",
      "2025-05-15 15:48:54,270\tINFO compiled_dag_node.py:2161 -- Killing actor: Actor(TorchTensorWorker, 85006c8763699da43c36922401000000)\n",
      "2025-05-15 15:48:54,270\tINFO compiled_dag_node.py:2203 -- Teardown complete\n",
      "\n",
      "test_torch_tensor_dag.py::test_torch_tensor_exceptions[True-False-True-ray_start_regular0] 2025-05-15 15:48:57,952\tINFO worker.py:1879 -- Started a local Ray instance. View the dashboard at \u001b[1m\u001b[32mhttp://127.0.0.1:8265 \u001b[39m\u001b[22m\n",
      "2025-05-15 15:49:00,129\tINFO torch_tensor_nccl_channel.py:772 -- Creating NCCL group f6d3cd1a-407a-42a4-ab6f-c4b370a9f84a on actors: [Actor(TorchTensorWorker, 4d0403b3c345cc8fe905039301000000), Actor(TorchTensorWorker, 009708b4e2a4345e1c731a8e01000000)]\n",
      "2025-05-15 15:49:01,209\tINFO torch_tensor_nccl_channel.py:797 -- NCCL group initialized.\n",
      "2025-05-15 15:49:01,399\tINFO dag_node_operation.py:634 -- Writing compiled graph schedule visualization to compiled_graph_schedule.png\n",
      "2025-05-15 15:49:01,608\tINFO compiled_dag_node.py:2173 -- Tearing down compiled DAG\n",
      "\u001b[32mPASSED\u001b[0m2025-05-15 15:49:01,610\tINFO compiled_dag_node.py:2178 -- Cancelling compiled worker on actor: Actor(TorchTensorWorker, 009708b4e2a4345e1c731a8e01000000)\n",
      "2025-05-15 15:49:01,610\tINFO compiled_dag_node.py:2178 -- Cancelling compiled worker on actor: Actor(TorchTensorWorker, 4d0403b3c345cc8fe905039301000000)\n",
      "\u001b[36m(TorchTensorWorker pid=2140378)\u001b[0m ERROR:root:Compiled DAG task exited with exception\n",
      "\u001b[36m(TorchTensorWorker pid=2140378)\u001b[0m Traceback (most recent call last):\n",
      "\u001b[36m(TorchTensorWorker pid=2140378)\u001b[0m   File \"/home/wxdeng/stbr-min/python/ray/dag/compiled_dag_node.py\", line 253, in do_exec_tasks\n",
      "\u001b[36m(TorchTensorWorker pid=2140378)\u001b[0m     done = tasks[operation.exec_task_idx].exec_operation(\n",
      "\u001b[36m(TorchTensorWorker pid=2140378)\u001b[0m         self, operation.type, overlap_gpu_communication\n",
      "\u001b[36m(TorchTensorWorker pid=2140378)\u001b[0m     )\n",
      "\u001b[36m(TorchTensorWorker pid=2140378)\u001b[0m   File \"/home/wxdeng/stbr-min/python/ray/dag/compiled_dag_node.py\", line 786, in exec_operation\n",
      "\u001b[36m(TorchTensorWorker pid=2140378)\u001b[0m     return self._write()\n",
      "\u001b[36m(TorchTensorWorker pid=2140378)\u001b[0m            ~~~~~~~~~~~^^\n",
      "\u001b[36m(TorchTensorWorker pid=2140378)\u001b[0m   File \"/home/wxdeng/stbr-min/python/ray/dag/compiled_dag_node.py\", line 751, in _write\n",
      "\u001b[36m(TorchTensorWorker pid=2140378)\u001b[0m     self.output_writer.write(output_val)\n",
      "\u001b[36m(TorchTensorWorker pid=2140378)\u001b[0m     ~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^\n",
      "\u001b[36m(TorchTensorWorker pid=2140378)\u001b[0m   File \"/home/wxdeng/stbr-min/python/ray/experimental/channel/common.py\", line 617, in write\n",
      "\u001b[36m(TorchTensorWorker pid=2140378)\u001b[0m     channel.write(val_i, timeout)\n",
      "\u001b[36m(TorchTensorWorker pid=2140378)\u001b[0m     ~~~~~~~~~~~~~^^^^^^^^^^^^^^^^\n",
      "\u001b[36m(TorchTensorWorker pid=2140378)\u001b[0m   File \"/home/wxdeng/stbr-min/python/ray/experimental/channel/torch_tensor_nccl_channel.py\", line 243, in write\n",
      "\u001b[36m(TorchTensorWorker pid=2140378)\u001b[0m     raise value\n",
      "\u001b[36m(TorchTensorWorker pid=2140378)\u001b[0m ray.exceptions.RayTaskError: \u001b[36mray::TorchTensorWorker.__ray_call__()\u001b[39m (pid=2140378, ip=128.208.3.124)\n",
      "\u001b[36m(TorchTensorWorker pid=2140378)\u001b[0m   File \"/home/wxdeng/stbr-min/python/ray/dag/tests/experimental/test_torch_tensor_dag.py\", line 67, in send_or_raise\n",
      "\u001b[36m(TorchTensorWorker pid=2140378)\u001b[0m     raise RuntimeError()\n",
      "\u001b[36m(TorchTensorWorker pid=2140378)\u001b[0m RuntimeError\n",
      "\u001b[36m(TorchTensorWorker pid=2140378)\u001b[0m Destructing NCCL group on actor: Actor(TorchTensorWorker, 009708b4e2a4345e1c731a8e01000000)\n",
      "\u001b[36m(TorchTensorWorker pid=2140380)\u001b[0m Destructing NCCL group on actor: Actor(TorchTensorWorker, 4d0403b3c345cc8fe905039301000000)\n",
      "2025-05-15 15:49:02,128\tINFO compiled_dag_node.py:2200 -- Waiting for worker tasks to exit\n",
      "2025-05-15 15:49:02,129\tINFO compiled_dag_node.py:2203 -- Teardown complete\n",
      "\n",
      "test_torch_tensor_dag.py::test_torch_tensor_exceptions[True-True-False-ray_start_regular0] 2025-05-15 15:49:06,679\tINFO worker.py:1879 -- Started a local Ray instance. View the dashboard at \u001b[1m\u001b[32mhttp://127.0.0.1:8265 \u001b[39m\u001b[22m\n",
      "2025-05-15 15:49:08,903\tINFO torch_tensor_nccl_channel.py:772 -- Creating NCCL group 44f1924b-44df-4e75-9990-75a0849434e9 on actors: [Actor(TorchTensorWorker, c60c12b6f9f1be08a5c3178f01000000), Actor(TorchTensorWorker, b37bce0e7e7be5993661fad901000000)]\n",
      "2025-05-15 15:49:09,973\tINFO torch_tensor_nccl_channel.py:797 -- NCCL group initialized.\n",
      "2025-05-15 15:49:10,149\tINFO dag_node_operation.py:634 -- Writing compiled graph schedule visualization to compiled_graph_schedule.png\n",
      "2025-05-15 15:49:10,352\tINFO compiled_dag_node.py:2173 -- Tearing down compiled DAG\n",
      "\u001b[32mPASSED\u001b[0m2025-05-15 15:49:10,355\tINFO compiled_dag_node.py:2178 -- Cancelling compiled worker on actor: Actor(TorchTensorWorker, c60c12b6f9f1be08a5c3178f01000000)\n",
      "2025-05-15 15:49:10,355\tINFO compiled_dag_node.py:2178 -- Cancelling compiled worker on actor: Actor(TorchTensorWorker, b37bce0e7e7be5993661fad901000000)\n",
      "\u001b[36m(TorchTensorWorker pid=2141643)\u001b[0m Destructing NCCL group on actor: Actor(TorchTensorWorker, b37bce0e7e7be5993661fad901000000)\n",
      "\u001b[36m(TorchTensorWorker pid=2141642)\u001b[0m ERROR:root:Compiled DAG task exited with exception\n",
      "\u001b[36m(TorchTensorWorker pid=2141642)\u001b[0m Traceback (most recent call last):\n",
      "\u001b[36m(TorchTensorWorker pid=2141642)\u001b[0m   File \"/home/wxdeng/stbr-min/python/ray/dag/compiled_dag_node.py\", line 253, in do_exec_tasks\n",
      "\u001b[36m(TorchTensorWorker pid=2141642)\u001b[0m     done = tasks[operation.exec_task_idx].exec_operation(\n",
      "\u001b[36m(TorchTensorWorker pid=2141642)\u001b[0m         self, operation.type, overlap_gpu_communication\n",
      "\u001b[36m(TorchTensorWorker pid=2141642)\u001b[0m     )\n",
      "\u001b[36m(TorchTensorWorker pid=2141642)\u001b[0m   File \"/home/wxdeng/stbr-min/python/ray/dag/compiled_dag_node.py\", line 786, in exec_operation\n",
      "\u001b[36m(TorchTensorWorker pid=2141642)\u001b[0m     return self._write()\n",
      "\u001b[36m(TorchTensorWorker pid=2141642)\u001b[0m            ~~~~~~~~~~~^^\n",
      "\u001b[36m(TorchTensorWorker pid=2141642)\u001b[0m   File \"/home/wxdeng/stbr-min/python/ray/dag/compiled_dag_node.py\", line 751, in _write\n",
      "\u001b[36m(TorchTensorWorker pid=2141642)\u001b[0m     self.output_writer.write(output_val)\n",
      "\u001b[36m(TorchTensorWorker pid=2141642)\u001b[0m     ~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^\n",
      "\u001b[36m(TorchTensorWorker pid=2141642)\u001b[0m   File \"/home/wxdeng/stbr-min/python/ray/experimental/channel/common.py\", line 617, in write\n",
      "\u001b[36m(TorchTensorWorker pid=2141642)\u001b[0m     channel.write(val_i, timeout)\n",
      "\u001b[36m(TorchTensorWorker pid=2141642)\u001b[0m     ~~~~~~~~~~~~~^^^^^^^^^^^^^^^^\n",
      "\u001b[36m(TorchTensorWorker pid=2141642)\u001b[0m   File \"/home/wxdeng/stbr-min/python/ray/experimental/channel/torch_tensor_nccl_channel.py\", line 243, in write\n",
      "\u001b[36m(TorchTensorWorker pid=2141642)\u001b[0m     raise value\n",
      "\u001b[36m(TorchTensorWorker pid=2141642)\u001b[0m ray.exceptions.RayTaskError: \u001b[36mray::TorchTensorWorker.__ray_call__()\u001b[39m (pid=2141642, ip=128.208.3.124)\n",
      "\u001b[36m(TorchTensorWorker pid=2141642)\u001b[0m   File \"/home/wxdeng/stbr-min/python/ray/dag/tests/experimental/test_torch_tensor_dag.py\", line 67, in send_or_raise\n",
      "\u001b[36m(TorchTensorWorker pid=2141642)\u001b[0m     raise RuntimeError()\n",
      "\u001b[36m(TorchTensorWorker pid=2141642)\u001b[0m RuntimeError\n",
      "\u001b[36m(TorchTensorWorker pid=2141642)\u001b[0m Destructing NCCL group on actor: Actor(TorchTensorWorker, c60c12b6f9f1be08a5c3178f01000000)\n",
      "2025-05-15 15:49:10,871\tINFO compiled_dag_node.py:2200 -- Waiting for worker tasks to exit\n",
      "2025-05-15 15:49:10,872\tINFO compiled_dag_node.py:2203 -- Teardown complete\n",
      "\n",
      "test_torch_tensor_dag.py::test_torch_tensor_exceptions[True-True-True-ray_start_regular0] 2025-05-15 15:49:14,374\tINFO worker.py:1879 -- Started a local Ray instance. View the dashboard at \u001b[1m\u001b[32mhttp://127.0.0.1:8265 \u001b[39m\u001b[22m\n",
      "2025-05-15 15:49:16,542\tINFO torch_tensor_nccl_channel.py:772 -- Creating NCCL group 48be56b7-7add-43b9-ac3f-f32ed0446925 on actors: [Actor(TorchTensorWorker, 00ea92668c4fbde0a93ec05001000000), Actor(TorchTensorWorker, 71f3c0ec4bbc9f7fe10a012401000000)]\n",
      "2025-05-15 15:49:17,631\tINFO torch_tensor_nccl_channel.py:797 -- NCCL group initialized.\n",
      "2025-05-15 15:49:17,815\tINFO dag_node_operation.py:634 -- Writing compiled graph schedule visualization to compiled_graph_schedule.png\n",
      "2025-05-15 15:49:18,059\tINFO compiled_dag_node.py:2173 -- Tearing down compiled DAG\n",
      "\u001b[32mPASSED\u001b[0m2025-05-15 15:49:18,061\tINFO compiled_dag_node.py:2178 -- Cancelling compiled worker on actor: Actor(TorchTensorWorker, 71f3c0ec4bbc9f7fe10a012401000000)\n",
      "2025-05-15 15:49:18,061\tINFO compiled_dag_node.py:2178 -- Cancelling compiled worker on actor: Actor(TorchTensorWorker, 00ea92668c4fbde0a93ec05001000000)\n",
      "\u001b[36m(TorchTensorWorker pid=2142858)\u001b[0m Destructing NCCL group on actor: Actor(TorchTensorWorker, 00ea92668c4fbde0a93ec05001000000)\n",
      "\u001b[36m(TorchTensorWorker pid=2142859)\u001b[0m ERROR:root:Compiled DAG task exited with exception\n",
      "\u001b[36m(TorchTensorWorker pid=2142859)\u001b[0m Traceback (most recent call last):\n",
      "\u001b[36m(TorchTensorWorker pid=2142859)\u001b[0m   File \"/home/wxdeng/stbr-min/python/ray/dag/compiled_dag_node.py\", line 253, in do_exec_tasks\n",
      "\u001b[36m(TorchTensorWorker pid=2142859)\u001b[0m     done = tasks[operation.exec_task_idx].exec_operation(\n",
      "\u001b[36m(TorchTensorWorker pid=2142859)\u001b[0m         self, operation.type, overlap_gpu_communication\n",
      "\u001b[36m(TorchTensorWorker pid=2142859)\u001b[0m     )\n",
      "\u001b[36m(TorchTensorWorker pid=2142859)\u001b[0m   File \"/home/wxdeng/stbr-min/python/ray/dag/compiled_dag_node.py\", line 786, in exec_operation\n",
      "\u001b[36m(TorchTensorWorker pid=2142859)\u001b[0m     return self._write()\n",
      "\u001b[36m(TorchTensorWorker pid=2142859)\u001b[0m            ~~~~~~~~~~~^^\n",
      "\u001b[36m(TorchTensorWorker pid=2142859)\u001b[0m   File \"/home/wxdeng/stbr-min/python/ray/dag/compiled_dag_node.py\", line 751, in _write\n",
      "\u001b[36m(TorchTensorWorker pid=2142859)\u001b[0m     self.output_writer.write(output_val)\n",
      "\u001b[36m(TorchTensorWorker pid=2142859)\u001b[0m     ~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^\n",
      "\u001b[36m(TorchTensorWorker pid=2142859)\u001b[0m   File \"/home/wxdeng/stbr-min/python/ray/experimental/channel/common.py\", line 617, in write\n",
      "\u001b[36m(TorchTensorWorker pid=2142859)\u001b[0m     channel.write(val_i, timeout)\n",
      "\u001b[36m(TorchTensorWorker pid=2142859)\u001b[0m     ~~~~~~~~~~~~~^^^^^^^^^^^^^^^^\n",
      "\u001b[36m(TorchTensorWorker pid=2142859)\u001b[0m   File \"/home/wxdeng/stbr-min/python/ray/experimental/channel/torch_tensor_nccl_channel.py\", line 243, in write\n",
      "\u001b[36m(TorchTensorWorker pid=2142859)\u001b[0m     raise value\n",
      "\u001b[36m(TorchTensorWorker pid=2142859)\u001b[0m ray.exceptions.RayTaskError: \u001b[36mray::TorchTensorWorker.__ray_call__()\u001b[39m (pid=2142859, ip=128.208.3.124)\n",
      "\u001b[36m(TorchTensorWorker pid=2142859)\u001b[0m   File \"/home/wxdeng/stbr-min/python/ray/dag/tests/experimental/test_torch_tensor_dag.py\", line 67, in send_or_raise\n",
      "\u001b[36m(TorchTensorWorker pid=2142859)\u001b[0m     raise RuntimeError()\n",
      "\u001b[36m(TorchTensorWorker pid=2142859)\u001b[0m RuntimeError\n",
      "\u001b[36m(TorchTensorWorker pid=2142859)\u001b[0m Destructing NCCL group on actor: Actor(TorchTensorWorker, 71f3c0ec4bbc9f7fe10a012401000000)\n",
      "2025-05-15 15:49:18,547\tINFO compiled_dag_node.py:2200 -- Waiting for worker tasks to exit\n",
      "2025-05-15 15:49:18,548\tINFO compiled_dag_node.py:2203 -- Teardown complete\n",
      "\n",
      "test_torch_tensor_dag.py::test_torch_tensor_exceptions2[ray_start_regular0] 2025-05-15 15:49:22,185\tINFO worker.py:1879 -- Started a local Ray instance. View the dashboard at \u001b[1m\u001b[32mhttp://127.0.0.1:8265 \u001b[39m\u001b[22m\n",
      "2025-05-15 15:49:24,412\tINFO torch_tensor_nccl_channel.py:772 -- Creating NCCL group e8662dda-0bc0-467a-abde-8dade577ce46 on actors: [Actor(TorchTensorWorker, 2240a630da5d83f4614dc9ca01000000), Actor(TorchTensorWorker, fd6b17d7d58d9acb4ef51f7001000000)]\n",
      "2025-05-15 15:49:25,483\tINFO torch_tensor_nccl_channel.py:797 -- NCCL group initialized.\n",
      "2025-05-15 15:49:25,572\tINFO dag_node_operation.py:634 -- Writing compiled graph schedule visualization to compiled_graph_schedule.png\n",
      "2025-05-15 15:49:25,746\tINFO compiled_dag_node.py:2173 -- Tearing down compiled DAG\n",
      "\u001b[32mPASSED\u001b[0m2025-05-15 15:49:25,748\tINFO compiled_dag_node.py:2178 -- Cancelling compiled worker on actor: Actor(TorchTensorWorker, 2240a630da5d83f4614dc9ca01000000)\n",
      "2025-05-15 15:49:25,748\tINFO compiled_dag_node.py:2178 -- Cancelling compiled worker on actor: Actor(TorchTensorWorker, fd6b17d7d58d9acb4ef51f7001000000)\n",
      "\u001b[36m(TorchTensorWorker pid=2144124)\u001b[0m Destructing NCCL group on actor: Actor(TorchTensorWorker, fd6b17d7d58d9acb4ef51f7001000000)\n",
      "\u001b[36m(TorchTensorWorker pid=2144123)\u001b[0m ERROR:root:Compiled DAG task exited with exception\n",
      "\u001b[36m(TorchTensorWorker pid=2144123)\u001b[0m Traceback (most recent call last):\n",
      "\u001b[36m(TorchTensorWorker pid=2144123)\u001b[0m   File \"/home/wxdeng/stbr-min/python/ray/dag/compiled_dag_node.py\", line 253, in do_exec_tasks\n",
      "\u001b[36m(TorchTensorWorker pid=2144123)\u001b[0m     done = tasks[operation.exec_task_idx].exec_operation(\n",
      "\u001b[36m(TorchTensorWorker pid=2144123)\u001b[0m         self, operation.type, overlap_gpu_communication\n",
      "\u001b[36m(TorchTensorWorker pid=2144123)\u001b[0m     )\n",
      "\u001b[36m(TorchTensorWorker pid=2144123)\u001b[0m   File \"/home/wxdeng/stbr-min/python/ray/dag/compiled_dag_node.py\", line 786, in exec_operation\n",
      "\u001b[36m(TorchTensorWorker pid=2144123)\u001b[0m     return self._write()\n",
      "\u001b[36m(TorchTensorWorker pid=2144123)\u001b[0m            ~~~~~~~~~~~^^\n",
      "\u001b[36m(TorchTensorWorker pid=2144123)\u001b[0m   File \"/home/wxdeng/stbr-min/python/ray/dag/compiled_dag_node.py\", line 751, in _write\n",
      "\u001b[36m(TorchTensorWorker pid=2144123)\u001b[0m     self.output_writer.write(output_val)\n",
      "\u001b[36m(TorchTensorWorker pid=2144123)\u001b[0m     ~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^\n",
      "\u001b[36m(TorchTensorWorker pid=2144123)\u001b[0m   File \"/home/wxdeng/stbr-min/python/ray/experimental/channel/common.py\", line 617, in write\n",
      "\u001b[36m(TorchTensorWorker pid=2144123)\u001b[0m     channel.write(val_i, timeout)\n",
      "\u001b[36m(TorchTensorWorker pid=2144123)\u001b[0m     ~~~~~~~~~~~~~^^^^^^^^^^^^^^^^\n",
      "\u001b[36m(TorchTensorWorker pid=2144123)\u001b[0m   File \"/home/wxdeng/stbr-min/python/ray/experimental/channel/torch_tensor_nccl_channel.py\", line 257, in write\n",
      "\u001b[36m(TorchTensorWorker pid=2144123)\u001b[0m     raise ValueError(\n",
      "\u001b[36m(TorchTensorWorker pid=2144123)\u001b[0m     ...<3 lines>...\n",
      "\u001b[36m(TorchTensorWorker pid=2144123)\u001b[0m     )\n",
      "\u001b[36m(TorchTensorWorker pid=2144123)\u001b[0m ValueError: Task annotated with _direct_return=True must return a CUDA torch.Tensor, instead found value `1`. DAG will shut down.\n",
      "\u001b[36m(TorchTensorWorker pid=2144123)\u001b[0m Destructing NCCL group on actor: Actor(TorchTensorWorker, 2240a630da5d83f4614dc9ca01000000)\n",
      "2025-05-15 15:49:26,489\tINFO compiled_dag_node.py:2200 -- Waiting for worker tasks to exit\n",
      "2025-05-15 15:49:26,490\tINFO compiled_dag_node.py:2203 -- Teardown complete\n",
      "\n",
      "test_torch_tensor_dag.py::test_torch_tensor_explicit_communicator[ray_start_regular0] 2025-05-15 15:49:30,073\tINFO worker.py:1879 -- Started a local Ray instance. View the dashboard at \u001b[1m\u001b[32mhttp://127.0.0.1:8265 \u001b[39m\u001b[22m\n",
      "\u001b[32mPASSED\u001b[0m\n",
      "test_torch_tensor_dag.py::test_torch_tensor_nccl_collective_ops[operation0-None-ray_start_regular0] 2025-05-15 15:49:35,165\tINFO worker.py:1879 -- Started a local Ray instance. View the dashboard at \u001b[1m\u001b[32mhttp://127.0.0.1:8265 \u001b[39m\u001b[22m\n",
      "2025-05-15 15:49:37,364\tINFO torch_tensor_nccl_channel.py:772 -- Creating NCCL group 0624b2fe-0a02-4d2d-9a3d-bb8c82595bc8 on actors: [Actor(TorchTensorWorker, b1bf2bedf45a7d522e7d81e801000000), Actor(TorchTensorWorker, bb84868c8e92d1318a245a6201000000)]\n",
      "2025-05-15 15:49:38,431\tINFO torch_tensor_nccl_channel.py:797 -- NCCL group initialized.\n",
      "2025-05-15 15:49:38,607\tINFO dag_node_operation.py:634 -- Writing compiled graph schedule visualization to compiled_graph_schedule.png\n",
      "\u001b[32mPASSED\u001b[0m2025-05-15 15:49:39,172\tINFO compiled_dag_node.py:2173 -- Tearing down compiled DAG\n",
      "2025-05-15 15:49:39,173\tINFO compiled_dag_node.py:2178 -- Cancelling compiled worker on actor: Actor(TorchTensorWorker, b1bf2bedf45a7d522e7d81e801000000)\n",
      "2025-05-15 15:49:39,173\tINFO compiled_dag_node.py:2178 -- Cancelling compiled worker on actor: Actor(TorchTensorWorker, bb84868c8e92d1318a245a6201000000)\n",
      "\u001b[36m(TorchTensorWorker pid=2146570)\u001b[0m Destructing NCCL group on actor: Actor(TorchTensorWorker, b1bf2bedf45a7d522e7d81e801000000)\n",
      "\u001b[36m(TorchTensorWorker pid=2146571)\u001b[0m Destructing NCCL group on actor: Actor(TorchTensorWorker, bb84868c8e92d1318a245a6201000000)\n",
      "2025-05-15 15:49:39,392\tINFO compiled_dag_node.py:2200 -- Waiting for worker tasks to exit\n",
      "2025-05-15 15:49:39,392\tINFO compiled_dag_node.py:2161 -- Killing actor: Actor(TorchTensorWorker, b1bf2bedf45a7d522e7d81e801000000)\n",
      "2025-05-15 15:49:39,393\tINFO compiled_dag_node.py:2161 -- Killing actor: Actor(TorchTensorWorker, bb84868c8e92d1318a245a6201000000)\n",
      "2025-05-15 15:49:39,393\tINFO compiled_dag_node.py:2203 -- Teardown complete\n",
      "\n",
      "test_torch_tensor_dag.py::test_torch_tensor_nccl_collective_ops[operation1-ReduceOp.SUM-ray_start_regular0] 2025-05-15 15:49:42,872\tINFO worker.py:1879 -- Started a local Ray instance. View the dashboard at \u001b[1m\u001b[32mhttp://127.0.0.1:8265 \u001b[39m\u001b[22m\n",
      "2025-05-15 15:49:45,097\tINFO torch_tensor_nccl_channel.py:772 -- Creating NCCL group 74543366-3da4-4109-b2d4-8e8645c13c30 on actors: [Actor(TorchTensorWorker, aac8d57c7b95fca9141ad20901000000), Actor(TorchTensorWorker, eb70de1e55901f30c77eb1e001000000)]\n",
      "2025-05-15 15:49:45,959\tINFO torch_tensor_nccl_channel.py:797 -- NCCL group initialized.\n",
      "2025-05-15 15:49:46,128\tINFO dag_node_operation.py:634 -- Writing compiled graph schedule visualization to compiled_graph_schedule.png\n",
      "\u001b[32mPASSED\u001b[0m2025-05-15 15:49:46,429\tINFO compiled_dag_node.py:2173 -- Tearing down compiled DAG\n",
      "2025-05-15 15:49:46,430\tINFO compiled_dag_node.py:2178 -- Cancelling compiled worker on actor: Actor(TorchTensorWorker, aac8d57c7b95fca9141ad20901000000)\n",
      "2025-05-15 15:49:46,430\tINFO compiled_dag_node.py:2178 -- Cancelling compiled worker on actor: Actor(TorchTensorWorker, eb70de1e55901f30c77eb1e001000000)\n",
      "\u001b[36m(TorchTensorWorker pid=2147816)\u001b[0m Destructing NCCL group on actor: Actor(TorchTensorWorker, eb70de1e55901f30c77eb1e001000000)\n",
      "\u001b[36m(TorchTensorWorker pid=2147814)\u001b[0m Destructing NCCL group on actor: Actor(TorchTensorWorker, aac8d57c7b95fca9141ad20901000000)\n",
      "2025-05-15 15:49:46,921\tINFO compiled_dag_node.py:2200 -- Waiting for worker tasks to exit\n",
      "2025-05-15 15:49:46,921\tINFO compiled_dag_node.py:2161 -- Killing actor: Actor(TorchTensorWorker, aac8d57c7b95fca9141ad20901000000)\n",
      "2025-05-15 15:49:46,921\tINFO compiled_dag_node.py:2161 -- Killing actor: Actor(TorchTensorWorker, eb70de1e55901f30c77eb1e001000000)\n",
      "2025-05-15 15:49:46,922\tINFO compiled_dag_node.py:2203 -- Teardown complete\n",
      "\n",
      "test_torch_tensor_dag.py::test_torch_tensor_nccl_collective_ops[operation2-ReduceOp.PRODUCT-ray_start_regular0] 2025-05-15 15:49:51,300\tINFO worker.py:1879 -- Started a local Ray instance. View the dashboard at \u001b[1m\u001b[32mhttp://127.0.0.1:8265 \u001b[39m\u001b[22m\n",
      "2025-05-15 15:49:53,514\tINFO torch_tensor_nccl_channel.py:772 -- Creating NCCL group 78bd6790-2c44-409d-b6ac-dc02f1a20b27 on actors: [Actor(TorchTensorWorker, 3602b0b9c35bf6fe5de2dd7001000000), Actor(TorchTensorWorker, 2ba99cb76d7530627337eddc01000000)]\n",
      "2025-05-15 15:49:54,361\tINFO torch_tensor_nccl_channel.py:797 -- NCCL group initialized.\n",
      "2025-05-15 15:49:54,521\tINFO dag_node_operation.py:634 -- Writing compiled graph schedule visualization to compiled_graph_schedule.png\n",
      "\u001b[32mPASSED\u001b[0m2025-05-15 15:49:54,827\tINFO compiled_dag_node.py:2173 -- Tearing down compiled DAG\n",
      "2025-05-15 15:49:54,828\tINFO compiled_dag_node.py:2178 -- Cancelling compiled worker on actor: Actor(TorchTensorWorker, 3602b0b9c35bf6fe5de2dd7001000000)\n",
      "2025-05-15 15:49:54,828\tINFO compiled_dag_node.py:2178 -- Cancelling compiled worker on actor: Actor(TorchTensorWorker, 2ba99cb76d7530627337eddc01000000)\n",
      "\u001b[36m(TorchTensorWorker pid=2149116)\u001b[0m Destructing NCCL group on actor: Actor(TorchTensorWorker, 3602b0b9c35bf6fe5de2dd7001000000)\n",
      "\u001b[36m(TorchTensorWorker pid=2149117)\u001b[0m Destructing NCCL group on actor: Actor(TorchTensorWorker, 2ba99cb76d7530627337eddc01000000)\n",
      "2025-05-15 15:49:55,348\tINFO compiled_dag_node.py:2200 -- Waiting for worker tasks to exit\n",
      "2025-05-15 15:49:55,348\tINFO compiled_dag_node.py:2161 -- Killing actor: Actor(TorchTensorWorker, 3602b0b9c35bf6fe5de2dd7001000000)\n",
      "2025-05-15 15:49:55,348\tINFO compiled_dag_node.py:2161 -- Killing actor: Actor(TorchTensorWorker, 2ba99cb76d7530627337eddc01000000)\n",
      "2025-05-15 15:49:55,349\tINFO compiled_dag_node.py:2203 -- Teardown complete\n",
      "\n",
      "test_torch_tensor_dag.py::test_torch_tensor_nccl_collective_ops[operation3-ReduceOp.MIN-ray_start_regular0] 2025-05-15 15:49:58,826\tINFO worker.py:1879 -- Started a local Ray instance. View the dashboard at \u001b[1m\u001b[32mhttp://127.0.0.1:8265 \u001b[39m\u001b[22m\n",
      "2025-05-15 15:50:01,132\tINFO torch_tensor_nccl_channel.py:772 -- Creating NCCL group 68cc4872-846e-4e44-8104-35d0b698058e on actors: [Actor(TorchTensorWorker, 502a9374d9ad1f9426573db601000000), Actor(TorchTensorWorker, 9580ac3b4aabdbf3b6762a6001000000)]\n",
      "2025-05-15 15:50:02,027\tINFO torch_tensor_nccl_channel.py:797 -- NCCL group initialized.\n",
      "2025-05-15 15:50:02,191\tINFO dag_node_operation.py:634 -- Writing compiled graph schedule visualization to compiled_graph_schedule.png\n",
      "\u001b[32mPASSED\u001b[0m2025-05-15 15:50:02,491\tINFO compiled_dag_node.py:2173 -- Tearing down compiled DAG\n",
      "2025-05-15 15:50:02,492\tINFO compiled_dag_node.py:2178 -- Cancelling compiled worker on actor: Actor(TorchTensorWorker, 502a9374d9ad1f9426573db601000000)\n",
      "2025-05-15 15:50:02,492\tINFO compiled_dag_node.py:2178 -- Cancelling compiled worker on actor: Actor(TorchTensorWorker, 9580ac3b4aabdbf3b6762a6001000000)\n",
      "\u001b[36m(TorchTensorWorker pid=2150330)\u001b[0m Destructing NCCL group on actor: Actor(TorchTensorWorker, 9580ac3b4aabdbf3b6762a6001000000)\n",
      "\u001b[36m(TorchTensorWorker pid=2150328)\u001b[0m Destructing NCCL group on actor: Actor(TorchTensorWorker, 502a9374d9ad1f9426573db601000000)\n",
      "2025-05-15 15:50:02,981\tINFO compiled_dag_node.py:2200 -- Waiting for worker tasks to exit\n",
      "2025-05-15 15:50:02,982\tINFO compiled_dag_node.py:2161 -- Killing actor: Actor(TorchTensorWorker, 502a9374d9ad1f9426573db601000000)\n",
      "2025-05-15 15:50:02,982\tINFO compiled_dag_node.py:2161 -- Killing actor: Actor(TorchTensorWorker, 9580ac3b4aabdbf3b6762a6001000000)\n",
      "2025-05-15 15:50:02,983\tINFO compiled_dag_node.py:2203 -- Teardown complete\n",
      "\n",
      "test_torch_tensor_dag.py::test_torch_tensor_nccl_collective_ops[operation4-ReduceOp.MAX-ray_start_regular0] 2025-05-15 15:50:07,223\tINFO worker.py:1879 -- Started a local Ray instance. View the dashboard at \u001b[1m\u001b[32mhttp://127.0.0.1:8265 \u001b[39m\u001b[22m\n",
      "2025-05-15 15:50:09,442\tINFO torch_tensor_nccl_channel.py:772 -- Creating NCCL group 2a59ef74-e69b-4f2f-b9ba-d21eed2b0866 on actors: [Actor(TorchTensorWorker, ddf04402420843c7ce909a6b01000000), Actor(TorchTensorWorker, 6be0f68b35dfd0a18d6e8c6d01000000)]\n",
      "2025-05-15 15:50:10,294\tINFO torch_tensor_nccl_channel.py:797 -- NCCL group initialized.\n",
      "2025-05-15 15:50:10,470\tINFO dag_node_operation.py:634 -- Writing compiled graph schedule visualization to compiled_graph_schedule.png\n",
      "\u001b[32mPASSED\u001b[0m2025-05-15 15:50:10,777\tINFO compiled_dag_node.py:2173 -- Tearing down compiled DAG\n",
      "2025-05-15 15:50:10,777\tINFO compiled_dag_node.py:2178 -- Cancelling compiled worker on actor: Actor(TorchTensorWorker, ddf04402420843c7ce909a6b01000000)\n",
      "2025-05-15 15:50:10,777\tINFO compiled_dag_node.py:2178 -- Cancelling compiled worker on actor: Actor(TorchTensorWorker, 6be0f68b35dfd0a18d6e8c6d01000000)\n",
      "\u001b[36m(TorchTensorWorker pid=2151618)\u001b[0m Destructing NCCL group on actor: Actor(TorchTensorWorker, 6be0f68b35dfd0a18d6e8c6d01000000)\n",
      "\u001b[36m(TorchTensorWorker pid=2151616)\u001b[0m Destructing NCCL group on actor: Actor(TorchTensorWorker, ddf04402420843c7ce909a6b01000000)\n",
      "2025-05-15 15:50:11,263\tINFO compiled_dag_node.py:2200 -- Waiting for worker tasks to exit\n",
      "2025-05-15 15:50:11,264\tINFO compiled_dag_node.py:2161 -- Killing actor: Actor(TorchTensorWorker, ddf04402420843c7ce909a6b01000000)\n",
      "2025-05-15 15:50:11,264\tINFO compiled_dag_node.py:2161 -- Killing actor: Actor(TorchTensorWorker, 6be0f68b35dfd0a18d6e8c6d01000000)\n",
      "2025-05-15 15:50:11,264\tINFO compiled_dag_node.py:2203 -- Teardown complete\n",
      "\n",
      "test_torch_tensor_dag.py::test_torch_tensor_nccl_collective_ops[operation5-ReduceOp.SUM-ray_start_regular0] 2025-05-15 15:50:14,832\tINFO worker.py:1879 -- Started a local Ray instance. View the dashboard at \u001b[1m\u001b[32mhttp://127.0.0.1:8265 \u001b[39m\u001b[22m\n",
      "2025-05-15 15:50:17,076\tINFO torch_tensor_nccl_channel.py:772 -- Creating NCCL group 2b6065ad-ec18-4529-a4b6-17aa89bd6ccf on actors: [Actor(TorchTensorWorker, b71f1212f9334b9b26afdecd01000000), Actor(TorchTensorWorker, 74f905c698ac1c35030832a201000000)]\n",
      "2025-05-15 15:50:17,936\tINFO torch_tensor_nccl_channel.py:797 -- NCCL group initialized.\n",
      "2025-05-15 15:50:18,101\tINFO dag_node_operation.py:634 -- Writing compiled graph schedule visualization to compiled_graph_schedule.png\n",
      "\u001b[32mPASSED\u001b[0m2025-05-15 15:50:18,414\tINFO compiled_dag_node.py:2173 -- Tearing down compiled DAG\n",
      "2025-05-15 15:50:18,415\tINFO compiled_dag_node.py:2178 -- Cancelling compiled worker on actor: Actor(TorchTensorWorker, b71f1212f9334b9b26afdecd01000000)\n",
      "2025-05-15 15:50:18,415\tINFO compiled_dag_node.py:2178 -- Cancelling compiled worker on actor: Actor(TorchTensorWorker, 74f905c698ac1c35030832a201000000)\n",
      "\u001b[36m(TorchTensorWorker pid=2152825)\u001b[0m Destructing NCCL group on actor: Actor(TorchTensorWorker, b71f1212f9334b9b26afdecd01000000)\n",
      "\u001b[36m(TorchTensorWorker pid=2152826)\u001b[0m Destructing NCCL group on actor: Actor(TorchTensorWorker, 74f905c698ac1c35030832a201000000)\n",
      "2025-05-15 15:50:18,904\tINFO compiled_dag_node.py:2200 -- Waiting for worker tasks to exit\n",
      "2025-05-15 15:50:18,905\tINFO compiled_dag_node.py:2161 -- Killing actor: Actor(TorchTensorWorker, b71f1212f9334b9b26afdecd01000000)\n",
      "2025-05-15 15:50:18,905\tINFO compiled_dag_node.py:2161 -- Killing actor: Actor(TorchTensorWorker, 74f905c698ac1c35030832a201000000)\n",
      "2025-05-15 15:50:18,905\tINFO compiled_dag_node.py:2203 -- Teardown complete\n",
      "\n",
      "test_torch_tensor_dag.py::test_torch_tensor_nccl_collective_ops[operation6-ReduceOp.PRODUCT-ray_start_regular0] 2025-05-15 15:50:22,440\tINFO worker.py:1879 -- Started a local Ray instance. View the dashboard at \u001b[1m\u001b[32mhttp://127.0.0.1:8265 \u001b[39m\u001b[22m\n",
      "2025-05-15 15:50:24,669\tINFO torch_tensor_nccl_channel.py:772 -- Creating NCCL group a1a02975-e93b-4e05-bd08-59bf0959d51d on actors: [Actor(TorchTensorWorker, fb1c30100a196c9e107ab91701000000), Actor(TorchTensorWorker, e32b9aac063012642b362d6101000000)]\n",
      "2025-05-15 15:50:25,511\tINFO torch_tensor_nccl_channel.py:797 -- NCCL group initialized.\n",
      "2025-05-15 15:50:25,687\tINFO dag_node_operation.py:634 -- Writing compiled graph schedule visualization to compiled_graph_schedule.png\n",
      "\u001b[32mPASSED\u001b[0m2025-05-15 15:50:26,006\tINFO compiled_dag_node.py:2173 -- Tearing down compiled DAG\n",
      "2025-05-15 15:50:26,007\tINFO compiled_dag_node.py:2178 -- Cancelling compiled worker on actor: Actor(TorchTensorWorker, fb1c30100a196c9e107ab91701000000)\n",
      "2025-05-15 15:50:26,007\tINFO compiled_dag_node.py:2178 -- Cancelling compiled worker on actor: Actor(TorchTensorWorker, e32b9aac063012642b362d6101000000)\n",
      "\u001b[36m(TorchTensorWorker pid=2154092)\u001b[0m Destructing NCCL group on actor: Actor(TorchTensorWorker, fb1c30100a196c9e107ab91701000000)\n",
      "\u001b[36m(TorchTensorWorker pid=2154091)\u001b[0m Destructing NCCL group on actor: Actor(TorchTensorWorker, e32b9aac063012642b362d6101000000)\n",
      "2025-05-15 15:50:26,496\tINFO compiled_dag_node.py:2200 -- Waiting for worker tasks to exit\n",
      "2025-05-15 15:50:26,497\tINFO compiled_dag_node.py:2161 -- Killing actor: Actor(TorchTensorWorker, fb1c30100a196c9e107ab91701000000)\n",
      "2025-05-15 15:50:26,497\tINFO compiled_dag_node.py:2161 -- Killing actor: Actor(TorchTensorWorker, e32b9aac063012642b362d6101000000)\n",
      "2025-05-15 15:50:26,497\tINFO compiled_dag_node.py:2203 -- Teardown complete\n",
      "\n",
      "test_torch_tensor_dag.py::test_torch_tensor_nccl_collective_ops[operation7-ReduceOp.MIN-ray_start_regular0] 2025-05-15 15:50:30,761\tINFO worker.py:1879 -- Started a local Ray instance. View the dashboard at \u001b[1m\u001b[32mhttp://127.0.0.1:8265 \u001b[39m\u001b[22m\n",
      "2025-05-15 15:50:33,004\tINFO torch_tensor_nccl_channel.py:772 -- Creating NCCL group d38d09b2-daae-402d-a538-5c22ad826c39 on actors: [Actor(TorchTensorWorker, 189ec2dac21d9c3f82fe307901000000), Actor(TorchTensorWorker, 1d539fd68b0a4cb7d83e5b0801000000)]\n",
      "2025-05-15 15:50:33,845\tINFO torch_tensor_nccl_channel.py:797 -- NCCL group initialized.\n",
      "2025-05-15 15:50:34,002\tINFO dag_node_operation.py:634 -- Writing compiled graph schedule visualization to compiled_graph_schedule.png\n",
      "\u001b[32mPASSED\u001b[0m2025-05-15 15:50:34,316\tINFO compiled_dag_node.py:2173 -- Tearing down compiled DAG\n",
      "2025-05-15 15:50:34,317\tINFO compiled_dag_node.py:2178 -- Cancelling compiled worker on actor: Actor(TorchTensorWorker, 189ec2dac21d9c3f82fe307901000000)\n",
      "2025-05-15 15:50:34,317\tINFO compiled_dag_node.py:2178 -- Cancelling compiled worker on actor: Actor(TorchTensorWorker, 1d539fd68b0a4cb7d83e5b0801000000)\n",
      "\u001b[36m(TorchTensorWorker pid=2155363)\u001b[0m Destructing NCCL group on actor: Actor(TorchTensorWorker, 1d539fd68b0a4cb7d83e5b0801000000)\n",
      "\u001b[36m(TorchTensorWorker pid=2155364)\u001b[0m Destructing NCCL group on actor: Actor(TorchTensorWorker, 189ec2dac21d9c3f82fe307901000000)\n",
      "2025-05-15 15:50:34,806\tINFO compiled_dag_node.py:2200 -- Waiting for worker tasks to exit\n",
      "2025-05-15 15:50:34,806\tINFO compiled_dag_node.py:2161 -- Killing actor: Actor(TorchTensorWorker, 189ec2dac21d9c3f82fe307901000000)\n",
      "2025-05-15 15:50:34,807\tINFO compiled_dag_node.py:2161 -- Killing actor: Actor(TorchTensorWorker, 1d539fd68b0a4cb7d83e5b0801000000)\n",
      "2025-05-15 15:50:34,807\tINFO compiled_dag_node.py:2203 -- Teardown complete\n",
      "\n",
      "test_torch_tensor_dag.py::test_torch_tensor_nccl_collective_ops[operation8-ReduceOp.MAX-ray_start_regular0] 2025-05-15 15:50:38,169\tINFO worker.py:1879 -- Started a local Ray instance. View the dashboard at \u001b[1m\u001b[32mhttp://127.0.0.1:8265 \u001b[39m\u001b[22m\n",
      "2025-05-15 15:50:40,445\tINFO torch_tensor_nccl_channel.py:772 -- Creating NCCL group a79cb827-33f9-4d66-a287-95c34ad151f6 on actors: [Actor(TorchTensorWorker, 026a2e012a3afcc9d602023901000000), Actor(TorchTensorWorker, 760daeb509e091fba5ea87b101000000)]\n",
      "2025-05-15 15:50:41,298\tINFO torch_tensor_nccl_channel.py:797 -- NCCL group initialized.\n",
      "2025-05-15 15:50:41,453\tINFO dag_node_operation.py:634 -- Writing compiled graph schedule visualization to compiled_graph_schedule.png\n",
      "\u001b[32mPASSED\u001b[0m2025-05-15 15:50:41,762\tINFO compiled_dag_node.py:2173 -- Tearing down compiled DAG\n",
      "2025-05-15 15:50:41,762\tINFO compiled_dag_node.py:2178 -- Cancelling compiled worker on actor: Actor(TorchTensorWorker, 026a2e012a3afcc9d602023901000000)\n",
      "2025-05-15 15:50:41,762\tINFO compiled_dag_node.py:2178 -- Cancelling compiled worker on actor: Actor(TorchTensorWorker, 760daeb509e091fba5ea87b101000000)\n",
      "\u001b[36m(TorchTensorWorker pid=2156566)\u001b[0m Destructing NCCL group on actor: Actor(TorchTensorWorker, 026a2e012a3afcc9d602023901000000)\n",
      "\u001b[36m(TorchTensorWorker pid=2156568)\u001b[0m Destructing NCCL group on actor: Actor(TorchTensorWorker, 760daeb509e091fba5ea87b101000000)\n",
      "2025-05-15 15:50:42,250\tINFO compiled_dag_node.py:2200 -- Waiting for worker tasks to exit\n",
      "2025-05-15 15:50:42,250\tINFO compiled_dag_node.py:2161 -- Killing actor: Actor(TorchTensorWorker, 026a2e012a3afcc9d602023901000000)\n",
      "2025-05-15 15:50:42,251\tINFO compiled_dag_node.py:2161 -- Killing actor: Actor(TorchTensorWorker, 760daeb509e091fba5ea87b101000000)\n",
      "2025-05-15 15:50:42,251\tINFO compiled_dag_node.py:2203 -- Teardown complete\n",
      "\n",
      "test_torch_tensor_dag.py::test_torch_tensor_nccl_all_reduce_get_partial[ray_start_regular0] 2025-05-15 15:50:45,875\tINFO worker.py:1879 -- Started a local Ray instance. View the dashboard at \u001b[1m\u001b[32mhttp://127.0.0.1:8265 \u001b[39m\u001b[22m\n",
      "2025-05-15 15:50:48,103\tINFO torch_tensor_nccl_channel.py:772 -- Creating NCCL group dfc7fbb5-5922-454e-a80d-feeb2b61c923 on actors: [Actor(TorchTensorWorker, 8a9856a33b5408261809410501000000), Actor(TorchTensorWorker, 84f89c86b6f51f0d24eb3cba01000000)]\n",
      "2025-05-15 15:50:48,967\tINFO torch_tensor_nccl_channel.py:797 -- NCCL group initialized.\n",
      "2025-05-15 15:50:49,145\tINFO dag_node_operation.py:634 -- Writing compiled graph schedule visualization to compiled_graph_schedule.png\n",
      "\u001b[32mPASSED\u001b[0m2025-05-15 15:50:49,391\tINFO compiled_dag_node.py:2173 -- Tearing down compiled DAG\n",
      "2025-05-15 15:50:49,391\tINFO compiled_dag_node.py:2178 -- Cancelling compiled worker on actor: Actor(TorchTensorWorker, 8a9856a33b5408261809410501000000)\n",
      "2025-05-15 15:50:49,391\tINFO compiled_dag_node.py:2178 -- Cancelling compiled worker on actor: Actor(TorchTensorWorker, 84f89c86b6f51f0d24eb3cba01000000)\n",
      "\u001b[36m(TorchTensorWorker pid=2157809)\u001b[0m Destructing NCCL group on actor: Actor(TorchTensorWorker, 84f89c86b6f51f0d24eb3cba01000000)\n",
      "\u001b[36m(TorchTensorWorker pid=2157808)\u001b[0m Destructing NCCL group on actor: Actor(TorchTensorWorker, 8a9856a33b5408261809410501000000)\n",
      "2025-05-15 15:50:49,912\tINFO compiled_dag_node.py:2200 -- Waiting for worker tasks to exit\n",
      "2025-05-15 15:50:49,913\tINFO compiled_dag_node.py:2161 -- Killing actor: Actor(TorchTensorWorker, 8a9856a33b5408261809410501000000)\n",
      "2025-05-15 15:50:49,913\tINFO compiled_dag_node.py:2161 -- Killing actor: Actor(TorchTensorWorker, 84f89c86b6f51f0d24eb3cba01000000)\n",
      "2025-05-15 15:50:49,913\tINFO compiled_dag_node.py:2203 -- Teardown complete\n",
      "\n",
      "test_torch_tensor_dag.py::test_torch_tensor_nccl_all_reduce_wrong_shape[ray_start_regular0] 2025-05-15 15:50:53,290\tINFO worker.py:1879 -- Started a local Ray instance. View the dashboard at \u001b[1m\u001b[32mhttp://127.0.0.1:8265 \u001b[39m\u001b[22m\n",
      "2025-05-15 15:50:55,529\tINFO torch_tensor_nccl_channel.py:772 -- Creating NCCL group a86c3236-6d49-488a-a0f6-fc4c94f10b64 on actors: [Actor(TorchTensorWorker, b9662dfb03940283dbd93e7501000000), Actor(TorchTensorWorker, d87fb23002566f5f6fb4c9aa01000000)]\n",
      "2025-05-15 15:50:56,384\tINFO torch_tensor_nccl_channel.py:797 -- NCCL group initialized.\n",
      "2025-05-15 15:50:56,515\tINFO dag_node_operation.py:634 -- Writing compiled graph schedule visualization to compiled_graph_schedule.png\n",
      "\u001b[32mPASSED\u001b[0m2025-05-15 15:51:16,831\tINFO compiled_dag_node.py:2173 -- Tearing down compiled DAG\n",
      "2025-05-15 15:51:16,832\tINFO compiled_dag_node.py:2178 -- Cancelling compiled worker on actor: Actor(TorchTensorWorker, b9662dfb03940283dbd93e7501000000)\n",
      "2025-05-15 15:51:16,832\tINFO compiled_dag_node.py:2178 -- Cancelling compiled worker on actor: Actor(TorchTensorWorker, d87fb23002566f5f6fb4c9aa01000000)\n",
      "2025-05-15 15:51:16,923\tINFO compiled_dag_node.py:2200 -- Waiting for worker tasks to exit\n",
      "2025-05-15 15:51:16,924\tINFO compiled_dag_node.py:2161 -- Killing actor: Actor(TorchTensorWorker, b9662dfb03940283dbd93e7501000000)\n",
      "2025-05-15 15:51:16,924\tINFO compiled_dag_node.py:2161 -- Killing actor: Actor(TorchTensorWorker, d87fb23002566f5f6fb4c9aa01000000)\n",
      "2025-05-15 15:51:16,924\tINFO compiled_dag_node.py:2203 -- Teardown complete\n",
      "\n",
      "test_torch_tensor_dag.py::test_torch_tensor_nccl_all_reduce_custom_comm[ray_start_regular0] 2025-05-15 15:51:21,320\tINFO worker.py:1879 -- Started a local Ray instance. View the dashboard at \u001b[1m\u001b[32mhttp://127.0.0.1:8265 \u001b[39m\u001b[22m\n",
      "2025-05-15 15:51:23,557\tINFO torch_tensor_nccl_channel.py:770 -- Initializing custom NCCL group 6f7bb8a1-01b0-4e85-a0f5-10257bcb3414 on actors: [Actor(TorchTensorWorker, 6e1a0a6b47f9eb1bc52dcc4201000000), Actor(TorchTensorWorker, 8c0e7c4416df21ce7f2b800601000000)]\n",
      "2025-05-15 15:51:24,324\tINFO torch_tensor_nccl_channel.py:797 -- NCCL group initialized.\n",
      "2025-05-15 15:51:24,455\tINFO dag_node_operation.py:634 -- Writing compiled graph schedule visualization to compiled_graph_schedule.png\n",
      "\u001b[32mPASSED\u001b[0m2025-05-15 15:51:24,802\tINFO compiled_dag_node.py:2173 -- Tearing down compiled DAG\n",
      "2025-05-15 15:51:24,803\tINFO compiled_dag_node.py:2178 -- Cancelling compiled worker on actor: Actor(TorchTensorWorker, 6e1a0a6b47f9eb1bc52dcc4201000000)\n",
      "2025-05-15 15:51:24,803\tINFO compiled_dag_node.py:2178 -- Cancelling compiled worker on actor: Actor(TorchTensorWorker, 8c0e7c4416df21ce7f2b800601000000)\n",
      "2025-05-15 15:51:24,813\tINFO compiled_dag_node.py:2200 -- Waiting for worker tasks to exit\n",
      "2025-05-15 15:51:24,814\tINFO compiled_dag_node.py:2161 -- Killing actor: Actor(TorchTensorWorker, 6e1a0a6b47f9eb1bc52dcc4201000000)\n",
      "2025-05-15 15:51:24,814\tINFO compiled_dag_node.py:2161 -- Killing actor: Actor(TorchTensorWorker, 8c0e7c4416df21ce7f2b800601000000)\n",
      "2025-05-15 15:51:24,814\tINFO compiled_dag_node.py:2203 -- Teardown complete\n",
      "\n",
      "test_torch_tensor_dag.py::test_torch_tensor_nccl_all_reduce_scheduling[ray_start_regular0] 2025-05-15 15:51:28,353\tINFO worker.py:1879 -- Started a local Ray instance. View the dashboard at \u001b[1m\u001b[32mhttp://127.0.0.1:8265 \u001b[39m\u001b[22m\n",
      "2025-05-15 15:51:30,615\tINFO torch_tensor_nccl_channel.py:772 -- Creating NCCL group d2bdd36b-47ca-49c5-91ab-d39fc868a6bd on actors: [Actor(TorchTensorWorker, e7f016e6579c1567c17f85d601000000), Actor(TorchTensorWorker, 5cbded9789090555e48c917601000000)]\n",
      "2025-05-15 15:51:31,516\tINFO torch_tensor_nccl_channel.py:797 -- NCCL group initialized.\n",
      "2025-05-15 15:51:31,677\tINFO dag_node_operation.py:634 -- Writing compiled graph schedule visualization to compiled_graph_schedule.png\n",
      "\u001b[32mPASSED\u001b[0m2025-05-15 15:51:31,959\tINFO compiled_dag_node.py:2173 -- Tearing down compiled DAG\n",
      "2025-05-15 15:51:31,960\tINFO compiled_dag_node.py:2178 -- Cancelling compiled worker on actor: Actor(TorchTensorWorker, e7f016e6579c1567c17f85d601000000)\n",
      "2025-05-15 15:51:31,960\tINFO compiled_dag_node.py:2178 -- Cancelling compiled worker on actor: Actor(TorchTensorWorker, 5cbded9789090555e48c917601000000)\n",
      "\u001b[36m(TorchTensorWorker pid=2161656)\u001b[0m Destructing NCCL group on actor: Actor(TorchTensorWorker, e7f016e6579c1567c17f85d601000000)\n",
      "\u001b[36m(TorchTensorWorker pid=2161657)\u001b[0m Destructing NCCL group on actor: Actor(TorchTensorWorker, 5cbded9789090555e48c917601000000)\n",
      "2025-05-15 15:51:32,483\tINFO compiled_dag_node.py:2200 -- Waiting for worker tasks to exit\n",
      "2025-05-15 15:51:32,483\tINFO compiled_dag_node.py:2161 -- Killing actor: Actor(TorchTensorWorker, e7f016e6579c1567c17f85d601000000)\n",
      "2025-05-15 15:51:32,483\tINFO compiled_dag_node.py:2161 -- Killing actor: Actor(TorchTensorWorker, 5cbded9789090555e48c917601000000)\n",
      "2025-05-15 15:51:32,484\tINFO compiled_dag_node.py:2203 -- Teardown complete\n",
      "\n",
      "test_torch_tensor_dag.py::test_nccl_all_reduce_with_class_method_output_node[ray_start_regular0] 2025-05-15 15:51:36,976\tINFO worker.py:1879 -- Started a local Ray instance. View the dashboard at \u001b[1m\u001b[32mhttp://127.0.0.1:8265 \u001b[39m\u001b[22m\n",
      "2025-05-15 15:51:39,158\tINFO torch_tensor_nccl_channel.py:772 -- Creating NCCL group c745903e-1b3a-4475-a6bc-93a02986dce0 on actors: [Actor(TorchTensorWorker, 9a730aa26042c33ed9fc2fa201000000), Actor(TorchTensorWorker, 1089a9460678579f2493eec001000000)]\n",
      "2025-05-15 15:51:40,019\tINFO torch_tensor_nccl_channel.py:797 -- NCCL group initialized.\n",
      "2025-05-15 15:51:40,267\tINFO dag_node_operation.py:634 -- Writing compiled graph schedule visualization to compiled_graph_schedule.png\n",
      "\u001b[32mPASSED\u001b[0m2025-05-15 15:51:40,581\tINFO compiled_dag_node.py:2173 -- Tearing down compiled DAG\n",
      "2025-05-15 15:51:40,581\tINFO compiled_dag_node.py:2178 -- Cancelling compiled worker on actor: Actor(TorchTensorWorker, 9a730aa26042c33ed9fc2fa201000000)\n",
      "2025-05-15 15:51:40,581\tINFO compiled_dag_node.py:2178 -- Cancelling compiled worker on actor: Actor(TorchTensorWorker, 1089a9460678579f2493eec001000000)\n",
      "\u001b[36m(TorchTensorWorker pid=2162947)\u001b[0m Destructing NCCL group on actor: Actor(TorchTensorWorker, 9a730aa26042c33ed9fc2fa201000000)\n",
      "\u001b[36m(TorchTensorWorker pid=2162945)\u001b[0m Destructing NCCL group on actor: Actor(TorchTensorWorker, 1089a9460678579f2493eec001000000)\n",
      "2025-05-15 15:51:41,102\tINFO compiled_dag_node.py:2200 -- Waiting for worker tasks to exit\n",
      "2025-05-15 15:51:41,103\tINFO compiled_dag_node.py:2161 -- Killing actor: Actor(TorchTensorWorker, 9a730aa26042c33ed9fc2fa201000000)\n",
      "2025-05-15 15:51:41,103\tINFO compiled_dag_node.py:2161 -- Killing actor: Actor(TorchTensorWorker, 1089a9460678579f2493eec001000000)\n",
      "2025-05-15 15:51:41,103\tINFO compiled_dag_node.py:2203 -- Teardown complete\n",
      "\n",
      "test_torch_tensor_dag.py::test_tensor_writable_warning_suppressed[ray_start_regular0] 2025-05-15 15:51:45,784\tINFO worker.py:1879 -- Started a local Ray instance. View the dashboard at \u001b[1m\u001b[32mhttp://127.0.0.1:8265 \u001b[39m\u001b[22m\n",
      "2025-05-15 15:51:46,319\tINFO dag_node_operation.py:634 -- Writing compiled graph schedule visualization to compiled_graph_schedule.png\n",
      "2025-05-15 15:51:50,832\tINFO compiled_dag_node.py:2173 -- Tearing down compiled DAG\n",
      "2025-05-15 15:51:50,832\tINFO compiled_dag_node.py:2178 -- Cancelling compiled worker on actor: Actor(test_tensor_writable_warning_suppressed.<locals>.A, f8ee421ce63dc9788843d7e401000000)\n",
      "2025-05-15 15:51:50,837\tINFO compiled_dag_node.py:2200 -- Waiting for worker tasks to exit\n",
      "2025-05-15 15:51:50,837\tINFO compiled_dag_node.py:2203 -- Teardown complete\n",
      "\u001b[32mPASSED\u001b[0m\n",
      "test_torch_tensor_dag.py::test_torch_nccl_channel_with_local_reader[ray_start_regular0] 2025-05-15 15:51:54,410\tINFO worker.py:1879 -- Started a local Ray instance. View the dashboard at \u001b[1m\u001b[32mhttp://127.0.0.1:8265 \u001b[39m\u001b[22m\n",
      "2025-05-15 15:51:56,587\tINFO torch_tensor_nccl_channel.py:772 -- Creating NCCL group 68906c5f-974a-4d1f-9b0c-0ebd5194d63c on actors: [Actor(TorchTensorWorker, a2a92577c28a9e77c77b8b7a01000000), Actor(TorchTensorWorker, 704e84bc41157b63baa73b9d01000000)]\n",
      "2025-05-15 15:51:57,436\tINFO torch_tensor_nccl_channel.py:797 -- NCCL group initialized.\n",
      "2025-05-15 15:51:57,634\tINFO dag_node_operation.py:634 -- Writing compiled graph schedule visualization to compiled_graph_schedule.png\n",
      "\u001b[32mPASSED\u001b[0m2025-05-15 15:51:57,841\tINFO compiled_dag_node.py:2173 -- Tearing down compiled DAG\n",
      "2025-05-15 15:51:57,841\tINFO compiled_dag_node.py:2178 -- Cancelling compiled worker on actor: Actor(TorchTensorWorker, 704e84bc41157b63baa73b9d01000000)\n",
      "2025-05-15 15:51:57,841\tINFO compiled_dag_node.py:2178 -- Cancelling compiled worker on actor: Actor(TorchTensorWorker, a2a92577c28a9e77c77b8b7a01000000)\n",
      "\u001b[36m(TorchTensorWorker pid=2165276)\u001b[0m Destructing NCCL group on actor: Actor(TorchTensorWorker, 704e84bc41157b63baa73b9d01000000)\n",
      "\u001b[36m(TorchTensorWorker pid=2165277)\u001b[0m Destructing NCCL group on actor: Actor(TorchTensorWorker, a2a92577c28a9e77c77b8b7a01000000)\n",
      "2025-05-15 15:51:58,358\tINFO compiled_dag_node.py:2200 -- Waiting for worker tasks to exit\n",
      "2025-05-15 15:51:58,358\tINFO compiled_dag_node.py:2161 -- Killing actor: Actor(TorchTensorWorker, 704e84bc41157b63baa73b9d01000000)\n",
      "2025-05-15 15:51:58,359\tINFO compiled_dag_node.py:2161 -- Killing actor: Actor(TorchTensorWorker, a2a92577c28a9e77c77b8b7a01000000)\n",
      "2025-05-15 15:51:58,359\tINFO compiled_dag_node.py:2203 -- Teardown complete\n",
      "\n",
      "test_torch_tensor_dag.py::test_torch_nccl_channel_with_two_local_readers[ray_start_regular0] 2025-05-15 15:52:02,069\tINFO worker.py:1879 -- Started a local Ray instance. View the dashboard at \u001b[1m\u001b[32mhttp://127.0.0.1:8265 \u001b[39m\u001b[22m\n",
      "2025-05-15 15:52:04,224\tINFO torch_tensor_nccl_channel.py:772 -- Creating NCCL group 124f85bf-c04e-4ac8-8cfb-3688a746f89c on actors: [Actor(TorchTensorWorker, 6b6693bfe7e34c9f025c145501000000), Actor(TorchTensorWorker, adbdc6fb284405655b02f42601000000)]\n",
      "2025-05-15 15:52:05,116\tINFO torch_tensor_nccl_channel.py:797 -- NCCL group initialized.\n",
      "2025-05-15 15:52:05,322\tINFO dag_node_operation.py:634 -- Writing compiled graph schedule visualization to compiled_graph_schedule.png\n",
      "\u001b[32mPASSED\u001b[0m2025-05-15 15:52:05,547\tINFO compiled_dag_node.py:2173 -- Tearing down compiled DAG\n",
      "2025-05-15 15:52:05,548\tINFO compiled_dag_node.py:2178 -- Cancelling compiled worker on actor: Actor(TorchTensorWorker, adbdc6fb284405655b02f42601000000)\n",
      "2025-05-15 15:52:05,548\tINFO compiled_dag_node.py:2178 -- Cancelling compiled worker on actor: Actor(TorchTensorWorker, 6b6693bfe7e34c9f025c145501000000)\n",
      "\u001b[36m(TorchTensorWorker pid=2166495)\u001b[0m Destructing NCCL group on actor: Actor(TorchTensorWorker, adbdc6fb284405655b02f42601000000)\n",
      "\u001b[36m(TorchTensorWorker pid=2166494)\u001b[0m Destructing NCCL group on actor: Actor(TorchTensorWorker, 6b6693bfe7e34c9f025c145501000000)\n",
      "2025-05-15 15:52:06,067\tINFO compiled_dag_node.py:2200 -- Waiting for worker tasks to exit\n",
      "2025-05-15 15:52:06,067\tINFO compiled_dag_node.py:2161 -- Killing actor: Actor(TorchTensorWorker, adbdc6fb284405655b02f42601000000)\n",
      "2025-05-15 15:52:06,068\tINFO compiled_dag_node.py:2161 -- Killing actor: Actor(TorchTensorWorker, 6b6693bfe7e34c9f025c145501000000)\n",
      "2025-05-15 15:52:06,068\tINFO compiled_dag_node.py:2203 -- Teardown complete\n",
      "\n",
      "test_torch_tensor_dag.py::test_torch_nccl_channel_with_all_local_readers[ray_start_regular0] 2025-05-15 15:52:09,565\tINFO worker.py:1879 -- Started a local Ray instance. View the dashboard at \u001b[1m\u001b[32mhttp://127.0.0.1:8265 \u001b[39m\u001b[22m\n",
      "2025-05-15 15:52:11,733\tINFO torch_tensor_nccl_channel.py:772 -- Creating NCCL group 0c6ae81c-de6d-4630-9822-03e0ddafa093 on actors: [Actor(TorchTensorWorker, 672759c260cea7b8183ab96c01000000)]\n",
      "2025-05-15 15:52:12,218\tINFO torch_tensor_nccl_channel.py:797 -- NCCL group initialized.\n",
      "\u001b[32mPASSED\u001b[0m\n",
      "\n",
      "=================================== FAILURES ===================================\n",
      "\u001b[31m\u001b[1m________ test_torch_tensor_nccl_overlap_timed[ray_start_regular0-False] ________\u001b[0m\n",
      "\n",
      "ray_start_regular = RayContext(dashboard_url='127.0.0.1:8265', python_version='3.13.3', ray_version='3.0.0.dev0', ray_commit='{{RAY_COMMIT_SHA}}')\n",
      "overlap_gpu_communication = False\n",
      "\n",
      "    \u001b[0m\u001b[37m@pytest\u001b[39;49;00m.mark.skipif(\u001b[95mnot\u001b[39;49;00m USE_GPU, reason=\u001b[33m\"\u001b[39;49;00m\u001b[33mSkipping GPU Test\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[33m\"\u001b[39;49;00m\u001b[33mray_start_regular, overlap_gpu_communication\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m,\u001b[90m\u001b[39;49;00m\n",
      "        [({\u001b[33m\"\u001b[39;49;00m\u001b[33mnum_cpus\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m: \u001b[94m4\u001b[39;49;00m}, \u001b[94mFalse\u001b[39;49;00m), ({\u001b[33m\"\u001b[39;49;00m\u001b[33mnum_cpus\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m: \u001b[94m4\u001b[39;49;00m}, \u001b[94mTrue\u001b[39;49;00m)],\u001b[90m\u001b[39;49;00m\n",
      "        indirect=[\u001b[33m\"\u001b[39;49;00m\u001b[33mray_start_regular\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m],\u001b[90m\u001b[39;49;00m\n",
      "    )\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mtest_torch_tensor_nccl_overlap_timed\u001b[39;49;00m(ray_start_regular, overlap_gpu_communication):\u001b[90m\u001b[39;49;00m\n",
      ">       \u001b[94massert\u001b[39;49;00m (\u001b[90m\u001b[39;49;00m\n",
      "            \u001b[96msum\u001b[39;49;00m(node[\u001b[33m\"\u001b[39;49;00m\u001b[33mResources\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m].get(\u001b[33m\"\u001b[39;49;00m\u001b[33mGPU\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[94m0\u001b[39;49;00m) \u001b[94mfor\u001b[39;49;00m node \u001b[95min\u001b[39;49;00m ray.nodes()) >= \u001b[94m4\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      "        ), \u001b[33m\"\u001b[39;49;00m\u001b[33mThis test requires at least 4 GPUs\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      "\u001b[1m\u001b[31mE       AssertionError: This test requires at least 4 GPUs\u001b[0m\n",
      "\u001b[1m\u001b[31mE       assert 2.0 >= 4\u001b[0m\n",
      "\u001b[1m\u001b[31mE        +  where 2.0 = sum(<generator object test_torch_tensor_nccl_overlap_timed.<locals>.<genexpr> at 0x705e80f6f300>)\u001b[0m\n",
      "\n",
      "\u001b[1m\u001b[31mtest_torch_tensor_dag.py\u001b[0m:379: AssertionError\n",
      "\u001b[31m\u001b[1m________ test_torch_tensor_nccl_overlap_timed[ray_start_regular1-True] _________\u001b[0m\n",
      "\n",
      "ray_start_regular = RayContext(dashboard_url='127.0.0.1:8265', python_version='3.13.3', ray_version='3.0.0.dev0', ray_commit='{{RAY_COMMIT_SHA}}')\n",
      "overlap_gpu_communication = True\n",
      "\n",
      "    \u001b[0m\u001b[37m@pytest\u001b[39;49;00m.mark.skipif(\u001b[95mnot\u001b[39;49;00m USE_GPU, reason=\u001b[33m\"\u001b[39;49;00m\u001b[33mSkipping GPU Test\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[33m\"\u001b[39;49;00m\u001b[33mray_start_regular, overlap_gpu_communication\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m,\u001b[90m\u001b[39;49;00m\n",
      "        [({\u001b[33m\"\u001b[39;49;00m\u001b[33mnum_cpus\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m: \u001b[94m4\u001b[39;49;00m}, \u001b[94mFalse\u001b[39;49;00m), ({\u001b[33m\"\u001b[39;49;00m\u001b[33mnum_cpus\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m: \u001b[94m4\u001b[39;49;00m}, \u001b[94mTrue\u001b[39;49;00m)],\u001b[90m\u001b[39;49;00m\n",
      "        indirect=[\u001b[33m\"\u001b[39;49;00m\u001b[33mray_start_regular\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m],\u001b[90m\u001b[39;49;00m\n",
      "    )\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mtest_torch_tensor_nccl_overlap_timed\u001b[39;49;00m(ray_start_regular, overlap_gpu_communication):\u001b[90m\u001b[39;49;00m\n",
      ">       \u001b[94massert\u001b[39;49;00m (\u001b[90m\u001b[39;49;00m\n",
      "            \u001b[96msum\u001b[39;49;00m(node[\u001b[33m\"\u001b[39;49;00m\u001b[33mResources\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m].get(\u001b[33m\"\u001b[39;49;00m\u001b[33mGPU\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[94m0\u001b[39;49;00m) \u001b[94mfor\u001b[39;49;00m node \u001b[95min\u001b[39;49;00m ray.nodes()) >= \u001b[94m4\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      "        ), \u001b[33m\"\u001b[39;49;00m\u001b[33mThis test requires at least 4 GPUs\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      "\u001b[1m\u001b[31mE       AssertionError: This test requires at least 4 GPUs\u001b[0m\n",
      "\u001b[1m\u001b[31mE       assert 2.0 >= 4\u001b[0m\n",
      "\u001b[1m\u001b[31mE        +  where 2.0 = sum(<generator object test_torch_tensor_nccl_overlap_timed.<locals>.<genexpr> at 0x705e947c3840>)\u001b[0m\n",
      "\n",
      "\u001b[1m\u001b[31mtest_torch_tensor_dag.py\u001b[0m:379: AssertionError\n",
      "\u001b[31m\u001b[1m________ test_torch_tensor_default_comm[transports0-ray_start_regular0] ________\u001b[0m\n",
      "\n",
      "ray_start_regular = RayContext(dashboard_url='127.0.0.1:8265', python_version='3.13.3', ray_version='3.0.0.dev0', ray_commit='{{RAY_COMMIT_SHA}}')\n",
      "transports = ['auto', 'nccl']\n",
      "\n",
      "    \u001b[0m\u001b[37m@pytest\u001b[39;49;00m.mark.skipif(\u001b[95mnot\u001b[39;49;00m USE_GPU, reason=\u001b[33m\"\u001b[39;49;00m\u001b[33mSkipping GPU Test\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mray_start_regular\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [{\u001b[33m\"\u001b[39;49;00m\u001b[33mnum_cpus\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m: \u001b[94m4\u001b[39;49;00m}], indirect=\u001b[94mTrue\u001b[39;49;00m)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[33m\"\u001b[39;49;00m\u001b[33mtransports\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m,\u001b[90m\u001b[39;49;00m\n",
      "        [[\u001b[33m\"\u001b[39;49;00m\u001b[33mauto\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33mnccl\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m], [\u001b[33m\"\u001b[39;49;00m\u001b[33mcustom\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33mnccl\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m], [\u001b[33m\"\u001b[39;49;00m\u001b[33mauto\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33mnccl\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m], [\u001b[33m\"\u001b[39;49;00m\u001b[33mcustom\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33mcustom\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m]],\u001b[90m\u001b[39;49;00m\n",
      "    )\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mtest_torch_tensor_default_comm\u001b[39;49;00m(ray_start_regular, transports):\u001b[90m\u001b[39;49;00m\n",
      ">       \u001b[94massert\u001b[39;49;00m (\u001b[90m\u001b[39;49;00m\n",
      "            \u001b[96msum\u001b[39;49;00m(node[\u001b[33m\"\u001b[39;49;00m\u001b[33mResources\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m].get(\u001b[33m\"\u001b[39;49;00m\u001b[33mGPU\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[94m0\u001b[39;49;00m) \u001b[94mfor\u001b[39;49;00m node \u001b[95min\u001b[39;49;00m ray.nodes()) > \u001b[94m2\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      "        ), \u001b[33m\"\u001b[39;49;00m\u001b[33mThis test requires at least 3 GPUs\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      "\u001b[1m\u001b[31mE       AssertionError: This test requires at least 3 GPUs\u001b[0m\n",
      "\u001b[1m\u001b[31mE       assert 2.0 > 2\u001b[0m\n",
      "\u001b[1m\u001b[31mE        +  where 2.0 = sum(<generator object test_torch_tensor_default_comm.<locals>.<genexpr> at 0x705e9464cac0>)\u001b[0m\n",
      "\n",
      "\u001b[1m\u001b[31mtest_torch_tensor_dag.py\u001b[0m:744: AssertionError\n",
      "\u001b[31m\u001b[1m________ test_torch_tensor_default_comm[transports1-ray_start_regular0] ________\u001b[0m\n",
      "\n",
      "ray_start_regular = RayContext(dashboard_url='127.0.0.1:8265', python_version='3.13.3', ray_version='3.0.0.dev0', ray_commit='{{RAY_COMMIT_SHA}}')\n",
      "transports = ['custom', 'nccl']\n",
      "\n",
      "    \u001b[0m\u001b[37m@pytest\u001b[39;49;00m.mark.skipif(\u001b[95mnot\u001b[39;49;00m USE_GPU, reason=\u001b[33m\"\u001b[39;49;00m\u001b[33mSkipping GPU Test\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mray_start_regular\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [{\u001b[33m\"\u001b[39;49;00m\u001b[33mnum_cpus\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m: \u001b[94m4\u001b[39;49;00m}], indirect=\u001b[94mTrue\u001b[39;49;00m)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[33m\"\u001b[39;49;00m\u001b[33mtransports\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m,\u001b[90m\u001b[39;49;00m\n",
      "        [[\u001b[33m\"\u001b[39;49;00m\u001b[33mauto\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33mnccl\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m], [\u001b[33m\"\u001b[39;49;00m\u001b[33mcustom\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33mnccl\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m], [\u001b[33m\"\u001b[39;49;00m\u001b[33mauto\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33mnccl\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m], [\u001b[33m\"\u001b[39;49;00m\u001b[33mcustom\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33mcustom\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m]],\u001b[90m\u001b[39;49;00m\n",
      "    )\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mtest_torch_tensor_default_comm\u001b[39;49;00m(ray_start_regular, transports):\u001b[90m\u001b[39;49;00m\n",
      ">       \u001b[94massert\u001b[39;49;00m (\u001b[90m\u001b[39;49;00m\n",
      "            \u001b[96msum\u001b[39;49;00m(node[\u001b[33m\"\u001b[39;49;00m\u001b[33mResources\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m].get(\u001b[33m\"\u001b[39;49;00m\u001b[33mGPU\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[94m0\u001b[39;49;00m) \u001b[94mfor\u001b[39;49;00m node \u001b[95min\u001b[39;49;00m ray.nodes()) > \u001b[94m2\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      "        ), \u001b[33m\"\u001b[39;49;00m\u001b[33mThis test requires at least 3 GPUs\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      "\u001b[1m\u001b[31mE       AssertionError: This test requires at least 3 GPUs\u001b[0m\n",
      "\u001b[1m\u001b[31mE       assert 2.0 > 2\u001b[0m\n",
      "\u001b[1m\u001b[31mE        +  where 2.0 = sum(<generator object test_torch_tensor_default_comm.<locals>.<genexpr> at 0x705e9464e0a0>)\u001b[0m\n",
      "\n",
      "\u001b[1m\u001b[31mtest_torch_tensor_dag.py\u001b[0m:744: AssertionError\n",
      "\u001b[31m\u001b[1m________ test_torch_tensor_default_comm[transports2-ray_start_regular0] ________\u001b[0m\n",
      "\n",
      "ray_start_regular = RayContext(dashboard_url='127.0.0.1:8265', python_version='3.13.3', ray_version='3.0.0.dev0', ray_commit='{{RAY_COMMIT_SHA}}')\n",
      "transports = ['auto', 'nccl']\n",
      "\n",
      "    \u001b[0m\u001b[37m@pytest\u001b[39;49;00m.mark.skipif(\u001b[95mnot\u001b[39;49;00m USE_GPU, reason=\u001b[33m\"\u001b[39;49;00m\u001b[33mSkipping GPU Test\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mray_start_regular\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [{\u001b[33m\"\u001b[39;49;00m\u001b[33mnum_cpus\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m: \u001b[94m4\u001b[39;49;00m}], indirect=\u001b[94mTrue\u001b[39;49;00m)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[33m\"\u001b[39;49;00m\u001b[33mtransports\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m,\u001b[90m\u001b[39;49;00m\n",
      "        [[\u001b[33m\"\u001b[39;49;00m\u001b[33mauto\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33mnccl\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m], [\u001b[33m\"\u001b[39;49;00m\u001b[33mcustom\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33mnccl\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m], [\u001b[33m\"\u001b[39;49;00m\u001b[33mauto\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33mnccl\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m], [\u001b[33m\"\u001b[39;49;00m\u001b[33mcustom\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33mcustom\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m]],\u001b[90m\u001b[39;49;00m\n",
      "    )\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mtest_torch_tensor_default_comm\u001b[39;49;00m(ray_start_regular, transports):\u001b[90m\u001b[39;49;00m\n",
      ">       \u001b[94massert\u001b[39;49;00m (\u001b[90m\u001b[39;49;00m\n",
      "            \u001b[96msum\u001b[39;49;00m(node[\u001b[33m\"\u001b[39;49;00m\u001b[33mResources\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m].get(\u001b[33m\"\u001b[39;49;00m\u001b[33mGPU\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[94m0\u001b[39;49;00m) \u001b[94mfor\u001b[39;49;00m node \u001b[95min\u001b[39;49;00m ray.nodes()) > \u001b[94m2\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      "        ), \u001b[33m\"\u001b[39;49;00m\u001b[33mThis test requires at least 3 GPUs\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      "\u001b[1m\u001b[31mE       AssertionError: This test requires at least 3 GPUs\u001b[0m\n",
      "\u001b[1m\u001b[31mE       assert 2.0 > 2\u001b[0m\n",
      "\u001b[1m\u001b[31mE        +  where 2.0 = sum(<generator object test_torch_tensor_default_comm.<locals>.<genexpr> at 0x705e9464e6c0>)\u001b[0m\n",
      "\n",
      "\u001b[1m\u001b[31mtest_torch_tensor_dag.py\u001b[0m:744: AssertionError\n",
      "\u001b[31m\u001b[1m________ test_torch_tensor_default_comm[transports3-ray_start_regular0] ________\u001b[0m\n",
      "\n",
      "ray_start_regular = RayContext(dashboard_url='127.0.0.1:8265', python_version='3.13.3', ray_version='3.0.0.dev0', ray_commit='{{RAY_COMMIT_SHA}}')\n",
      "transports = ['custom', 'custom']\n",
      "\n",
      "    \u001b[0m\u001b[37m@pytest\u001b[39;49;00m.mark.skipif(\u001b[95mnot\u001b[39;49;00m USE_GPU, reason=\u001b[33m\"\u001b[39;49;00m\u001b[33mSkipping GPU Test\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mray_start_regular\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [{\u001b[33m\"\u001b[39;49;00m\u001b[33mnum_cpus\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m: \u001b[94m4\u001b[39;49;00m}], indirect=\u001b[94mTrue\u001b[39;49;00m)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[33m\"\u001b[39;49;00m\u001b[33mtransports\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m,\u001b[90m\u001b[39;49;00m\n",
      "        [[\u001b[33m\"\u001b[39;49;00m\u001b[33mauto\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33mnccl\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m], [\u001b[33m\"\u001b[39;49;00m\u001b[33mcustom\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33mnccl\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m], [\u001b[33m\"\u001b[39;49;00m\u001b[33mauto\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33mnccl\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m], [\u001b[33m\"\u001b[39;49;00m\u001b[33mcustom\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33mcustom\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m]],\u001b[90m\u001b[39;49;00m\n",
      "    )\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mtest_torch_tensor_default_comm\u001b[39;49;00m(ray_start_regular, transports):\u001b[90m\u001b[39;49;00m\n",
      ">       \u001b[94massert\u001b[39;49;00m (\u001b[90m\u001b[39;49;00m\n",
      "            \u001b[96msum\u001b[39;49;00m(node[\u001b[33m\"\u001b[39;49;00m\u001b[33mResources\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m].get(\u001b[33m\"\u001b[39;49;00m\u001b[33mGPU\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[94m0\u001b[39;49;00m) \u001b[94mfor\u001b[39;49;00m node \u001b[95min\u001b[39;49;00m ray.nodes()) > \u001b[94m2\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      "        ), \u001b[33m\"\u001b[39;49;00m\u001b[33mThis test requires at least 3 GPUs\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      "\u001b[1m\u001b[31mE       AssertionError: This test requires at least 3 GPUs\u001b[0m\n",
      "\u001b[1m\u001b[31mE       assert 2.0 > 2\u001b[0m\n",
      "\u001b[1m\u001b[31mE        +  where 2.0 = sum(<generator object test_torch_tensor_default_comm.<locals>.<genexpr> at 0x705e9464fe60>)\u001b[0m\n",
      "\n",
      "\u001b[1m\u001b[31mtest_torch_tensor_dag.py\u001b[0m:744: AssertionError\n",
      "\u001b[36m\u001b[1m=========================== short test summary info ============================\u001b[0m\n",
      "\u001b[31mFAILED\u001b[0m test_torch_tensor_dag.py::\u001b[1mtest_torch_tensor_nccl_overlap_timed[ray_start_regular0-False]\u001b[0m - AssertionError: This test requires at least 4 GPUs\n",
      "\u001b[31mFAILED\u001b[0m test_torch_tensor_dag.py::\u001b[1mtest_torch_tensor_nccl_overlap_timed[ray_start_regular1-True]\u001b[0m - AssertionError: This test requires at least 4 GPUs\n",
      "\u001b[31mFAILED\u001b[0m test_torch_tensor_dag.py::\u001b[1mtest_torch_tensor_default_comm[transports0-ray_start_regular0]\u001b[0m - AssertionError: This test requires at least 3 GPUs\n",
      "\u001b[31mFAILED\u001b[0m test_torch_tensor_dag.py::\u001b[1mtest_torch_tensor_default_comm[transports1-ray_start_regular0]\u001b[0m - AssertionError: This test requires at least 3 GPUs\n",
      "\u001b[31mFAILED\u001b[0m test_torch_tensor_dag.py::\u001b[1mtest_torch_tensor_default_comm[transports2-ray_start_regular0]\u001b[0m - AssertionError: This test requires at least 3 GPUs\n",
      "\u001b[31mFAILED\u001b[0m test_torch_tensor_dag.py::\u001b[1mtest_torch_tensor_default_comm[transports3-ray_start_regular0]\u001b[0m - AssertionError: This test requires at least 3 GPUs\n",
      "\u001b[31m============= \u001b[31m\u001b[1m6 failed\u001b[0m, \u001b[32m45 passed\u001b[0m, \u001b[33m2 skipped\u001b[0m\u001b[31m in 401.09s (0:06:41)\u001b[0m\u001b[31m ==============\u001b[0m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!python -m pytest -v -s test_torch_tensor_dag.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m============================= test session starts ==============================\u001b[0m\n",
      "platform linux -- Python 3.13.3, pytest-8.3.5, pluggy-1.5.0 -- /home/wxdeng/miniconda3/envs/stbr-unify/bin/python\n",
      "cachedir: .pytest_cache\n",
      "rootdir: /home/wxdeng/stbr-min\n",
      "configfile: pytest.ini\n",
      "plugins: anyio-4.9.0\n",
      "collected 1 item                                                               \u001b[0m\u001b[1m\n",
      "\n",
      "test_accelerated_dag.py::test_event_profiling 2025-05-15 15:52:19,995\tINFO worker.py:1879 -- Started a local Ray instance. View the dashboard at \u001b[1m\u001b[32mhttp://127.0.0.1:8265 \u001b[39m\u001b[22m\n",
      "2025-05-15 15:52:21,382\tINFO dag_node_operation.py:634 -- Writing compiled graph schedule visualization to compiled_graph_schedule.png\n",
      "\u001b[32mPASSED\u001b[0m2025-05-15 15:52:22,847\tINFO compiled_dag_node.py:2173 -- Tearing down compiled DAG\n",
      "2025-05-15 15:52:22,847\tINFO compiled_dag_node.py:2178 -- Cancelling compiled worker on actor: Actor(Actor, 1c26290143ff0749ee64a41201000000)\n",
      "2025-05-15 15:52:22,847\tINFO compiled_dag_node.py:2178 -- Cancelling compiled worker on actor: Actor(Actor, 363c5864c16c9ede506a615901000000)\n",
      "2025-05-15 15:52:22,853\tINFO compiled_dag_node.py:2200 -- Waiting for worker tasks to exit\n",
      "2025-05-15 15:52:22,853\tINFO compiled_dag_node.py:2161 -- Killing actor: Actor(Actor, 1c26290143ff0749ee64a41201000000)\n",
      "2025-05-15 15:52:22,854\tINFO compiled_dag_node.py:2161 -- Killing actor: Actor(Actor, 363c5864c16c9ede506a615901000000)\n",
      "2025-05-15 15:52:22,854\tINFO compiled_dag_node.py:2203 -- Teardown complete\n",
      "\n",
      "\n",
      "\u001b[32m============================== \u001b[32m\u001b[1m1 passed\u001b[0m\u001b[32m in 8.90s\u001b[0m\u001b[32m ===============================\u001b[0m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!python -m pytest -v -s test_accelerated_dag.py::test_event_profiling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "# !python -m pytest -v -s test_cpu_communicator_dag.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "# !python -m pytest -v -s test_execution_schedule_gpu.py"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "stbr-coll-sched-0512",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
